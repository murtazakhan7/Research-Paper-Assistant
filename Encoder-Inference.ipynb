{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Encoder-Only Transformer Inference\n",
        "\n",
        "This notebook loads a pre-trained encoder-only transformer model and classifies paper abstracts into research topics:\n",
        "\n",
        "- Neural Networks & Deep Learning\n",
        "- Natural Language Processing\n",
        "- Computer Vision\n",
        "- Reinforcement Learning\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from tokenizers import Tokenizer\n",
        "import json\n",
        "import os\n",
        "import math\n",
        "import pandas as pd\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "# MODEL COMPONENTS\n",
        "\n",
        "class SinusoidalPositionalEncoding(nn.Module):\n",
        "    \"\"\"Sinusoidal positional encoding.\"\"\"\n",
        "\n",
        "    def __init__(self, d_model, max_len=256):\n",
        "        super().__init__()\n",
        "        pe = torch.zeros(max_len, d_model)\n",
        "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
        "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
        "        pe[:, 0::2] = torch.sin(position * div_term)\n",
        "        pe[:, 1::2] = torch.cos(position * div_term)\n",
        "        self.register_buffer('pe', pe.unsqueeze(0))\n",
        "\n",
        "    def forward(self, x):\n",
        "        return x + self.pe[:, :x.size(1), :]\n",
        "\n",
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "    \"\"\"Bidirectional multi-head self-attention.\"\"\"\n",
        "\n",
        "    def __init__(self, d_model, num_heads):\n",
        "        super().__init__()\n",
        "        assert d_model % num_heads == 0\n",
        "        self.d_model = d_model\n",
        "        self.num_heads = num_heads\n",
        "        self.d_k = d_model // num_heads\n",
        "        self.W_q = nn.Linear(d_model, d_model)\n",
        "        self.W_k = nn.Linear(d_model, d_model)\n",
        "        self.W_v = nn.Linear(d_model, d_model)\n",
        "        self.W_o = nn.Linear(d_model, d_model)\n",
        "\n",
        "    def forward(self, x, mask=None):\n",
        "        batch_size, seq_len, _ = x.shape\n",
        "        Q = self.W_q(x).view(batch_size, seq_len, self.num_heads, self.d_k).transpose(1, 2)\n",
        "        K = self.W_k(x).view(batch_size, seq_len, self.num_heads, self.d_k).transpose(1, 2)\n",
        "        V = self.W_v(x).view(batch_size, seq_len, self.num_heads, self.d_k).transpose(1, 2)\n",
        "        scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(self.d_k)\n",
        "        if mask is not None:\n",
        "            mask = mask.unsqueeze(1).unsqueeze(2)\n",
        "            scores = scores.masked_fill(mask == 0, float('-inf'))\n",
        "        attn_weights = F.softmax(scores, dim=-1)\n",
        "        attn_weights = torch.nan_to_num(attn_weights)\n",
        "        attn_output = torch.matmul(attn_weights, V)\n",
        "        attn_output = attn_output.transpose(1, 2).contiguous().view(batch_size, seq_len, self.d_model)\n",
        "        return self.W_o(attn_output)\n",
        "\n",
        "\n",
        "class FeedForward(nn.Module):\n",
        "    \"\"\"Position-wise feed-forward network.\"\"\"\n",
        "\n",
        "    def __init__(self, d_model, d_ff):\n",
        "        super().__init__()\n",
        "        self.linear1 = nn.Linear(d_model, d_ff)\n",
        "        self.linear2 = nn.Linear(d_ff, d_model)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.linear2(F.relu(self.linear1(x)))\n",
        "\n",
        "\n",
        "class EncoderBlock(nn.Module):\n",
        "    \"\"\"Single encoder block.\"\"\"\n",
        "\n",
        "    def __init__(self, d_model, num_heads, d_ff, dropout=0.1):\n",
        "        super().__init__()\n",
        "        self.attention = MultiHeadAttention(d_model, num_heads)\n",
        "        self.norm1 = nn.LayerNorm(d_model)\n",
        "        self.ffn = FeedForward(d_model, d_ff)\n",
        "        self.norm2 = nn.LayerNorm(d_model)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x, mask=None):\n",
        "        attn_output = self.attention(x, mask)\n",
        "        x = self.norm1(x + self.dropout(attn_output))\n",
        "        ffn_output = self.ffn(x)\n",
        "        x = self.norm2(x + self.dropout(ffn_output))\n",
        "        return x\n",
        "\n",
        "\n",
        "class EncoderOnlyTransformer(nn.Module):\n",
        "    \"\"\"Encoder-only Transformer for classification.\"\"\"\n",
        "\n",
        "    def __init__(self, vocab_size, d_model=256, num_layers=4, num_heads=8,\n",
        "                 d_ff=1024, max_seq_len=256, num_classes=4, dropout=0.1, pooling_type='cls'):\n",
        "        super().__init__()\n",
        "        self.d_model = d_model\n",
        "        self.pooling_type = pooling_type\n",
        "        self.token_embedding = nn.Embedding(vocab_size, d_model)\n",
        "        self.pos_encoding = SinusoidalPositionalEncoding(d_model, max_seq_len)\n",
        "        self.encoder_blocks = nn.ModuleList([\n",
        "            EncoderBlock(d_model, num_heads, d_ff, dropout)\n",
        "            for _ in range(num_layers)\n",
        "        ])\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(d_model, num_classes)\n",
        "        )\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, input_ids, attention_mask=None):\n",
        "        x = self.token_embedding(input_ids)\n",
        "        x = self.pos_encoding(x)\n",
        "        x = self.dropout(x)\n",
        "        for encoder_block in self.encoder_blocks:\n",
        "            x = encoder_block(x, attention_mask)\n",
        "        if self.pooling_type == 'cls':\n",
        "            pooled = x[:, 0, :]\n",
        "        else:\n",
        "            if attention_mask is not None:\n",
        "                mask_expanded = attention_mask.unsqueeze(-1).float()\n",
        "                x_masked = x * mask_expanded\n",
        "                sum_pooled = x_masked.sum(dim=1)\n",
        "                lengths = attention_mask.sum(dim=1, keepdim=True).float()\n",
        "                pooled = sum_pooled / lengths\n",
        "            else:\n",
        "                pooled = x.mean(dim=1)\n",
        "        return self.classifier(pooled)\n",
        "\n",
        "\n",
        "TOPIC_NAMES = [\n",
        "    'Neural Networks & Deep Learning',\n",
        "    'Natural Language Processing',\n",
        "    'Computer Vision',\n",
        "    'Reinforcement Learning'\n",
        "]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "# MODEL LOADING\n",
        "\n",
        "def load_model(model_dir, device=None):\n",
        "    \"\"\"Load a saved encoder model from directory.\"\"\"\n",
        "    if device is None:\n",
        "        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    \n",
        "    print(f\"Loading model from: {model_dir}\")\n",
        "    print(f\"Using device: {device}\")\n",
        "    \n",
        "    config_path = os.path.join(model_dir, \"config.json\")\n",
        "    with open(config_path, 'r') as f:\n",
        "        config = json.load(f)\n",
        "    print(\"[OK] Config loaded\")\n",
        "    \n",
        "    tokenizer_path = os.path.join(model_dir, \"tokenizer.json\")\n",
        "    tokenizer = Tokenizer.from_file(tokenizer_path)\n",
        "    print(\"[OK] Tokenizer loaded\")\n",
        "    \n",
        "    metadata_path = os.path.join(model_dir, \"metadata.json\")\n",
        "    with open(metadata_path, 'r') as f:\n",
        "        metadata = json.load(f)\n",
        "    print(\"[OK] Metadata loaded\")\n",
        "    \n",
        "    model = EncoderOnlyTransformer(\n",
        "        vocab_size=tokenizer.get_vocab_size(),\n",
        "        d_model=config['d_model'],\n",
        "        num_layers=config['num_layers'],\n",
        "        num_heads=config['num_heads'],\n",
        "        d_ff=config['d_ff'],\n",
        "        max_seq_len=config['max_seq_len'],\n",
        "        num_classes=config['num_classes'],\n",
        "        dropout=config['dropout'],\n",
        "        pooling_type=config['pooling_type']\n",
        "    )\n",
        "    \n",
        "    model_path = os.path.join(model_dir, \"model.pt\")\n",
        "    model.load_state_dict(torch.load(model_path, map_location=device))\n",
        "    model = model.to(device)\n",
        "    model.eval()\n",
        "    print(\"[OK] Model weights loaded\")\n",
        "    \n",
        "    print(\"\\n\" + \"=\"*50)\n",
        "    print(\"MODEL INFO\")\n",
        "    print(\"=\"*50)\n",
        "    print(f\"Created: {metadata.get('created_at', 'Unknown')}\")\n",
        "    print(f\"Best Epoch: {metadata.get('best_epoch', 'Unknown')}\")\n",
        "    print(f\"Best Val Accuracy: {metadata.get('best_val_accuracy', 0):.4f}\")\n",
        "    print(f\"Parameters: {metadata.get('num_parameters', 0):,}\")\n",
        "    print(f\"Model Size: {metadata.get('model_size_mb', 0):.2f} MB\")\n",
        "    print(\"=\"*50)\n",
        "    \n",
        "    return model, tokenizer, config, metadata\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "# CLASSIFICATION FUNCTIONS\n",
        "\n",
        "def classify_abstract(abstract, model, tokenizer, config, device=None):\n",
        "    \"\"\"Classify a paper abstract.\"\"\"\n",
        "    if device is None:\n",
        "        device = next(model.parameters()).device\n",
        "    \n",
        "    model.eval()\n",
        "    cls_id = tokenizer.token_to_id(\"[CLS]\") or tokenizer.token_to_id(\"[BOS]\")\n",
        "    pad_id = tokenizer.token_to_id(\"[PAD]\")\n",
        "    sep_id = tokenizer.token_to_id(\"[SEP]\") or tokenizer.token_to_id(\"[EOS]\")\n",
        "    \n",
        "    encoding = tokenizer.encode(abstract)\n",
        "    tokens = encoding.ids\n",
        "    max_len = config['max_seq_len']\n",
        "    if len(tokens) > max_len - 2:\n",
        "        tokens = tokens[:max_len - 2]\n",
        "    tokens = [cls_id] + tokens + [sep_id]\n",
        "    pad_len = max_len - len(tokens)\n",
        "    tokens = tokens + [pad_id] * pad_len\n",
        "    attention_mask = [1 if t != pad_id else 0 for t in tokens]\n",
        "    \n",
        "    input_ids = torch.tensor([tokens], dtype=torch.long).to(device)\n",
        "    attention_mask = torch.tensor([attention_mask], dtype=torch.long).to(device)\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        logits = model(input_ids, attention_mask)\n",
        "        probabilities = F.softmax(logits, dim=-1)\n",
        "        predicted_class = torch.argmax(probabilities, dim=-1).item()\n",
        "        confidence = probabilities[0, predicted_class].item()\n",
        "        all_probs = {TOPIC_NAMES[i]: probabilities[0, i].item() for i in range(len(TOPIC_NAMES))}\n",
        "    \n",
        "    return {\n",
        "        'predicted_class': predicted_class,\n",
        "        'predicted_topic': TOPIC_NAMES[predicted_class],\n",
        "        'confidence': confidence,\n",
        "        'probabilities': all_probs\n",
        "    }\n",
        "\n",
        "\n",
        "def print_result(result):\n",
        "    \"\"\"Print classification result.\"\"\"\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"CLASSIFICATION RESULT\")\n",
        "    print(\"=\"*60)\n",
        "    print(f\"\\nPredicted Topic: {result['predicted_topic']}\")\n",
        "    print(f\"Confidence: {result['confidence']*100:.2f}%\")\n",
        "    print(\"\\nAll Probabilities:\")\n",
        "    sorted_probs = sorted(result['probabilities'].items(), key=lambda x: x[1], reverse=True)\n",
        "    for topic, prob in sorted_probs:\n",
        "        bar = \"#\" * int(prob * 40)\n",
        "        print(f\"  {topic:35s} {prob*100:6.2f}% {bar}\")\n",
        "    print(\"=\"*60)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Load the Model\n",
        "\n",
        "Set the path to your saved model directory. Models are saved in `models/` with names like `encoder_20241221_143025`.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Available models:\n",
            "  - models/encoder_20251220_194106\n",
            "\n",
            "Using most recent: models/encoder_20251220_194106\n"
          ]
        }
      ],
      "source": [
        "# Find available models\n",
        "MODEL_DIR = None\n",
        "\n",
        "if os.path.exists(\"models\"):\n",
        "    models = sorted([d for d in os.listdir(\"models\") if d.startswith(\"encoder_\")])\n",
        "    if models:\n",
        "        print(\"Available models:\")\n",
        "        for m in models:\n",
        "            print(f\"  - models/{m}\")\n",
        "        MODEL_DIR = os.path.join(\"models\", models[-1])\n",
        "        print(f\"\\nUsing most recent: {MODEL_DIR}\")\n",
        "    else:\n",
        "        print(\"No encoder models found in models/ directory.\")\n",
        "        print(\"Run Encoder-Only.ipynb first to train a model.\")\n",
        "else:\n",
        "    print(\"models/ directory not found.\")\n",
        "    print(\"Run Encoder-Only.ipynb first to train a model.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading model from: models/encoder_20251220_194106\n",
            "Using device: cpu\n",
            "[OK] Config loaded\n",
            "[OK] Tokenizer loaded\n",
            "[OK] Metadata loaded\n",
            "[OK] Model weights loaded\n",
            "\n",
            "==================================================\n",
            "MODEL INFO\n",
            "==================================================\n",
            "Created: 2025-12-20T19:41:09.545692\n",
            "Best Epoch: 20\n",
            "Best Val Accuracy: 0.8030\n",
            "Parameters: 5,208,068\n",
            "Model Size: 19.87 MB\n",
            "==================================================\n"
          ]
        }
      ],
      "source": [
        "# Load the model\n",
        "if MODEL_DIR:\n",
        "    model, tokenizer, config, metadata = load_model(MODEL_DIR)\n",
        "else:\n",
        "    print(\"Please set MODEL_DIR to a valid model path.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Classify Example Abstracts\n",
        "\n",
        "Test the model with example papers from different research areas.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "============================================================\n",
            "CLASSIFICATION RESULT\n",
            "============================================================\n",
            "\n",
            "Predicted Topic: Neural Networks & Deep Learning\n",
            "Confidence: 99.89%\n",
            "\n",
            "All Probabilities:\n",
            "  Neural Networks & Deep Learning      99.89% #######################################\n",
            "  Computer Vision                       0.09% \n",
            "  Reinforcement Learning                0.01% \n",
            "  Natural Language Processing           0.01% \n",
            "============================================================\n"
          ]
        }
      ],
      "source": [
        "# Example: Neural Networks paper\n",
        "abstract_nn = \"\"\"\n",
        "We propose a novel deep neural network architecture that combines \n",
        "convolutional layers with attention mechanisms. Our model uses \n",
        "batch normalization and dropout for regularization. The network \n",
        "is trained using backpropagation with Adam optimizer. Experiments \n",
        "show improved performance on benchmark datasets compared to \n",
        "standard CNN architectures.\n",
        "\"\"\"\n",
        "\n",
        "result = classify_abstract(abstract_nn, model, tokenizer, config)\n",
        "print_result(result)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "============================================================\n",
            "CLASSIFICATION RESULT\n",
            "============================================================\n",
            "\n",
            "Predicted Topic: Natural Language Processing\n",
            "Confidence: 97.31%\n",
            "\n",
            "All Probabilities:\n",
            "  Natural Language Processing          97.31% ######################################\n",
            "  Reinforcement Learning                2.19% \n",
            "  Computer Vision                       0.41% \n",
            "  Neural Networks & Deep Learning       0.09% \n",
            "============================================================\n"
          ]
        }
      ],
      "source": [
        "# Example: NLP paper\n",
        "abstract_nlp = \"\"\"\n",
        "This paper presents a new approach to sentiment analysis using \n",
        "transformer-based language models. We fine-tune BERT on a large \n",
        "corpus of product reviews and achieve state-of-the-art results \n",
        "on text classification benchmarks. Our method uses word embeddings \n",
        "and attention mechanisms to capture semantic relationships in text.\n",
        "\"\"\"\n",
        "\n",
        "result = classify_abstract(abstract_nlp, model, tokenizer, config)\n",
        "print_result(result)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "============================================================\n",
            "CLASSIFICATION RESULT\n",
            "============================================================\n",
            "\n",
            "Predicted Topic: Natural Language Processing\n",
            "Confidence: 98.28%\n",
            "\n",
            "All Probabilities:\n",
            "  Natural Language Processing          98.28% #######################################\n",
            "  Reinforcement Learning                0.91% \n",
            "  Computer Vision                       0.62% \n",
            "  Neural Networks & Deep Learning       0.19% \n",
            "============================================================\n"
          ]
        }
      ],
      "source": [
        "# Example: Computer Vision paper\n",
        "abstract_cv = \"\"\"\n",
        "We introduce a novel object detection framework for autonomous \n",
        "driving applications. Our approach uses semantic segmentation \n",
        "and depth estimation to improve detection accuracy. The model \n",
        "processes RGB images and outputs bounding boxes with class labels. \n",
        "Experiments on the KITTI dataset demonstrate superior performance \n",
        "compared to YOLO and Faster R-CNN baselines.\n",
        "\"\"\"\n",
        "\n",
        "result = classify_abstract(abstract_cv, model, tokenizer, config)\n",
        "print_result(result)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "============================================================\n",
            "CLASSIFICATION RESULT\n",
            "============================================================\n",
            "\n",
            "Predicted Topic: Reinforcement Learning\n",
            "Confidence: 95.60%\n",
            "\n",
            "All Probabilities:\n",
            "  Reinforcement Learning               95.60% ######################################\n",
            "  Natural Language Processing           3.52% #\n",
            "  Computer Vision                       0.84% \n",
            "  Neural Networks & Deep Learning       0.03% \n",
            "============================================================\n"
          ]
        }
      ],
      "source": [
        "# Example: Reinforcement Learning paper\n",
        "abstract_rl = \"\"\"\n",
        "This work proposes a new deep reinforcement learning algorithm \n",
        "for robotic control. Our method combines policy gradient with \n",
        "value function approximation using an actor-critic architecture. \n",
        "The agent learns to navigate complex environments through \n",
        "exploration and exploitation. We evaluate on the OpenAI Gym \n",
        "benchmark and achieve competitive performance with PPO.\n",
        "\"\"\"\n",
        "\n",
        "result = classify_abstract(abstract_rl, model, tokenizer, config)\n",
        "print_result(result)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Classify Your Own Abstract\n",
        "\n",
        "Paste your paper abstract below to classify it.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "============================================================\n",
            "CLASSIFICATION RESULT\n",
            "============================================================\n",
            "\n",
            "Predicted Topic: Neural Networks & Deep Learning\n",
            "Confidence: 99.89%\n",
            "\n",
            "All Probabilities:\n",
            "  Neural Networks & Deep Learning      99.89% #######################################\n",
            "  Computer Vision                       0.10% \n",
            "  Reinforcement Learning                0.01% \n",
            "  Natural Language Processing           0.00% \n",
            "============================================================\n"
          ]
        }
      ],
      "source": [
        "# CLASSIFY YOUR OWN ABSTRACT - Replace the text below\n",
        "\n",
        "your_abstract = \"\"\"\n",
        "Paste your paper abstract here.\n",
        "\"\"\"\n",
        "\n",
        "result = classify_abstract(your_abstract, model, tokenizer, config)\n",
        "print_result(result)\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.14.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
