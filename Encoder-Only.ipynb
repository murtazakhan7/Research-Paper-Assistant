{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Encoder-Only Transformer for Classification\n",
        "\n",
        "This notebook implements a bidirectional encoder-only transformer (similar to BERT) for text classification tasks. Unlike decoder-only transformers (like GPT), encoder-only models use bidirectional attention, allowing each token to attend to all other tokens in the sequence.\n",
        "\n",
        "## Key Differences from Decoder-Only Transformers\n",
        "\n",
        "1. **Bidirectional Attention**: No causal masking - tokens can see both past and future tokens\n",
        "2. **Classification Task**: Trained for classification rather than next-token prediction\n",
        "3. **Pooling Strategy**: Uses CLS token or mean pooling to create sequence-level representations\n",
        "4. **No Autoregressive Generation**: Processes entire sequences at once\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from tokenizers import Tokenizer\n",
        "from tokenizers.models import WordPiece\n",
        "from tokenizers.trainers import WordPieceTrainer\n",
        "from tokenizers.pre_tokenizers import Whitespace\n",
        "import math\n",
        "import pickle\n",
        "import os\n",
        "import csv\n",
        "import json\n",
        "from datetime import datetime\n",
        "import matplotlib.pyplot as plt\n",
        "from datasets import load_dataset\n",
        "import random\n",
        "from tqdm import tqdm\n",
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "import seaborn as sns\n",
        "import pandas as pd\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Dataset Loading\n",
        "\n",
        "We'll reuse the ArXiv dataset loading functions from the decoder-only notebook. For classification, we categorize abstracts based on **topic keywords** into different research areas:\n",
        "\n",
        "- **Class 0: Neural Networks & Deep Learning** - Papers about neural networks, CNNs, deep learning architectures\n",
        "- **Class 1: Natural Language Processing** - Papers about NLP, language models, text processing\n",
        "- **Class 2: Computer Vision** - Papers about image processing, object detection, visual recognition\n",
        "- **Class 3: Reinforcement Learning** - Papers about RL, agents, policies, rewards\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# DATASET LOADING\n",
        "\n",
        "def load_arxiv_huggingface(num_samples=10000):\n",
        "    \"\"\"Load ML ArXiv papers from Hugging Face.\"\"\"\n",
        "    print(\"=\" * 60)\n",
        "    print(\"LOADING ARXIV DATASET FROM HUGGING FACE\")\n",
        "    print(\"=\" * 60)\n",
        "    print(f\"Dataset: CShorten/ML-ArXiv-Papers\")\n",
        "    print(f\"Target samples: {num_samples}\\n\")\n",
        "\n",
        "    try:\n",
        "        print(\"Downloading dataset...\")\n",
        "        dataset = load_dataset(\"CShorten/ML-ArXiv-Papers\", split=\"train\")\n",
        "        print(f\"[OK] Dataset loaded: {len(dataset)} papers available\")\n",
        "\n",
        "        abstracts = []\n",
        "        for i, paper in enumerate(dataset):\n",
        "            if len(abstracts) >= num_samples:\n",
        "                break\n",
        "            abstract = paper['abstract'].strip()\n",
        "            if 100 < len(abstract) < 5000:\n",
        "                abstract = ' '.join(abstract.split())\n",
        "                abstracts.append(abstract)\n",
        "\n",
        "        print(f\"[OK] Extracted {len(abstracts)} quality abstracts\")\n",
        "        print(f\"[OK] Average length: {sum(len(a.split()) for a in abstracts) / len(abstracts):.1f} words\")\n",
        "        print(\"=\" * 60)\n",
        "        return abstracts\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"[ERROR] Loading dataset: {e}\")\n",
        "        print(\"\\nFalling back to synthetic generation...\")\n",
        "        return generate_synthetic_abstracts(num_samples)\n",
        "\n",
        "\n",
        "def generate_synthetic_abstracts(num_samples=1000):\n",
        "    \"\"\"Generate synthetic academic abstracts as fallback.\"\"\"\n",
        "    print(\"\\n[WARNING] Generating synthetic abstracts as fallback\")\n",
        "\n",
        "    templates = [\n",
        "        \"We present a novel approach to {topic} using {method}. \"\n",
        "        \"Our experiments demonstrate that {result}. \"\n",
        "        \"The proposed framework {contribution} and achieves {performance}. \"\n",
        "        \"We analyze the {analysis} and provide {insights}. \"\n",
        "        \"Experimental results on {datasets} show that our method {comparison}.\",\n",
        "        \"This paper introduces a new method for {topic} based on {method}. \"\n",
        "        \"The key innovation is {innovation}. \"\n",
        "        \"We evaluate our approach on {datasets} and show that {result}. \"\n",
        "        \"Compared to existing methods, our approach {comparison}. \"\n",
        "        \"The main contributions include {contribution}.\",\n",
        "        \"In this work, we propose {method} for {topic}. \"\n",
        "        \"Our method {advantage} while {constraint}. \"\n",
        "        \"We demonstrate {result} through extensive experiments on {datasets}. \"\n",
        "        \"The results show that {performance}. \"\n",
        "        \"We also provide {analysis} of {insights}.\",\n",
        "    ]\n",
        "\n",
        "    components = {\n",
        "        'topic': [\"machine learning\", \"deep neural networks\", \"natural language processing\",\n",
        "                 \"computer vision\", \"reinforcement learning\", \"optimization algorithms\"],\n",
        "        'method': [\"transformer architectures\", \"self-supervised learning\", \"contrastive learning\",\n",
        "                  \"adversarial training\", \"multi-task learning\", \"few-shot learning\"],\n",
        "        'result': [\"significant improvements\", \"state-of-the-art performance\", \"competitive results\"],\n",
        "        'contribution': [\"combines multiple techniques\", \"introduces a novel loss function\",\n",
        "                        \"provides theoretical guarantees\"],\n",
        "        'performance': [\"10% improvement in accuracy\", \"better sample efficiency\", \"superior generalization\"],\n",
        "        'datasets': [\"standard benchmarks\", \"multiple datasets\", \"real-world scenarios\"],\n",
        "        'comparison': [\"outperforms previous methods\", \"achieves comparable results\", \"demonstrates robustness\"],\n",
        "        'analysis': [\"model behavior\", \"learned representations\", \"convergence properties\"],\n",
        "        'insights': [\"theoretical justification\", \"practical guidelines\", \"design principles\"],\n",
        "        'innovation': [\"a new architecture design\", \"an improved training procedure\", \"a novel technique\"],\n",
        "        'advantage': [\"improves accuracy\", \"reduces computational cost\", \"enhances interpretability\"],\n",
        "        'constraint': [\"maintaining efficiency\", \"preserving simplicity\", \"ensuring stability\"]\n",
        "    }\n",
        "\n",
        "    abstracts = []\n",
        "    for _ in range(num_samples):\n",
        "        template = random.choice(templates)\n",
        "        kwargs = {k: random.choice(v) for k, v in components.items()}\n",
        "        abstracts.append(template.format(**kwargs))\n",
        "\n",
        "    print(f\"[OK] Generated {len(abstracts)} synthetic abstracts\")\n",
        "    return abstracts\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Dataset Class for Classification\n",
        "\n",
        "The dataset class is adapted for topic-based classification. We create labels by detecting **topic keywords** in abstracts, categorizing them into research areas:\n",
        "\n",
        "| Class | Topic | Example Keywords |\n",
        "|-------|-------|------------------|\n",
        "| 0 | Neural Networks & Deep Learning | neural network, deep learning, CNN, RNN, LSTM |\n",
        "| 1 | Natural Language Processing | NLP, language model, text, word embedding, BERT |\n",
        "| 2 | Computer Vision | image, vision, object detection, segmentation |\n",
        "| 3 | Reinforcement Learning | reinforcement learning, agent, policy, reward |\n",
        "\n",
        "We also add a CLS token at the beginning of each sequence for classification.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# DATASET CLASS FOR CLASSIFICATION (TOPIC-BASED)\n",
        "\n",
        "TOPIC_CATEGORIES = {\n",
        "    0: {\n",
        "        'name': 'Neural Networks & Deep Learning',\n",
        "        'keywords': [\n",
        "            'neural network', 'deep learning', 'cnn', 'convolutional neural',\n",
        "            'rnn', 'recurrent neural', 'lstm', 'gru', 'feedforward',\n",
        "            'backpropagation', 'gradient descent', 'activation function',\n",
        "            'batch normalization', 'dropout', 'deep neural', 'mlp',\n",
        "            'perceptron', 'autoencoder', 'variational autoencoder', 'vae',\n",
        "            'generative adversarial', 'gan', 'discriminator', 'generator'\n",
        "        ]\n",
        "    },\n",
        "    1: {\n",
        "        'name': 'Natural Language Processing',\n",
        "        'keywords': [\n",
        "            'natural language', 'nlp', 'language model', 'text classification',\n",
        "            'sentiment analysis', 'named entity', 'ner', 'machine translation',\n",
        "            'word embedding', 'word2vec', 'glove', 'bert', 'gpt', 'transformer',\n",
        "            'attention mechanism', 'sequence to sequence', 'seq2seq',\n",
        "            'text generation', 'question answering', 'summarization',\n",
        "            'tokenization', 'parsing', 'syntax', 'semantic', 'corpus',\n",
        "            'vocabulary', 'embedding', 'language understanding'\n",
        "        ]\n",
        "    },\n",
        "    2: {\n",
        "        'name': 'Computer Vision',\n",
        "        'keywords': [\n",
        "            'computer vision', 'image classification', 'object detection',\n",
        "            'image segmentation', 'semantic segmentation', 'instance segmentation',\n",
        "            'face recognition', 'facial', 'pose estimation', 'image recognition',\n",
        "            'visual', 'pixel', 'convolution', 'resnet', 'vgg', 'inception',\n",
        "            'yolo', 'faster rcnn', 'mask rcnn', 'feature extraction',\n",
        "            'image processing', 'video', 'optical flow', 'depth estimation',\n",
        "            '3d reconstruction', 'scene understanding', 'visual question'\n",
        "        ]\n",
        "    },\n",
        "    3: {\n",
        "        'name': 'Reinforcement Learning',\n",
        "        'keywords': [\n",
        "            'reinforcement learning', 'rl', 'q-learning', 'deep q',\n",
        "            'dqn', 'policy gradient', 'actor critic', 'a2c', 'a3c', 'ppo',\n",
        "            'proximal policy', 'reward', 'agent', 'environment', 'mdp',\n",
        "            'markov decision', 'exploration', 'exploitation', 'epsilon greedy',\n",
        "            'value function', 'bellman', 'temporal difference', 'td learning',\n",
        "            'monte carlo', 'sarsa', 'multi-agent', 'game playing', 'control'\n",
        "        ]\n",
        "    }\n",
        "}\n",
        "\n",
        "TOPIC_NAMES = [TOPIC_CATEGORIES[i]['name'] for i in range(len(TOPIC_CATEGORIES))]\n",
        "\n",
        "\n",
        "def classify_by_topic(abstract):\n",
        "    \"\"\"Classify an abstract based on topic keywords.\"\"\"\n",
        "    abstract_lower = abstract.lower()\n",
        "    scores = {}\n",
        "    for class_id, category in TOPIC_CATEGORIES.items():\n",
        "        score = sum(1 for keyword in category['keywords'] if keyword in abstract_lower)\n",
        "        scores[class_id] = score\n",
        "    max_score = max(scores.values())\n",
        "    if max_score == 0:\n",
        "        return 0\n",
        "    return max(scores, key=scores.get)\n",
        "\n",
        "\n",
        "class ArXivClassificationDataset(Dataset):\n",
        "    \"\"\"Dataset for arXiv abstracts with topic-based classification labels.\"\"\"\n",
        "\n",
        "    def __init__(self, abstracts, tokenizer, max_len=256, num_classes=4):\n",
        "        self.abstracts = abstracts\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_len = max_len\n",
        "        self.num_classes = num_classes\n",
        "        self.cls_id = tokenizer.token_to_id(\"[CLS]\") if tokenizer.token_to_id(\"[CLS]\") is not None else tokenizer.token_to_id(\"[BOS]\")\n",
        "        self.pad_id = tokenizer.token_to_id(\"[PAD]\")\n",
        "        self.sep_id = tokenizer.token_to_id(\"[SEP]\") if tokenizer.token_to_id(\"[SEP]\") is not None else tokenizer.token_to_id(\"[EOS]\")\n",
        "        self.labels = self._create_topic_labels(abstracts)\n",
        "        self._print_class_distribution()\n",
        "\n",
        "    def _create_topic_labels(self, abstracts):\n",
        "        \"\"\"Create classification labels based on topic keywords.\"\"\"\n",
        "        return [classify_by_topic(abstract) for abstract in abstracts]\n",
        "    \n",
        "    def _print_class_distribution(self):\n",
        "        \"\"\"Print the distribution of classes.\"\"\"\n",
        "        from collections import Counter\n",
        "        distribution = Counter(self.labels)\n",
        "        print(\"\\nTopic Distribution:\")\n",
        "        for class_id in sorted(distribution.keys()):\n",
        "            count = distribution[class_id]\n",
        "            percentage = count / len(self.labels) * 100\n",
        "            print(f\"  - {TOPIC_NAMES[class_id]}: {count} ({percentage:.1f}%)\")\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.abstracts)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        text = self.abstracts[idx]\n",
        "        encoding = self.tokenizer.encode(text)\n",
        "        tokens = encoding.ids\n",
        "        if len(tokens) > self.max_len - 2:\n",
        "            tokens = tokens[:self.max_len - 2]\n",
        "        tokens = [self.cls_id] + tokens + [self.sep_id]\n",
        "        pad_len = self.max_len - len(tokens)\n",
        "        tokens = tokens + [self.pad_id] * pad_len\n",
        "        attention_mask = [1 if t != self.pad_id else 0 for t in tokens]\n",
        "        return {\n",
        "            'input_ids': torch.tensor(tokens, dtype=torch.long),\n",
        "            'attention_mask': torch.tensor(attention_mask, dtype=torch.long),\n",
        "            'label': torch.tensor(self.labels[idx], dtype=torch.long)\n",
        "        }\n",
        "\n",
        "\n",
        "def build_tokenizer(abstracts, vocab_size=8000):\n",
        "    \"\"\"Train WordPiece tokenizer from scratch with CLS and SEP tokens.\"\"\"\n",
        "    tokenizer = Tokenizer(WordPiece(unk_token=\"[UNK]\"))\n",
        "    tokenizer.pre_tokenizer = Whitespace()\n",
        "\n",
        "    trainer = WordPieceTrainer(\n",
        "        vocab_size=vocab_size,\n",
        "        special_tokens=[\"[PAD]\", \"[CLS]\", \"[SEP]\", \"[UNK]\", \"[BOS]\", \"[EOS]\"]\n",
        "    )\n",
        "\n",
        "    tokenizer.train_from_iterator(abstracts, trainer)\n",
        "    return tokenizer\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Positional Encoding\n",
        "\n",
        "The positional encoding is identical to the decoder-only transformer. It uses sinusoidal functions to encode position information.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# MODEL COMPONENTS\n",
        "\n",
        "class SinusoidalPositionalEncoding(nn.Module):\n",
        "    \"\"\"Sinusoidal positional encoding.\"\"\"\n",
        "\n",
        "    def __init__(self, d_model, max_len=256):\n",
        "        super().__init__()\n",
        "        pe = torch.zeros(max_len, d_model)\n",
        "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
        "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
        "        pe[:, 0::2] = torch.sin(position * div_term)\n",
        "        pe[:, 1::2] = torch.cos(position * div_term)\n",
        "        self.register_buffer('pe', pe.unsqueeze(0))\n",
        "\n",
        "    def forward(self, x):\n",
        "        return x + self.pe[:, :x.size(1), :]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Multi-Head Self-Attention (Bidirectional)\n",
        "\n",
        "**Key Difference from Decoder-Only**: This attention mechanism is **bidirectional** - tokens can attend to all other tokens in the sequence, not just previous ones. There is no causal masking.\n",
        "\n",
        "The attention mechanism computes:\n",
        "- Query (Q), Key (K), Value (V) matrices\n",
        "- Attention scores = QK^T / âˆšd_k\n",
        "- Apply padding mask (but NO causal mask)\n",
        "- Softmax to get attention weights\n",
        "- Weighted sum of values\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class MultiHeadAttention(nn.Module):\n",
        "    \"\"\"Bidirectional multi-head self-attention (no causal masking).\"\"\"\n",
        "\n",
        "    def __init__(self, d_model, num_heads):\n",
        "        super().__init__()\n",
        "        assert d_model % num_heads == 0\n",
        "        self.d_model = d_model\n",
        "        self.num_heads = num_heads\n",
        "        self.d_k = d_model // num_heads\n",
        "        self.W_q = nn.Linear(d_model, d_model)\n",
        "        self.W_k = nn.Linear(d_model, d_model)\n",
        "        self.W_v = nn.Linear(d_model, d_model)\n",
        "        self.W_o = nn.Linear(d_model, d_model)\n",
        "\n",
        "    def forward(self, x, mask=None):\n",
        "        \"\"\"Forward pass with optional padding mask.\"\"\"\n",
        "        batch_size, seq_len, _ = x.shape\n",
        "        Q = self.W_q(x).view(batch_size, seq_len, self.num_heads, self.d_k).transpose(1, 2)\n",
        "        K = self.W_k(x).view(batch_size, seq_len, self.num_heads, self.d_k).transpose(1, 2)\n",
        "        V = self.W_v(x).view(batch_size, seq_len, self.num_heads, self.d_k).transpose(1, 2)\n",
        "        scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(self.d_k)\n",
        "        if mask is not None:\n",
        "            mask = mask.unsqueeze(1).unsqueeze(2)\n",
        "            scores = scores.masked_fill(mask == 0, float('-inf'))\n",
        "        attn_weights = F.softmax(scores, dim=-1)\n",
        "        attn_weights = torch.nan_to_num(attn_weights)\n",
        "        attn_output = torch.matmul(attn_weights, V)\n",
        "        attn_output = attn_output.transpose(1, 2).contiguous().view(batch_size, seq_len, self.d_model)\n",
        "        output = self.W_o(attn_output)\n",
        "        return output\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Feed-Forward Network\n",
        "\n",
        "The feed-forward network is identical to the decoder-only transformer - a two-layer MLP with ReLU activation.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class FeedForward(nn.Module):\n",
        "    \"\"\"Position-wise feed-forward network.\"\"\"\n",
        "\n",
        "    def __init__(self, d_model, d_ff):\n",
        "        super().__init__()\n",
        "        self.linear1 = nn.Linear(d_model, d_ff)\n",
        "        self.linear2 = nn.Linear(d_ff, d_model)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.linear2(F.relu(self.linear1(x)))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Encoder Block\n",
        "\n",
        "The encoder block combines:\n",
        "1. **Bidirectional Multi-Head Attention** (no causal masking)\n",
        "2. **Layer Normalization** with residual connection\n",
        "3. **Feed-Forward Network**\n",
        "4. **Layer Normalization** with residual connection\n",
        "5. **Dropout** for regularization\n",
        "\n",
        "This is similar to the decoder block, but uses bidirectional attention instead of masked attention.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class EncoderBlock(nn.Module):\n",
        "    \"\"\"Single encoder block with bidirectional attention.\"\"\"\n",
        "\n",
        "    def __init__(self, d_model, num_heads, d_ff, dropout=0.1):\n",
        "        super().__init__()\n",
        "        self.attention = MultiHeadAttention(d_model, num_heads)\n",
        "        self.norm1 = nn.LayerNorm(d_model)\n",
        "        self.ffn = FeedForward(d_model, d_ff)\n",
        "        self.norm2 = nn.LayerNorm(d_model)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x, mask=None):\n",
        "        \"\"\"Forward pass with attention and feed-forward layers.\"\"\"\n",
        "        attn_output = self.attention(x, mask)\n",
        "        x = self.norm1(x + self.dropout(attn_output))\n",
        "        ffn_output = self.ffn(x)\n",
        "        x = self.norm2(x + self.dropout(ffn_output))\n",
        "        return x\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Encoder-Only Transformer\n",
        "\n",
        "The complete encoder-only transformer model includes:\n",
        "\n",
        "1. **Token Embeddings**: Maps token IDs to dense vectors\n",
        "2. **Positional Encoding**: Adds position information\n",
        "3. **Stack of Encoder Blocks**: Multiple layers of bidirectional attention\n",
        "4. **Pooling Layer**: Extracts sequence-level representation\n",
        "   - **CLS token pooling**: Uses the first token (CLS) representation\n",
        "   - **Mean pooling**: Averages all token representations\n",
        "5. **Classification Head**: Linear layer mapping to number of classes\n",
        "\n",
        "**Key Architectural Choices**:\n",
        "- All hyperparameters are configurable (d_model, num_layers, num_heads, etc.)\n",
        "- Supports both CLS token and mean pooling strategies\n",
        "- Designed for classification tasks\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class EncoderOnlyTransformer(nn.Module):\n",
        "    \"\"\"Encoder-only Transformer for classification tasks.\"\"\"\n",
        "\n",
        "    def __init__(self, vocab_size, d_model=256, num_layers=4, num_heads=8,\n",
        "                 d_ff=1024, max_seq_len=256, num_classes=3, dropout=0.1, pooling_type='cls'):\n",
        "        \"\"\"Initialize encoder transformer with configurable architecture.\"\"\"\n",
        "        super().__init__()\n",
        "        self.d_model = d_model\n",
        "        self.vocab_size = vocab_size\n",
        "        self.num_classes = num_classes\n",
        "        self.pooling_type = pooling_type\n",
        "        self.token_embedding = nn.Embedding(vocab_size, d_model)\n",
        "        self.pos_encoding = SinusoidalPositionalEncoding(d_model, max_seq_len)\n",
        "        self.encoder_blocks = nn.ModuleList([\n",
        "            EncoderBlock(d_model, num_heads, d_ff, dropout)\n",
        "            for _ in range(num_layers)\n",
        "        ])\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(d_model, num_classes)\n",
        "        )\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self._init_weights()\n",
        "\n",
        "    def _init_weights(self):\n",
        "        \"\"\"Initialize weights using Xavier uniform.\"\"\"\n",
        "        for p in self.parameters():\n",
        "            if p.dim() > 1:\n",
        "                nn.init.xavier_uniform_(p)\n",
        "\n",
        "    def forward(self, input_ids, attention_mask=None):\n",
        "        \"\"\"Forward pass returning classification logits.\"\"\"\n",
        "        x = self.token_embedding(input_ids)\n",
        "        x = self.pos_encoding(x)\n",
        "        x = self.dropout(x)\n",
        "        for encoder_block in self.encoder_blocks:\n",
        "            x = encoder_block(x, attention_mask)\n",
        "        if self.pooling_type == 'cls':\n",
        "            pooled = x[:, 0, :]\n",
        "        else:\n",
        "            if attention_mask is not None:\n",
        "                mask_expanded = attention_mask.unsqueeze(-1).float()\n",
        "                x_masked = x * mask_expanded\n",
        "                sum_pooled = x_masked.sum(dim=1)\n",
        "                lengths = attention_mask.sum(dim=1, keepdim=True).float()\n",
        "                pooled = sum_pooled / lengths\n",
        "            else:\n",
        "                pooled = x.mean(dim=1)\n",
        "        logits = self.classifier(pooled)\n",
        "        return logits\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# This cell is kept for reference but config is defined near main() for easy editing\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Training Functions\n",
        "\n",
        "Training functions for classification tasks. We use CrossEntropyLoss for multi-class classification and track both loss and accuracy metrics.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# TRAINING FUNCTIONS\n",
        "\n",
        "def train_epoch(model, dataloader, optimizer, device):\n",
        "    \"\"\"Train for one epoch.\"\"\"\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    total_correct = 0\n",
        "    total_samples = 0\n",
        "    batch_losses = []\n",
        "    progress_bar = tqdm(dataloader, desc=\"Training\", leave=False)\n",
        "\n",
        "    for batch in progress_bar:\n",
        "        input_ids = batch['input_ids'].to(device)\n",
        "        attention_mask = batch['attention_mask'].to(device)\n",
        "        labels = batch['label'].to(device)\n",
        "        logits = model(input_ids, attention_mask)\n",
        "        loss = F.cross_entropy(logits, labels)\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "        optimizer.step()\n",
        "        predictions = torch.argmax(logits, dim=-1)\n",
        "        correct = (predictions == labels).sum().item()\n",
        "        total_correct += correct\n",
        "        total_samples += labels.size(0)\n",
        "        total_loss += loss.item()\n",
        "        batch_losses.append(loss.item())\n",
        "        accuracy = correct / labels.size(0)\n",
        "        progress_bar.set_postfix({'loss': f'{loss.item():.4f}', 'acc': f'{accuracy:.4f}'})\n",
        "\n",
        "    avg_loss = total_loss / len(dataloader)\n",
        "    avg_accuracy = total_correct / total_samples\n",
        "    return avg_loss, avg_accuracy, batch_losses\n",
        "\n",
        "\n",
        "def evaluate_model(model, dataloader, device):\n",
        "    \"\"\"Evaluate model on validation set.\"\"\"\n",
        "    model.eval()\n",
        "    total_loss = 0\n",
        "    total_correct = 0\n",
        "    total_samples = 0\n",
        "    all_predictions = []\n",
        "    all_labels = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in dataloader:\n",
        "            input_ids = batch['input_ids'].to(device)\n",
        "            attention_mask = batch['attention_mask'].to(device)\n",
        "            labels = batch['label'].to(device)\n",
        "            logits = model(input_ids, attention_mask)\n",
        "            loss = F.cross_entropy(logits, labels)\n",
        "            predictions = torch.argmax(logits, dim=-1)\n",
        "            correct = (predictions == labels).sum().item()\n",
        "            total_correct += correct\n",
        "            total_samples += labels.size(0)\n",
        "            total_loss += loss.item()\n",
        "            all_predictions.extend(predictions.cpu().numpy())\n",
        "            all_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "    avg_loss = total_loss / len(dataloader)\n",
        "    avg_accuracy = total_correct / total_samples\n",
        "    return avg_loss, avg_accuracy, all_predictions, all_labels\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Visualization Functions\n",
        "\n",
        "Visualization functions for training curves, accuracy metrics, and confusion matrices.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# VISUALIZATION FUNCTIONS\n",
        "\n",
        "def plot_training_curves(train_losses, train_accuracies, val_losses=None,\n",
        "                         val_accuracies=None, save_dir=None):\n",
        "    \"\"\"Plot training curves and save to files.\"\"\"\n",
        "    fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
        "    fig.suptitle('Encoder-Only Transformer Training Curves', fontsize=16, fontweight='bold')\n",
        "    epochs = range(1, len(train_losses) + 1)\n",
        "\n",
        "    # Loss plot\n",
        "    ax1 = axes[0, 0]\n",
        "    ax1.plot(epochs, train_losses, 'b-o', linewidth=2, markersize=6, label='Training Loss')\n",
        "    if val_losses is not None:\n",
        "        ax1.plot(epochs, val_losses, 'r-s', linewidth=2, markersize=6, label='Validation Loss')\n",
        "    ax1.set_xlabel('Epoch', fontsize=12)\n",
        "    ax1.set_ylabel('Loss', fontsize=12)\n",
        "    ax1.set_title('Training Loss per Epoch', fontsize=13, fontweight='bold')\n",
        "    ax1.grid(True, alpha=0.3)\n",
        "    ax1.legend()\n",
        "\n",
        "    # Accuracy plot\n",
        "    ax2 = axes[0, 1]\n",
        "    ax2.plot(epochs, train_accuracies, 'g-o', linewidth=2, markersize=6, label='Training Accuracy')\n",
        "    if val_accuracies is not None:\n",
        "        ax2.plot(epochs, val_accuracies, 'm-s', linewidth=2, markersize=6, label='Validation Accuracy')\n",
        "    ax2.set_xlabel('Epoch', fontsize=12)\n",
        "    ax2.set_ylabel('Accuracy', fontsize=12)\n",
        "    ax2.set_title('Training Accuracy per Epoch', fontsize=13, fontweight='bold')\n",
        "    ax2.grid(True, alpha=0.3)\n",
        "    ax2.legend()\n",
        "    ax2.set_ylim([0, 1])\n",
        "\n",
        "    # Loss log scale\n",
        "    ax3 = axes[1, 0]\n",
        "    ax3.semilogy(epochs, train_losses, 'b-o', linewidth=2, markersize=6, label='Training Loss')\n",
        "    if val_losses is not None:\n",
        "        ax3.semilogy(epochs, val_losses, 'r-s', linewidth=2, markersize=6, label='Validation Loss')\n",
        "    ax3.set_xlabel('Epoch', fontsize=12)\n",
        "    ax3.set_ylabel('Loss (log scale)', fontsize=12)\n",
        "    ax3.set_title('Loss Convergence (Log Scale)', fontsize=13, fontweight='bold')\n",
        "    ax3.grid(True, alpha=0.3, which='both')\n",
        "    ax3.legend()\n",
        "\n",
        "    # Accuracy comparison\n",
        "    ax4 = axes[1, 1]\n",
        "    if val_accuracies is not None:\n",
        "        ax4.plot(epochs, train_accuracies, 'g-o', linewidth=2, markersize=6, label='Training Accuracy', alpha=0.7)\n",
        "        ax4.plot(epochs, val_accuracies, 'm-s', linewidth=2, markersize=6, label='Validation Accuracy')\n",
        "        ax4.fill_between(epochs, train_accuracies, val_accuracies, alpha=0.2)\n",
        "    else:\n",
        "        ax4.plot(epochs, train_accuracies, 'g-o', linewidth=2, markersize=6, label='Training Accuracy')\n",
        "    ax4.set_xlabel('Epoch', fontsize=12)\n",
        "    ax4.set_ylabel('Accuracy', fontsize=12)\n",
        "    ax4.set_title('Accuracy Over Time', fontsize=13, fontweight='bold')\n",
        "    ax4.grid(True, alpha=0.3)\n",
        "    ax4.legend()\n",
        "    ax4.set_ylim([0, 1])\n",
        "\n",
        "    plt.tight_layout()\n",
        "\n",
        "    if save_dir:\n",
        "        curves_path = os.path.join(save_dir, \"training_curves.png\")\n",
        "        plt.savefig(curves_path, dpi=300, bbox_inches='tight')\n",
        "        print(f\"[OK] Training curves saved to {curves_path}\")\n",
        "\n",
        "    plt.show()\n",
        "\n",
        "    # Print statistics\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"TRAINING STATISTICS\")\n",
        "    print(\"=\"*60)\n",
        "    print(f\"Initial Loss: {train_losses[0]:.4f}\")\n",
        "    print(f\"Final Loss: {train_losses[-1]:.4f}\")\n",
        "    print(f\"Loss Reduction: {((train_losses[0] - train_losses[-1]) / train_losses[0] * 100):.2f}%\")\n",
        "    print(f\"Best Loss: {min(train_losses):.4f} (Epoch {train_losses.index(min(train_losses)) + 1})\")\n",
        "    print(f\"\\nInitial Accuracy: {train_accuracies[0]:.4f}\")\n",
        "    print(f\"Final Accuracy: {train_accuracies[-1]:.4f}\")\n",
        "    print(f\"Accuracy Improvement: {((train_accuracies[-1] - train_accuracies[0]) * 100):.2f}%\")\n",
        "    if val_accuracies:\n",
        "        print(f\"Best Validation Accuracy: {max(val_accuracies):.4f} (Epoch {val_accuracies.index(max(val_accuracies)) + 1})\")\n",
        "    print(\"=\"*60)\n",
        "\n",
        "\n",
        "def plot_confusion_matrix(y_true, y_pred, num_classes, class_names=None, save_dir=None):\n",
        "    \"\"\"Plot confusion matrix and save to file.\"\"\"\n",
        "    cm = confusion_matrix(y_true, y_pred)\n",
        "    \n",
        "    plt.figure(figsize=(10, 8))\n",
        "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
        "                xticklabels=class_names or [f'Class {i}' for i in range(num_classes)],\n",
        "                yticklabels=class_names or [f'Class {i}' for i in range(num_classes)])\n",
        "    plt.title('Confusion Matrix', fontsize=14, fontweight='bold')\n",
        "    plt.ylabel('True Label', fontsize=12)\n",
        "    plt.xlabel('Predicted Label', fontsize=12)\n",
        "    plt.tight_layout()\n",
        "    \n",
        "    if save_dir:\n",
        "        cm_path = os.path.join(save_dir, \"confusion_matrix.png\")\n",
        "        plt.savefig(cm_path, dpi=300, bbox_inches='tight')\n",
        "        print(f\"[OK] Confusion matrix saved to {cm_path}\")\n",
        "    \n",
        "    plt.show()\n",
        "    \n",
        "    # Print classification report\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"CLASSIFICATION REPORT\")\n",
        "    print(\"=\"*60)\n",
        "    report = classification_report(y_true, y_pred, \n",
        "                                   target_names=class_names or [f'Class {i}' for i in range(num_classes)])\n",
        "    print(report)\n",
        "    print(\"=\"*60)\n",
        "    \n",
        "    return cm\n",
        "\n",
        "\n",
        "def save_training_metrics_csv(save_dir, train_losses, train_accuracies, val_losses, val_accuracies, config):\n",
        "    \"\"\"Save all training metrics to CSV files for report generation.\"\"\"\n",
        "    # Epoch-by-epoch metrics\n",
        "    epochs_data = {\n",
        "        'epoch': list(range(1, len(train_losses) + 1)),\n",
        "        'train_loss': train_losses,\n",
        "        'train_accuracy': train_accuracies,\n",
        "        'val_loss': val_losses,\n",
        "        'val_accuracy': val_accuracies\n",
        "    }\n",
        "    df_epochs = pd.DataFrame(epochs_data)\n",
        "    epochs_csv = os.path.join(save_dir, \"training_metrics_epochs.csv\")\n",
        "    df_epochs.to_csv(epochs_csv, index=False)\n",
        "    print(f\"[OK] Epoch metrics saved to {epochs_csv}\")\n",
        "    \n",
        "    # Summary statistics\n",
        "    summary_data = {\n",
        "        'metric': [\n",
        "            'initial_train_loss', 'final_train_loss', 'best_train_loss', 'best_train_loss_epoch',\n",
        "            'initial_train_accuracy', 'final_train_accuracy', 'best_train_accuracy', 'best_train_accuracy_epoch',\n",
        "            'initial_val_loss', 'final_val_loss', 'best_val_loss', 'best_val_loss_epoch',\n",
        "            'initial_val_accuracy', 'final_val_accuracy', 'best_val_accuracy', 'best_val_accuracy_epoch',\n",
        "            'loss_reduction_pct', 'accuracy_improvement_pct', 'total_epochs'\n",
        "        ],\n",
        "        'value': [\n",
        "            train_losses[0], train_losses[-1], min(train_losses), train_losses.index(min(train_losses)) + 1,\n",
        "            train_accuracies[0], train_accuracies[-1], max(train_accuracies), train_accuracies.index(max(train_accuracies)) + 1,\n",
        "            val_losses[0], val_losses[-1], min(val_losses), val_losses.index(min(val_losses)) + 1,\n",
        "            val_accuracies[0], val_accuracies[-1], max(val_accuracies), val_accuracies.index(max(val_accuracies)) + 1,\n",
        "            (train_losses[0] - train_losses[-1]) / train_losses[0] * 100,\n",
        "            (train_accuracies[-1] - train_accuracies[0]) * 100,\n",
        "            len(train_losses)\n",
        "        ]\n",
        "    }\n",
        "    df_summary = pd.DataFrame(summary_data)\n",
        "    summary_csv = os.path.join(save_dir, \"training_summary.csv\")\n",
        "    df_summary.to_csv(summary_csv, index=False)\n",
        "    print(f\"[OK] Summary statistics saved to {summary_csv}\")\n",
        "    \n",
        "    # Config as CSV\n",
        "    config_data = {'parameter': list(config.keys()), 'value': list(config.values())}\n",
        "    df_config = pd.DataFrame(config_data)\n",
        "    config_csv = os.path.join(save_dir, \"config.csv\")\n",
        "    df_config.to_csv(config_csv, index=False)\n",
        "    print(f\"[OK] Config saved to {config_csv}\")\n",
        "    \n",
        "    return epochs_csv, summary_csv, config_csv\n",
        "\n",
        "\n",
        "def save_classification_report_csv(save_dir, y_true, y_pred, class_names):\n",
        "    \"\"\"Save classification report as CSV.\"\"\"\n",
        "    report_dict = classification_report(y_true, y_pred, target_names=class_names, output_dict=True)\n",
        "    df_report = pd.DataFrame(report_dict).transpose()\n",
        "    report_csv = os.path.join(save_dir, \"classification_report.csv\")\n",
        "    df_report.to_csv(report_csv)\n",
        "    print(f\"[OK] Classification report saved to {report_csv}\")\n",
        "    \n",
        "    # Save confusion matrix as CSV\n",
        "    cm = confusion_matrix(y_true, y_pred)\n",
        "    df_cm = pd.DataFrame(cm, index=class_names, columns=class_names)\n",
        "    cm_csv = os.path.join(save_dir, \"confusion_matrix.csv\")\n",
        "    df_cm.to_csv(cm_csv)\n",
        "    print(f\"[OK] Confusion matrix CSV saved to {cm_csv}\")\n",
        "    \n",
        "    return report_csv, cm_csv\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Model Saving Functions\n",
        "\n",
        "Functions to save the trained model with a timestamped folder and metadata file containing training information.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# MODEL SAVING FUNCTIONS\n",
        "\n",
        "def get_model_save_dir(base_dir=\"models\"):\n",
        "    \"\"\"Generate a timestamped directory name for saving the model.\"\"\"\n",
        "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "    save_dir = os.path.join(base_dir, f\"encoder_{timestamp}\")\n",
        "    return save_dir\n",
        "\n",
        "\n",
        "def save_model(model, tokenizer, config, save_dir, training_history=None, \n",
        "               best_epoch=None, best_val_accuracy=None):\n",
        "    \"\"\"Save the trained model, tokenizer, config, and metadata to a timestamped folder.\"\"\"\n",
        "    os.makedirs(save_dir, exist_ok=True)\n",
        "    \n",
        "    model_path = os.path.join(save_dir, \"model.pt\")\n",
        "    torch.save(model.state_dict(), model_path)\n",
        "    print(f\"[OK] Model weights saved to {model_path}\")\n",
        "    \n",
        "    tokenizer_path = os.path.join(save_dir, \"tokenizer.json\")\n",
        "    tokenizer.save(tokenizer_path)\n",
        "    print(f\"[OK] Tokenizer saved to {tokenizer_path}\")\n",
        "    \n",
        "    config_path = os.path.join(save_dir, \"config.json\")\n",
        "    with open(config_path, 'w') as f:\n",
        "        json.dump(config, f, indent=2)\n",
        "    print(f\"[OK] Config saved to {config_path}\")\n",
        "    \n",
        "    metadata = {\n",
        "        \"model_type\": \"EncoderOnlyTransformer\",\n",
        "        \"task\": \"Topic Classification\",\n",
        "        \"classes\": TOPIC_NAMES,\n",
        "        \"num_classes\": len(TOPIC_NAMES),\n",
        "        \"created_at\": datetime.now().isoformat(),\n",
        "        \"architecture\": {\n",
        "            \"vocab_size\": config['vocab_size'],\n",
        "            \"d_model\": config['d_model'],\n",
        "            \"num_layers\": config['num_layers'],\n",
        "            \"num_heads\": config['num_heads'],\n",
        "            \"d_ff\": config['d_ff'],\n",
        "            \"max_seq_len\": config['max_seq_len'],\n",
        "            \"dropout\": config['dropout'],\n",
        "            \"pooling_type\": config['pooling_type']\n",
        "        },\n",
        "        \"training\": {\n",
        "            \"num_epochs\": config['num_epochs'],\n",
        "            \"batch_size\": config['batch_size'],\n",
        "            \"learning_rate\": config['learning_rate'],\n",
        "            \"weight_decay\": config['weight_decay'],\n",
        "            \"num_samples\": config['num_samples'],\n",
        "            \"train_split\": config['train_split']\n",
        "        },\n",
        "        \"num_parameters\": sum(p.numel() for p in model.parameters()),\n",
        "        \"model_size_mb\": sum(p.numel() for p in model.parameters()) * 4 / 1024 / 1024\n",
        "    }\n",
        "    \n",
        "    if best_epoch is not None:\n",
        "        metadata[\"best_epoch\"] = best_epoch\n",
        "    if best_val_accuracy is not None:\n",
        "        metadata[\"best_val_accuracy\"] = best_val_accuracy\n",
        "    \n",
        "    if training_history is not None:\n",
        "        metadata[\"training_history\"] = {\n",
        "            \"train_losses\": training_history.get('train_losses', []),\n",
        "            \"train_accuracies\": training_history.get('train_accuracies', []),\n",
        "            \"val_losses\": training_history.get('val_losses', []),\n",
        "            \"val_accuracies\": training_history.get('val_accuracies', [])\n",
        "        }\n",
        "        if training_history.get('train_losses'):\n",
        "            metadata[\"final_train_loss\"] = training_history['train_losses'][-1]\n",
        "        if training_history.get('train_accuracies'):\n",
        "            metadata[\"final_train_accuracy\"] = training_history['train_accuracies'][-1]\n",
        "        if training_history.get('val_losses'):\n",
        "            metadata[\"final_val_loss\"] = training_history['val_losses'][-1]\n",
        "        if training_history.get('val_accuracies'):\n",
        "            metadata[\"final_val_accuracy\"] = training_history['val_accuracies'][-1]\n",
        "    \n",
        "    metadata_path = os.path.join(save_dir, \"metadata.json\")\n",
        "    with open(metadata_path, 'w') as f:\n",
        "        json.dump(metadata, f, indent=2)\n",
        "    print(f\"[OK] Metadata saved to {metadata_path}\")\n",
        "    \n",
        "    print(f\"\\n[OK] All files saved to: {save_dir}\")\n",
        "    return save_dir\n",
        "\n",
        "\n",
        "def load_saved_model(save_dir, device='cuda'):\n",
        "    \"\"\"Load a saved model from a timestamped directory.\"\"\"\n",
        "    print(f\"Loading model from {save_dir}...\")\n",
        "    \n",
        "    config_path = os.path.join(save_dir, \"config.json\")\n",
        "    with open(config_path, 'r') as f:\n",
        "        config = json.load(f)\n",
        "    print(f\"[OK] Config loaded\")\n",
        "    \n",
        "    tokenizer_path = os.path.join(save_dir, \"tokenizer.json\")\n",
        "    tokenizer = Tokenizer.from_file(tokenizer_path)\n",
        "    print(f\"[OK] Tokenizer loaded\")\n",
        "    \n",
        "    metadata_path = os.path.join(save_dir, \"metadata.json\")\n",
        "    with open(metadata_path, 'r') as f:\n",
        "        metadata = json.load(f)\n",
        "    print(f\"[OK] Metadata loaded\")\n",
        "    \n",
        "    model = EncoderOnlyTransformer(\n",
        "        vocab_size=tokenizer.get_vocab_size(),\n",
        "        d_model=config['d_model'],\n",
        "        num_layers=config['num_layers'],\n",
        "        num_heads=config['num_heads'],\n",
        "        d_ff=config['d_ff'],\n",
        "        max_seq_len=config['max_seq_len'],\n",
        "        num_classes=config['num_classes'],\n",
        "        dropout=config['dropout'],\n",
        "        pooling_type=config['pooling_type']\n",
        "    )\n",
        "    \n",
        "    model_path = os.path.join(save_dir, \"model.pt\")\n",
        "    model.load_state_dict(torch.load(model_path, map_location=device))\n",
        "    model = model.to(device)\n",
        "    model.eval()\n",
        "    print(f\"[OK] Model weights loaded\")\n",
        "    \n",
        "    print(f\"\\n[OK] Model loaded successfully!\")\n",
        "    print(f\"  - Created: {metadata.get('created_at', 'Unknown')}\")\n",
        "    print(f\"  - Best Epoch: {metadata.get('best_epoch', 'Unknown')}\")\n",
        "    best_val = metadata.get('best_val_accuracy', 0)\n",
        "    print(f\"  - Best Val Accuracy: {best_val:.4f}\" if best_val else \"  - Best Val Accuracy: Unknown\")\n",
        "    \n",
        "    return model, tokenizer, config, metadata\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# CONFIGURATION - Edit these values and re-run to train with different settings\n",
        "\n",
        "def get_default_config():\n",
        "    \"\"\"Get default configuration with all hyperparameters.\"\"\"\n",
        "    return {\n",
        "        'd_model': 256,           # embedding dimension\n",
        "        'num_layers': 4,          # number of encoder blocks\n",
        "        'num_heads': 8,           # number of attention heads\n",
        "        'd_ff': 1024,             # feed-forward dimension\n",
        "        'max_seq_len': 256,       # maximum sequence length\n",
        "        'vocab_size': 8000,       # vocabulary size\n",
        "        'num_classes': 4,         # number of topic classes (NN, NLP, CV, RL)\n",
        "        'dropout': 0.1,           # dropout rate\n",
        "        'pooling_type': 'cls',    # 'cls' or 'mean' pooling\n",
        "        'batch_size': 32,         # batch size\n",
        "        'num_epochs': 20,         # number of training epochs\n",
        "        'learning_rate': 3e-4,    # learning rate\n",
        "        'weight_decay': 0.01,     # weight decay for regularization\n",
        "        'num_samples': 10000,     # number of samples to load\n",
        "        'train_split': 0.9,       # train/validation split ratio\n",
        "    }\n",
        "\n",
        "\n",
        "def print_config(config):\n",
        "    \"\"\"Print configuration in a readable format.\"\"\"\n",
        "    print(\"=\" * 60)\n",
        "    print(\"MODEL CONFIGURATION\")\n",
        "    print(\"=\" * 60)\n",
        "    print(\"\\nArchitecture:\")\n",
        "    print(f\"  - Embedding Dimension (d_model): {config['d_model']}\")\n",
        "    print(f\"  - Number of Layers: {config['num_layers']}\")\n",
        "    print(f\"  - Number of Attention Heads: {config['num_heads']}\")\n",
        "    print(f\"  - Feed-Forward Dimension: {config['d_ff']}\")\n",
        "    print(f\"  - Max Sequence Length: {config['max_seq_len']}\")\n",
        "    print(f\"  - Vocabulary Size: {config['vocab_size']}\")\n",
        "    print(f\"  - Number of Classes: {config['num_classes']}\")\n",
        "    print(f\"  - Dropout Rate: {config['dropout']}\")\n",
        "    print(f\"  - Pooling Type: {config['pooling_type']}\")\n",
        "    print(\"\\nTraining:\")\n",
        "    print(f\"  - Batch Size: {config['batch_size']}\")\n",
        "    print(f\"  - Number of Epochs: {config['num_epochs']}\")\n",
        "    print(f\"  - Learning Rate: {config['learning_rate']}\")\n",
        "    print(f\"  - Weight Decay: {config['weight_decay']}\")\n",
        "    print(\"\\nDataset:\")\n",
        "    print(f\"  - Number of Samples: {config['num_samples']}\")\n",
        "    print(f\"  - Train Split: {config['train_split'] * 100:.0f}%\")\n",
        "    print(\"=\" * 60)\n",
        "\n",
        "\n",
        "# MAIN TRAINING FUNCTION\n",
        "\n",
        "def main():\n",
        "    \"\"\"Main training function for encoder-only transformer.\"\"\"\n",
        "    config = get_default_config()\n",
        "    print_config(config)\n",
        "\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    print(f\"\\nUsing device: {device}\\n\")\n",
        "\n",
        "    # Load dataset\n",
        "    print(\"=\"*60)\n",
        "    print(\"LOADING ARXIV DATASET\")\n",
        "    print(\"=\"*60 + \"\\n\")\n",
        "\n",
        "    all_abstracts = load_arxiv_huggingface(num_samples=config['num_samples'])\n",
        "\n",
        "    # Split data\n",
        "    random.shuffle(all_abstracts)\n",
        "    split_idx = int(config['train_split'] * len(all_abstracts))\n",
        "    train_abstracts = all_abstracts[:split_idx]\n",
        "    val_abstracts = all_abstracts[split_idx:]\n",
        "\n",
        "    print(f\"\\n[OK] Dataset split:\")\n",
        "    print(f\"  Training: {len(train_abstracts)} abstracts\")\n",
        "    print(f\"  Validation: {len(val_abstracts)} abstracts\")\n",
        "\n",
        "    # Build tokenizer\n",
        "    print(\"\\nTraining WordPiece tokenizer...\")\n",
        "    tokenizer = build_tokenizer(train_abstracts, vocab_size=config['vocab_size'])\n",
        "    print(f\"[OK] Vocabulary size: {tokenizer.get_vocab_size()}\")\n",
        "\n",
        "    # Create datasets\n",
        "    train_dataset = ArXivClassificationDataset(\n",
        "        train_abstracts, tokenizer, \n",
        "        max_len=config['max_seq_len'],\n",
        "        num_classes=config['num_classes']\n",
        "    )\n",
        "    val_dataset = ArXivClassificationDataset(\n",
        "        val_abstracts, tokenizer,\n",
        "        max_len=config['max_seq_len'],\n",
        "        num_classes=config['num_classes']\n",
        "    )\n",
        "\n",
        "    train_loader = DataLoader(train_dataset, batch_size=config['batch_size'], shuffle=True)\n",
        "    val_loader = DataLoader(val_dataset, batch_size=config['batch_size'], shuffle=False)\n",
        "\n",
        "    # Initialize model\n",
        "    model = EncoderOnlyTransformer(\n",
        "        vocab_size=tokenizer.get_vocab_size(),\n",
        "        d_model=config['d_model'],\n",
        "        num_layers=config['num_layers'],\n",
        "        num_heads=config['num_heads'],\n",
        "        d_ff=config['d_ff'],\n",
        "        max_seq_len=config['max_seq_len'],\n",
        "        num_classes=config['num_classes'],\n",
        "        dropout=config['dropout'],\n",
        "        pooling_type=config['pooling_type']\n",
        "    ).to(device)\n",
        "\n",
        "    num_params = sum(p.numel() for p in model.parameters())\n",
        "    print(f\"\\n[OK] Model parameters: {num_params:,}\")\n",
        "    print(f\"[OK] Model size: ~{num_params * 4 / 1024 / 1024:.2f} MB\")\n",
        "\n",
        "    # Training setup\n",
        "    optimizer = torch.optim.AdamW(\n",
        "        model.parameters(), \n",
        "        lr=config['learning_rate'],\n",
        "        weight_decay=config['weight_decay']\n",
        "    )\n",
        "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
        "        optimizer, mode='min', factor=0.5, patience=3\n",
        "    )\n",
        "\n",
        "    # Training loop\n",
        "    import time\n",
        "    train_losses = []\n",
        "    train_accuracies = []\n",
        "    val_losses = []\n",
        "    val_accuracies = []\n",
        "    best_val_accuracy = 0.0\n",
        "    best_epoch = 0\n",
        "\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"STARTING TRAINING\")\n",
        "    print(\"=\"*60 + \"\\n\")\n",
        "\n",
        "    for epoch in range(config['num_epochs']):\n",
        "        epoch_start_time = time.time()\n",
        "\n",
        "        train_loss, train_acc, batch_losses = train_epoch(model, train_loader, optimizer, device)\n",
        "        train_losses.append(train_loss)\n",
        "        train_accuracies.append(train_acc)\n",
        "\n",
        "        val_loss, val_acc, val_predictions, val_labels = evaluate_model(model, val_loader, device)\n",
        "        val_losses.append(val_loss)\n",
        "        val_accuracies.append(val_acc)\n",
        "\n",
        "        scheduler.step(val_loss)\n",
        "        epoch_time = time.time() - epoch_start_time\n",
        "\n",
        "        print(f\"Epoch {epoch+1}/{config['num_epochs']}:\")\n",
        "        print(f\"  Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f}\")\n",
        "        print(f\"  Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}\")\n",
        "        print(f\"  Time: {epoch_time:.1f}s\")\n",
        "\n",
        "        if val_acc > best_val_accuracy:\n",
        "            best_val_accuracy = val_acc\n",
        "            best_epoch = epoch + 1\n",
        "            print(f\"  [NEW BEST] Epoch {best_epoch}, Val Acc: {best_val_accuracy:.4f}\")\n",
        "\n",
        "        print()\n",
        "\n",
        "    # Create save directory first for saving graphs and CSVs\n",
        "    save_dir = get_model_save_dir()\n",
        "    os.makedirs(save_dir, exist_ok=True)\n",
        "\n",
        "    # Save training curves and metrics\n",
        "    print(\"\\nGenerating and saving training curves...\")\n",
        "    plot_training_curves(train_losses, train_accuracies, val_losses, val_accuracies, save_dir=save_dir)\n",
        "\n",
        "    print(\"\\nGenerating and saving confusion matrix...\")\n",
        "    plot_confusion_matrix(val_labels, val_predictions, config['num_classes'], TOPIC_NAMES, save_dir=save_dir)\n",
        "\n",
        "    # Save all metrics as CSV for reports\n",
        "    print(\"\\nSaving training metrics to CSV files...\")\n",
        "    save_training_metrics_csv(save_dir, train_losses, train_accuracies, val_losses, val_accuracies, config)\n",
        "    save_classification_report_csv(save_dir, val_labels, val_predictions, TOPIC_NAMES)\n",
        "\n",
        "    # Save model\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"SAVING MODEL\")\n",
        "    print(\"=\"*60)\n",
        "    \n",
        "    training_history = {\n",
        "        'train_losses': train_losses,\n",
        "        'train_accuracies': train_accuracies,\n",
        "        'val_losses': val_losses,\n",
        "        'val_accuracies': val_accuracies\n",
        "    }\n",
        "    save_model(\n",
        "        model=model,\n",
        "        tokenizer=tokenizer,\n",
        "        config=config,\n",
        "        save_dir=save_dir,\n",
        "        training_history=training_history,\n",
        "        best_epoch=best_epoch,\n",
        "        best_val_accuracy=best_val_accuracy\n",
        "    )\n",
        "\n",
        "    # Final summary\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"TRAINING COMPLETE!\")\n",
        "    print(\"=\"*60)\n",
        "    print(f\"\\nFinal Results:\")\n",
        "    print(f\"  Best Model: Epoch {best_epoch} (Val Accuracy: {best_val_accuracy:.4f})\")\n",
        "    print(f\"  Final Train Loss: {train_losses[-1]:.4f}\")\n",
        "    print(f\"  Final Train Accuracy: {train_accuracies[-1]:.4f}\")\n",
        "    print(f\"  Final Val Accuracy: {val_accuracies[-1]:.4f}\")\n",
        "    print(f\"  Model saved to: {save_dir}\")\n",
        "    print(\"\\nSaved files:\")\n",
        "    print(f\"  - model.pt (model weights)\")\n",
        "    print(f\"  - tokenizer.json (trained tokenizer)\")\n",
        "    print(f\"  - config.json (model configuration)\")\n",
        "    print(f\"  - metadata.json (training metadata)\")\n",
        "    print(f\"  - training_curves.png (training visualization)\")\n",
        "    print(f\"  - confusion_matrix.png (classification matrix)\")\n",
        "    print(f\"  - training_metrics_epochs.csv (per-epoch metrics)\")\n",
        "    print(f\"  - training_summary.csv (summary statistics)\")\n",
        "    print(f\"  - config.csv (configuration)\")\n",
        "    print(f\"  - classification_report.csv (classification metrics)\")\n",
        "    print(f\"  - confusion_matrix.csv (confusion matrix data)\")\n",
        "    print(\"=\"*60)\n",
        "\n",
        "    return model, tokenizer, config, train_losses, train_accuracies, val_losses, val_accuracies, save_dir\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "if __name__ == \"__main__\":\n",
        "    model, tokenizer, config, train_losses, train_accuracies, val_losses, val_accuracies, save_dir = main()\n",
        "    print(f\"\\nModel ready for inference!\")\n",
        "    print(f\"To load this model later, use:\")\n",
        "    print(f\"   model, tokenizer, config, metadata = load_saved_model('{save_dir}')\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Understanding Encoder-Only vs Decoder-Only Transformers\n",
        "\n",
        "### Key Architectural Differences\n",
        "\n",
        "**Encoder-Only Transformers (BERT-style)**:\n",
        "- **Bidirectional Attention**: Each token can attend to all tokens in the sequence (past and future)\n",
        "- **Use Case**: Understanding tasks (classification, NER, QA)\n",
        "- **Training**: Masked Language Modeling (MLM) or direct classification\n",
        "- **No Autoregressive Generation**: Processes entire sequence at once\n",
        "\n",
        "**Decoder-Only Transformers (GPT-style)**:\n",
        "- **Causal/Masked Attention**: Each token can only attend to previous tokens\n",
        "- **Use Case**: Generation tasks (text generation, translation)\n",
        "- **Training**: Next-token prediction (autoregressive)\n",
        "- **Autoregressive Generation**: Generates tokens one at a time\n",
        "\n",
        "### Why Bidirectional Attention for Classification?\n",
        "\n",
        "Bidirectional attention allows the model to use **full context** from both directions when making classification decisions. For example, when classifying a document, the model can consider:\n",
        "- The beginning of the document when processing the end\n",
        "- The end of the document when processing the beginning\n",
        "- All tokens simultaneously to make a holistic decision\n",
        "\n",
        "This is different from decoder-only models, which must process tokens sequentially and can only use past context.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Pooling Strategies\n",
        "\n",
        "After processing the sequence through encoder blocks, we need to extract a **single vector representation** for classification. Two common strategies:\n",
        "\n",
        "### 1. CLS Token Pooling\n",
        "- Add a special `[CLS]` token at the beginning of the sequence\n",
        "- After encoding, use the CLS token's representation as the sequence embedding\n",
        "- **Advantage**: The CLS token can learn to aggregate sequence information\n",
        "- **Used by**: BERT\n",
        "\n",
        "### 2. Mean Pooling\n",
        "- Average all token representations (excluding padding)\n",
        "- **Advantage**: Uses all tokens equally\n",
        "- **Used by**: Some models when CLS token is not available\n",
        "\n",
        "In this implementation, both strategies are supported and can be configured via the `pooling_type` parameter.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Configurable Hyperparameters\n",
        "\n",
        "All model dimensions and training hyperparameters are configurable:\n",
        "\n",
        "### Model Architecture\n",
        "- **d_model**: Embedding dimension (e.g., 256, 512, 768)\n",
        "- **num_layers**: Number of encoder blocks (e.g., 4, 6, 12)\n",
        "- **num_heads**: Number of attention heads (must divide d_model)\n",
        "- **d_ff**: Feed-forward dimension (typically 4Ã— d_model)\n",
        "- **num_classes**: Number of classification classes\n",
        "- **pooling_type**: 'cls' or 'mean'\n",
        "\n",
        "### Training\n",
        "- **batch_size**: Training batch size\n",
        "- **num_epochs**: Number of training epochs\n",
        "- **learning_rate**: Learning rate for optimizer\n",
        "- **weight_decay**: L2 regularization strength\n",
        "\n",
        "### Dataset\n",
        "- **num_samples**: Number of samples to load\n",
        "- **max_seq_len**: Maximum sequence length\n",
        "- **vocab_size**: Vocabulary size for tokenizer\n",
        "\n",
        "You can modify these in the `get_default_config()` function or override them in `main()`.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Attention Mechanism: Bidirectional vs Causal\n",
        "\n",
        "### Bidirectional Attention (Encoder-Only)\n",
        "\n",
        "```\n",
        "Token positions:  [0]  [1]  [2]  [3]\n",
        "Can attend to:    âœ“âœ“âœ“  âœ“âœ“âœ“  âœ“âœ“âœ“  âœ“âœ“âœ“\n",
        "                 (all positions)\n",
        "```\n",
        "\n",
        "Each token can attend to **all tokens** in the sequence, including future tokens. This is achieved by:\n",
        "1. Computing attention scores for all pairs of tokens\n",
        "2. **NOT** applying a causal mask (upper triangular mask)\n",
        "3. Only applying padding masks to ignore padding tokens\n",
        "\n",
        "### Causal Attention (Decoder-Only)\n",
        "\n",
        "```\n",
        "Token positions:  [0]  [1]  [2]  [3]\n",
        "Can attend to:    âœ“    âœ“âœ“   âœ“âœ“âœ“  âœ“âœ“âœ“âœ“\n",
        "                 (only past)\n",
        "```\n",
        "\n",
        "Each token can only attend to **previous tokens**. This is achieved by:\n",
        "1. Computing attention scores for all pairs\n",
        "2. **Applying** a causal mask (upper triangular mask set to -inf)\n",
        "3. This prevents tokens from seeing future information\n",
        "\n",
        "### Why This Matters\n",
        "\n",
        "- **Bidirectional**: Better for understanding tasks (classification, sentiment analysis)\n",
        "- **Causal**: Required for generation tasks (text generation, where future tokens don't exist yet)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Model Architecture Summary\n",
        "\n",
        "```\n",
        "Input Text\n",
        "    â†“\n",
        "Tokenization + CLS/SEP tokens\n",
        "    â†“\n",
        "Token Embeddings (vocab_size â†’ d_model)\n",
        "    â†“\n",
        "+ Positional Encoding\n",
        "    â†“\n",
        "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
        "â”‚ Encoder Block 1 â”‚ â† Bidirectional Attention + FFN\n",
        "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
        "    â†“\n",
        "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
        "â”‚ Encoder Block 2 â”‚ â† Bidirectional Attention + FFN\n",
        "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
        "    â†“\n",
        "    ...\n",
        "    â†“\n",
        "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
        "â”‚ Encoder Block N â”‚ â† Bidirectional Attention + FFN\n",
        "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
        "    â†“\n",
        "Pooling (CLS token or Mean)\n",
        "    â†“\n",
        "Classification Head (d_model â†’ num_classes)\n",
        "    â†“\n",
        "Class Predictions\n",
        "```\n",
        "\n",
        "### Data Flow\n",
        "\n",
        "1. **Input**: Raw text abstracts\n",
        "2. **Tokenization**: Convert to token IDs, add CLS/SEP tokens\n",
        "3. **Embedding**: Map token IDs to dense vectors (d_model dimensions)\n",
        "4. **Positional Encoding**: Add position information\n",
        "5. **Encoder Blocks**: Process through N layers of bidirectional attention\n",
        "6. **Pooling**: Extract sequence-level representation\n",
        "7. **Classification**: Map to class probabilities\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Paper Classification Inference\n",
        "\n",
        "Now that we have a trained model, we can use it to classify new paper abstracts. The `classify_paper` function takes a paper abstract as input and returns the predicted research topic with confidence scores.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# INFERENCE FUNCTION\n",
        "\n",
        "def classify_paper(abstract, model, tokenizer, config, device=None):\n",
        "    \"\"\"Classify a paper abstract using the trained encoder-only transformer.\"\"\"\n",
        "    if device is None:\n",
        "        device = next(model.parameters()).device\n",
        "    \n",
        "    model.eval()\n",
        "    \n",
        "    cls_id = tokenizer.token_to_id(\"[CLS]\") if tokenizer.token_to_id(\"[CLS]\") is not None else tokenizer.token_to_id(\"[BOS]\")\n",
        "    pad_id = tokenizer.token_to_id(\"[PAD]\")\n",
        "    sep_id = tokenizer.token_to_id(\"[SEP]\") if tokenizer.token_to_id(\"[SEP]\") is not None else tokenizer.token_to_id(\"[EOS]\")\n",
        "    \n",
        "    encoding = tokenizer.encode(abstract)\n",
        "    tokens = encoding.ids\n",
        "    \n",
        "    max_len = config['max_seq_len']\n",
        "    if len(tokens) > max_len - 2:\n",
        "        tokens = tokens[:max_len - 2]\n",
        "    \n",
        "    tokens = [cls_id] + tokens + [sep_id]\n",
        "    pad_len = max_len - len(tokens)\n",
        "    tokens = tokens + [pad_id] * pad_len\n",
        "    \n",
        "    attention_mask = [1 if t != pad_id else 0 for t in tokens]\n",
        "    \n",
        "    input_ids = torch.tensor([tokens], dtype=torch.long).to(device)\n",
        "    attention_mask = torch.tensor([attention_mask], dtype=torch.long).to(device)\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        logits = model(input_ids, attention_mask)\n",
        "        probabilities = F.softmax(logits, dim=-1)\n",
        "        predicted_class = torch.argmax(probabilities, dim=-1).item()\n",
        "        confidence = probabilities[0, predicted_class].item()\n",
        "        all_probs = {TOPIC_NAMES[i]: probabilities[0, i].item() for i in range(len(TOPIC_NAMES))}\n",
        "    \n",
        "    return {\n",
        "        'predicted_class': predicted_class,\n",
        "        'predicted_topic': TOPIC_NAMES[predicted_class],\n",
        "        'confidence': confidence,\n",
        "        'all_probabilities': all_probs\n",
        "    }\n",
        "\n",
        "\n",
        "def print_classification_result(result):\n",
        "    \"\"\"Pretty print the classification result.\"\"\"\n",
        "    print(\"\\n\" + \"=\"*70)\n",
        "    print(\"PAPER CLASSIFICATION RESULT\")\n",
        "    print(\"=\"*70)\n",
        "    print(f\"\\nPredicted Topic: {result['predicted_topic']}\")\n",
        "    print(f\"Confidence: {result['confidence']*100:.2f}%\")\n",
        "    print(\"\\nAll Topic Probabilities:\")\n",
        "    \n",
        "    sorted_probs = sorted(result['all_probabilities'].items(), key=lambda x: x[1], reverse=True)\n",
        "    for topic, prob in sorted_probs:\n",
        "        bar = \"#\" * int(prob * 30)\n",
        "        print(f\"  {topic:35s} {prob*100:6.2f}% {bar}\")\n",
        "    print(\"=\"*70)\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
