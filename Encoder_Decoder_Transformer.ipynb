{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "aa47be9f",
      "metadata": {
        "id": "aa47be9f"
      },
      "source": [
        "# Encoder-Decoder Transformer for TL;DR Summarization\n",
        "\n",
        "This notebook implements a small Transformer model for generating TL;DR summaries from abstracts.\n",
        "\n",
        "## Model Specifications\n",
        "- d_model: 128-256 (we'll use 192)\n",
        "- Layers: 2-4 (we'll use 3 for both encoder and decoder)\n",
        "- Heads: 2-4 (we'll use 4)\n",
        "- Max sequence length: 128-256 (we'll use 128)\n",
        "\n",
        "## Implementation Plan\n",
        "1. Implement basic components (positional encoding, attention, FFN, etc.)\n",
        "2. Build encoder and decoder layers\n",
        "3. Assemble full Transformer model\n",
        "4. Create data processing pipeline\n",
        "5. Train on a small dataset\n",
        "6. Evaluate with ROUGE and qualitative analysis"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5b96e562",
      "metadata": {
        "id": "5b96e562"
      },
      "source": [
        "## 1. Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e3b24efc",
      "metadata": {
        "id": "e3b24efc"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "a99c8049",
      "metadata": {
        "lines_to_next_cell": 1,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a99c8049",
        "outputId": "41e6bee9-8d08-4763-a7d8-707b4a324b02"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "import math\n",
        "import random\n",
        "import string\n",
        "import re\n",
        "from collections import Counter\n",
        "import matplotlib.pyplot as plt\n",
        "from IPython.display import clear_output\n",
        "import time\n",
        "import json\n",
        "import warnings\n",
        "import os\n",
        "import pickle\n",
        "# from torchtext.datasets import CNNDM\n",
        "from tqdm import tqdm\n",
        "warnings.filterwarnings('ignore')\n",
        "from datasets import load_dataset, Split\n",
        "\n",
        "\n",
        "# Set random seeds for reproducibility\n",
        "torch.manual_seed(42)\n",
        "np.random.seed(42)\n",
        "random.seed(42)\n",
        "\n",
        "# Device configuration\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Using device: {device}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3mkBp3ed_8rJ",
        "outputId": "17e0e6f7-0045-4016-99cd-5cdf17df5bfd"
      },
      "id": "3mkBp3ed_8rJ",
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def save_model(model, tokenizer, path='/content/drive/MyDrive/models/transformer_summarizer_2.pth'):\n",
        "    \"\"\"Save model and tokenizer\"\"\"\n",
        "    torch.save({\n",
        "        'model_state_dict': model.state_dict(),\n",
        "        'tokenizer': tokenizer,\n",
        "        'config': {\n",
        "            'src_vocab_size': model.src_vocab_size,\n",
        "            'tgt_vocab_size': model.tgt_vocab_size,\n",
        "            'd_model': model.d_model,\n",
        "            'n_heads': 8,\n",
        "            'num_encoder_layers': 4,\n",
        "            'num_decoder_layers': 4,\n",
        "            'd_ff': 512,\n",
        "            'max_seq_length': 256,\n",
        "            'dropout': 0.1\n",
        "        }\n",
        "    }, path)\n",
        "    print(f\"Model saved to {path}\")\n",
        "\n",
        "def load_model(path='transformer_summarizer.pth'):\n",
        "    \"\"\"Load model and tokenizer\"\"\"\n",
        "    checkpoint = torch.load(path, map_location=device)\n",
        "\n",
        "    config = checkpoint['config']\n",
        "    model = Transformer(**config).to(device)\n",
        "    model.load_state_dict(checkpoint['model_state_dict'])\n",
        "\n",
        "    tokenizer = checkpoint['tokenizer']\n",
        "\n",
        "    print(f\"Model loaded from {path}\")\n",
        "    return model, tokenizer\n"
      ],
      "metadata": {
        "id": "UBZ7iG_lAC8l"
      },
      "id": "UBZ7iG_lAC8l",
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "04aee45a",
      "metadata": {
        "id": "04aee45a"
      },
      "source": [
        "## 2. Tokenizer Implementation\n",
        "\n",
        "We'll implement a simple word-level tokenizer with special tokens."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "6a18d8e7",
      "metadata": {
        "lines_to_next_cell": 1,
        "id": "6a18d8e7"
      },
      "outputs": [],
      "source": [
        "class SimpleTokenizer:\n",
        "    \"\"\"Simple word-level tokenizer for demonstration\"\"\"\n",
        "\n",
        "    def __init__(self, vocab_size=5000):\n",
        "        self.vocab_size = vocab_size\n",
        "        self.word2idx = {}\n",
        "        self.idx2word = {}\n",
        "        self.special_tokens = {\n",
        "            '<pad>': 0,\n",
        "            '<sos>': 1,  # Start of sequence\n",
        "            '<eos>': 2,  # End of sequence\n",
        "            '<unk>': 3   # Unknown word\n",
        "        }\n",
        "\n",
        "    def fit(self, texts):\n",
        "        \"\"\"Build vocabulary from texts\"\"\"\n",
        "        word_counts = Counter()\n",
        "\n",
        "        for text in texts:\n",
        "            # Simple text cleaning and tokenization\n",
        "            tokens = self._tokenize(text)\n",
        "            word_counts.update(tokens)\n",
        "\n",
        "        # Keep most common words\n",
        "        most_common = word_counts.most_common(self.vocab_size - len(self.special_tokens))\n",
        "\n",
        "        # Build vocabulary\n",
        "        self.word2idx = self.special_tokens.copy()\n",
        "        self.idx2word = {idx: token for token, idx in self.special_tokens.items()}\n",
        "\n",
        "        # Add most common words\n",
        "        for word, _ in most_common:\n",
        "            idx = len(self.word2idx)\n",
        "            self.word2idx[word] = idx\n",
        "            self.idx2word[idx] = word\n",
        "\n",
        "        return self\n",
        "\n",
        "    def _tokenize(self, text):\n",
        "        \"\"\"Tokenize text into words\"\"\"\n",
        "        # Convert to lowercase and split on whitespace/punctuation\n",
        "        text = text.lower()\n",
        "        # Split on whitespace and punctuation (keeping apostrophes for contractions)\n",
        "        tokens = re.findall(r\"\\b[\\w']+\\b\", text)\n",
        "        return tokens\n",
        "\n",
        "    def encode(self, text, max_length=128, add_special_tokens=True):\n",
        "        \"\"\"Convert text to token indices\"\"\"\n",
        "        tokens = self._tokenize(text)\n",
        "\n",
        "        # Convert to indices\n",
        "        indices = []\n",
        "        if add_special_tokens:\n",
        "            indices.append(self.word2idx['<sos>'])\n",
        "\n",
        "        for token in tokens:\n",
        "            indices.append(self.word2idx.get(token, self.word2idx['<unk>']))\n",
        "\n",
        "        if add_special_tokens:\n",
        "            indices.append(self.word2idx['<eos>'])\n",
        "\n",
        "        # Truncate or pad\n",
        "        if len(indices) > max_length:\n",
        "            indices = indices[:max_length-1] + [self.word2idx['<eos>']]\n",
        "        else:\n",
        "            indices = indices + [self.word2idx['<pad>']] * (max_length - len(indices))\n",
        "\n",
        "        return indices[:max_length]\n",
        "\n",
        "    def decode(self, indices):\n",
        "        \"\"\"Convert token indices back to text\"\"\"\n",
        "        tokens = []\n",
        "        for idx in indices:\n",
        "            if idx == self.word2idx['<pad>']:\n",
        "                continue\n",
        "            if idx == self.word2idx['<sos>']:\n",
        "                continue\n",
        "            if idx == self.word2idx['<eos>']:\n",
        "                break\n",
        "            tokens.append(self.idx2word.get(idx, '<unk>'))\n",
        "\n",
        "        return ' '.join(tokens)\n",
        "\n",
        "    def get_vocab_size(self):\n",
        "        return len(self.word2idx)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9c3d5dcc",
      "metadata": {
        "id": "9c3d5dcc"
      },
      "source": [
        "## 3. Positional Encoding\n",
        "\n",
        "Implement sinusoidal positional encoding as in the original Transformer paper."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "ff6a83c9",
      "metadata": {
        "lines_to_next_cell": 1,
        "id": "ff6a83c9"
      },
      "outputs": [],
      "source": [
        "class PositionalEncoding(nn.Module):\n",
        "    \"\"\"Positional encoding for Transformer models\"\"\"\n",
        "\n",
        "    def __init__(self, d_model, max_len=5000, dropout=0.1):\n",
        "        super(PositionalEncoding, self).__init__()\n",
        "        self.dropout = nn.Dropout(p=dropout)\n",
        "\n",
        "        # Create positional encoding matrix\n",
        "        pe = torch.zeros(max_len, d_model)\n",
        "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
        "        div_term = torch.exp(torch.arange(0, d_model, 2).float() *\n",
        "                            (-math.log(10000.0) / d_model))\n",
        "\n",
        "        pe[:, 0::2] = torch.sin(position * div_term)  # Even indices\n",
        "        pe[:, 1::2] = torch.cos(position * div_term)  # Odd indices\n",
        "\n",
        "        pe = pe.unsqueeze(0)  # Shape: (1, max_len, d_model)\n",
        "        self.register_buffer('pe', pe)\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            x: Tensor of shape (batch_size, seq_len, d_model)\n",
        "        Returns:\n",
        "            Tensor with positional encoding added\n",
        "        \"\"\"\n",
        "        x = x + self.pe[:, :x.size(1)]\n",
        "        return self.dropout(x)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7d6da6a1",
      "metadata": {
        "id": "7d6da6a1"
      },
      "source": [
        "## 4. Multi-Head Attention\n",
        "\n",
        "We'll implement multi-head attention from scratch to understand the core logic."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "ff4d8b11",
      "metadata": {
        "lines_to_next_cell": 1,
        "id": "ff4d8b11"
      },
      "outputs": [],
      "source": [
        "class MultiHeadAttention(nn.Module):\n",
        "    \"\"\"Multi-head attention mechanism\"\"\"\n",
        "\n",
        "    def __init__(self, d_model, n_heads, dropout=0.1):\n",
        "        super(MultiHeadAttention, self).__init__()\n",
        "        assert d_model % n_heads == 0, \"d_model must be divisible by n_heads\"\n",
        "\n",
        "        self.d_model = d_model\n",
        "        self.n_heads = n_heads\n",
        "        self.d_k = d_model // n_heads\n",
        "\n",
        "        # Linear projections for query, key, value\n",
        "        self.w_q = nn.Linear(d_model, d_model)\n",
        "        self.w_k = nn.Linear(d_model, d_model)\n",
        "        self.w_v = nn.Linear(d_model, d_model)\n",
        "\n",
        "        # Output projection\n",
        "        self.w_o = nn.Linear(d_model, d_model)\n",
        "\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, query, key, value, mask=None):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            query, key, value: Tensors of shape (batch_size, seq_len, d_model)\n",
        "            mask: Optional mask tensor\n",
        "        Returns:\n",
        "            Attention output and attention weights\n",
        "        \"\"\"\n",
        "        batch_size = query.size(0)\n",
        "\n",
        "        # Project and reshape for multi-head attention\n",
        "        # (batch_size, seq_len, d_model) -> (batch_size, seq_len, n_heads, d_k)\n",
        "        # -> (batch_size, n_heads, seq_len, d_k)\n",
        "        Q = self.w_q(query).view(batch_size, -1, self.n_heads, self.d_k).transpose(1, 2)\n",
        "        K = self.w_k(key).view(batch_size, -1, self.n_heads, self.d_k).transpose(1, 2)\n",
        "        V = self.w_v(value).view(batch_size, -1, self.n_heads, self.d_k).transpose(1, 2)\n",
        "\n",
        "        # Compute attention scores\n",
        "        # (batch_size, n_heads, seq_len_q, seq_len_k)\n",
        "        scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(self.d_k)\n",
        "\n",
        "        # Apply mask if provided\n",
        "        if mask is not None:\n",
        "            scores = scores.masked_fill(mask == 0, -1e9)\n",
        "\n",
        "        # Apply softmax to get attention weights\n",
        "        attn_weights = F.softmax(scores, dim=-1)\n",
        "        attn_weights = self.dropout(attn_weights)\n",
        "\n",
        "        # Apply attention to values\n",
        "        # (batch_size, n_heads, seq_len_q, d_k)\n",
        "        context = torch.matmul(attn_weights, V)\n",
        "\n",
        "        # Reshape back to original dimensions\n",
        "        # (batch_size, seq_len_q, d_model)\n",
        "        context = context.transpose(1, 2).contiguous().view(\n",
        "            batch_size, -1, self.d_model)\n",
        "\n",
        "        # Final linear projection\n",
        "        output = self.w_o(context)\n",
        "\n",
        "        return output, attn_weights"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0991dffc",
      "metadata": {
        "id": "0991dffc"
      },
      "source": [
        "## 5. Feed-Forward Network\n",
        "\n",
        "Position-wise feed-forward network as in the original Transformer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "2a58b1a2",
      "metadata": {
        "lines_to_next_cell": 1,
        "id": "2a58b1a2"
      },
      "outputs": [],
      "source": [
        "class FeedForward(nn.Module):\n",
        "    \"\"\"Position-wise feed-forward network\"\"\"\n",
        "\n",
        "    def __init__(self, d_model, d_ff=512, dropout=0.1):\n",
        "        super(FeedForward, self).__init__()\n",
        "        self.linear1 = nn.Linear(d_model, d_ff)\n",
        "        self.linear2 = nn.Linear(d_ff, d_model)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.linear2(self.dropout(F.relu(self.linear1(x))))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e7f50434",
      "metadata": {
        "id": "e7f50434"
      },
      "source": [
        "## 6. Encoder Layer\n",
        "\n",
        "A single encoder layer with multi-head self-attention and feed-forward network."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "eef4b95d",
      "metadata": {
        "lines_to_next_cell": 1,
        "id": "eef4b95d"
      },
      "outputs": [],
      "source": [
        "class EncoderLayer(nn.Module):\n",
        "    \"\"\"Single Transformer encoder layer\"\"\"\n",
        "\n",
        "    def __init__(self, d_model, n_heads, d_ff=512, dropout=0.1):\n",
        "        super(EncoderLayer, self).__init__()\n",
        "        self.self_attn = MultiHeadAttention(d_model, n_heads, dropout)\n",
        "        self.feed_forward = FeedForward(d_model, d_ff, dropout)\n",
        "\n",
        "        self.norm1 = nn.LayerNorm(d_model)\n",
        "        self.norm2 = nn.LayerNorm(d_model)\n",
        "\n",
        "        self.dropout1 = nn.Dropout(dropout)\n",
        "        self.dropout2 = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x, mask=None):\n",
        "        # Self-attention with residual connection\n",
        "        attn_output, _ = self.self_attn(x, x, x, mask)\n",
        "        x = x + self.dropout1(attn_output)\n",
        "        x = self.norm1(x)\n",
        "\n",
        "        # Feed-forward with residual connection\n",
        "        ff_output = self.feed_forward(x)\n",
        "        x = x + self.dropout2(ff_output)\n",
        "        x = self.norm2(x)\n",
        "\n",
        "        return x"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fd69a484",
      "metadata": {
        "id": "fd69a484"
      },
      "source": [
        "## 7. Decoder Layer\n",
        "\n",
        "A single decoder layer with masked self-attention, encoder-decoder attention, and feed-forward network."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "87c547ed",
      "metadata": {
        "lines_to_next_cell": 1,
        "id": "87c547ed"
      },
      "outputs": [],
      "source": [
        "class DecoderLayer(nn.Module):\n",
        "    \"\"\"Single Transformer decoder layer\"\"\"\n",
        "\n",
        "    def __init__(self, d_model, n_heads, d_ff=512, dropout=0.1):\n",
        "        super(DecoderLayer, self).__init__()\n",
        "        self.self_attn = MultiHeadAttention(d_model, n_heads, dropout)\n",
        "        self.cross_attn = MultiHeadAttention(d_model, n_heads, dropout)\n",
        "        self.feed_forward = FeedForward(d_model, d_ff, dropout)\n",
        "\n",
        "        self.norm1 = nn.LayerNorm(d_model)\n",
        "        self.norm2 = nn.LayerNorm(d_model)\n",
        "        self.norm3 = nn.LayerNorm(d_model)\n",
        "\n",
        "        self.dropout1 = nn.Dropout(dropout)\n",
        "        self.dropout2 = nn.Dropout(dropout)\n",
        "        self.dropout3 = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x, encoder_output, src_mask=None, tgt_mask=None):\n",
        "        # Masked self-attention with residual connection\n",
        "        attn_output, _ = self.self_attn(x, x, x, tgt_mask)\n",
        "        x = x + self.dropout1(attn_output)\n",
        "        x = self.norm1(x)\n",
        "\n",
        "        # Encoder-decoder attention with residual connection\n",
        "        attn_output, _ = self.cross_attn(x, encoder_output, encoder_output, src_mask)\n",
        "        x = x + self.dropout2(attn_output)\n",
        "        x = self.norm2(x)\n",
        "\n",
        "        # Feed-forward with residual connection\n",
        "        ff_output = self.feed_forward(x)\n",
        "        x = x + self.dropout3(ff_output)\n",
        "        x = self.norm3(x)\n",
        "\n",
        "        return x"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "41d9ca67",
      "metadata": {
        "id": "41d9ca67"
      },
      "source": [
        "## 8. Full Transformer Model\n",
        "\n",
        "Assembling the encoder, decoder, and output layers into a complete Transformer model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "06ac48e1",
      "metadata": {
        "lines_to_next_cell": 1,
        "id": "06ac48e1"
      },
      "outputs": [],
      "source": [
        "class Transformer(nn.Module):\n",
        "    \"\"\"Full Transformer model for sequence-to-sequence tasks\"\"\"\n",
        "\n",
        "    def __init__(self, src_vocab_size, tgt_vocab_size, d_model=192, n_heads=4,\n",
        "                 num_encoder_layers=3, num_decoder_layers=3, d_ff=512,\n",
        "                 max_seq_length=128, dropout=0.1):\n",
        "        super(Transformer, self).__init__()\n",
        "\n",
        "        self.d_model = d_model\n",
        "        self.src_vocab_size = src_vocab_size\n",
        "        self.tgt_vocab_size = tgt_vocab_size\n",
        "\n",
        "        # Embedding layers\n",
        "        self.src_embedding = nn.Embedding(src_vocab_size, d_model)\n",
        "        self.tgt_embedding = nn.Embedding(tgt_vocab_size, d_model)\n",
        "\n",
        "        # Positional encoding\n",
        "        self.positional_encoding = PositionalEncoding(d_model, max_seq_length, dropout)\n",
        "\n",
        "        # Encoder\n",
        "        self.encoder_layers = nn.ModuleList([\n",
        "            EncoderLayer(d_model, n_heads, d_ff, dropout)\n",
        "            for _ in range(num_encoder_layers)\n",
        "        ])\n",
        "\n",
        "        # Decoder\n",
        "        self.decoder_layers = nn.ModuleList([\n",
        "            DecoderLayer(d_model, n_heads, d_ff, dropout)\n",
        "            for _ in range(num_decoder_layers)\n",
        "        ])\n",
        "\n",
        "        # Final output layer\n",
        "        self.output_layer = nn.Linear(d_model, tgt_vocab_size)\n",
        "\n",
        "        # Layer norm for encoder and decoder outputs\n",
        "        self.encoder_norm = nn.LayerNorm(d_model)\n",
        "        self.decoder_norm = nn.LayerNorm(d_model)\n",
        "\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "        # Initialize parameters\n",
        "        self._init_parameters()\n",
        "\n",
        "    def _init_parameters(self):\n",
        "        \"\"\"Initialize parameters with Xavier uniform\"\"\"\n",
        "        for p in self.parameters():\n",
        "            if p.dim() > 1:\n",
        "                nn.init.xavier_uniform_(p)\n",
        "\n",
        "    def encode(self, src, src_mask=None):\n",
        "        \"\"\"Encode source sequence\"\"\"\n",
        "        # Embed source tokens\n",
        "        src_embedded = self.src_embedding(src) * math.sqrt(self.d_model)\n",
        "        src_embedded = self.positional_encoding(src_embedded)\n",
        "        src_embedded = self.dropout(src_embedded)\n",
        "\n",
        "        # Pass through encoder layers\n",
        "        encoder_output = src_embedded\n",
        "        for encoder_layer in self.encoder_layers:\n",
        "            encoder_output = encoder_layer(encoder_output, src_mask)\n",
        "\n",
        "        encoder_output = self.encoder_norm(encoder_output)\n",
        "        return encoder_output\n",
        "\n",
        "    def decode(self, tgt, encoder_output, src_mask=None, tgt_mask=None):\n",
        "        \"\"\"Decode target sequence\"\"\"\n",
        "        # Embed target tokens\n",
        "        tgt_embedded = self.tgt_embedding(tgt) * math.sqrt(self.d_model)\n",
        "        tgt_embedded = self.positional_encoding(tgt_embedded)\n",
        "        tgt_embedded = self.dropout(tgt_embedded)\n",
        "\n",
        "        # Pass through decoder layers\n",
        "        decoder_output = tgt_embedded\n",
        "        for decoder_layer in self.decoder_layers:\n",
        "            decoder_output = decoder_layer(\n",
        "                decoder_output, encoder_output, src_mask, tgt_mask)\n",
        "\n",
        "        decoder_output = self.decoder_norm(decoder_output)\n",
        "        return decoder_output\n",
        "\n",
        "    def forward(self, src, tgt, src_mask=None, tgt_mask=None):\n",
        "        \"\"\"Forward pass through the Transformer\"\"\"\n",
        "        # Encode source\n",
        "        encoder_output = self.encode(src, src_mask)\n",
        "\n",
        "        # Decode target\n",
        "        decoder_output = self.decode(tgt, encoder_output, src_mask, tgt_mask)\n",
        "\n",
        "        # Project to vocabulary\n",
        "        output = self.output_layer(decoder_output)\n",
        "\n",
        "        return output\n",
        "\n",
        "    def generate(self, src, src_mask=None, max_length=128, temperature=1.0):\n",
        "        \"\"\"Generate summary for given source sequence\"\"\"\n",
        "        self.eval()\n",
        "        with torch.no_grad():\n",
        "            # Encode source\n",
        "            encoder_output = self.encode(src, src_mask)\n",
        "\n",
        "            # Start with SOS token\n",
        "            batch_size = src.size(0)\n",
        "            tgt = torch.full((batch_size, 1), 1, dtype=torch.long, device=device)  # SOS token\n",
        "\n",
        "            for _ in range(max_length - 1):\n",
        "                # Create target mask for autoregressive generation\n",
        "                tgt_mask = self.create_decoder_mask(tgt.size(1)).to(device)\n",
        "\n",
        "                # Decode\n",
        "                decoder_output = self.decode(tgt, encoder_output, src_mask, tgt_mask)\n",
        "\n",
        "                # Get next token prediction\n",
        "                next_token_logits = self.output_layer(decoder_output[:, -1, :]) / temperature\n",
        "                next_token_probs = F.softmax(next_token_logits, dim=-1)\n",
        "                next_token = torch.multinomial(next_token_probs, num_samples=1)\n",
        "\n",
        "                # Append to target sequence\n",
        "                tgt = torch.cat([tgt, next_token], dim=1)\n",
        "\n",
        "                # Stop if all sequences generated EOS\n",
        "                if (next_token == 2).all():  # EOS token\n",
        "                    break\n",
        "\n",
        "            return tgt\n",
        "\n",
        "    @staticmethod\n",
        "    def create_src_mask(src, pad_token=0):\n",
        "        \"\"\"Create source mask to ignore padding tokens\"\"\"\n",
        "        return (src != pad_token).unsqueeze(1).unsqueeze(2)\n",
        "\n",
        "    @staticmethod\n",
        "    def create_tgt_mask(tgt, pad_token=0):\n",
        "        \"\"\"Create target mask for autoregressive generation\"\"\"\n",
        "        batch_size, seq_len = tgt.size()\n",
        "\n",
        "        # Padding mask\n",
        "        pad_mask = (tgt != pad_token).unsqueeze(1).unsqueeze(2)\n",
        "\n",
        "        # Subsequent mask to prevent looking ahead\n",
        "        subsequent_mask = torch.tril(torch.ones(seq_len, seq_len, device=tgt.device)).bool()\n",
        "        subsequent_mask = subsequent_mask.unsqueeze(0).unsqueeze(0)\n",
        "\n",
        "        # Combine masks\n",
        "        tgt_mask = pad_mask & subsequent_mask\n",
        "\n",
        "        return tgt_mask\n",
        "\n",
        "    @staticmethod\n",
        "    def create_decoder_mask(seq_len):\n",
        "        \"\"\"Create decoder mask for autoregressive generation\"\"\"\n",
        "        mask = torch.tril(torch.ones(seq_len, seq_len)).bool()\n",
        "        return mask.unsqueeze(0).unsqueeze(0)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c04d87f1",
      "metadata": {
        "id": "c04d87f1"
      },
      "source": [
        "## 9. Data Preparation\n",
        "\n",
        "Create a synthetic dataset for training and evaluation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "id": "009abe9f",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "009abe9f",
        "outputId": "d2054417-7f26-4d47-bb4f-3dd243907e8a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "============================================================\n",
            "LOADING ARXIV DATASET FROM HUGGING FACE\n",
            "============================================================\n",
            "Dataset: CShorten/ML-ArXiv-Papers\n",
            "Target samples: 25000\n",
            "\n",
            "Downloading dataset...\n",
            "✓ Dataset loaded: 117592 papers available\n",
            "✓ Extracted 25000 quality abstracts\n",
            "✓ Average length: 156.5 words\n",
            "============================================================\n",
            "Training samples: 20000\n",
            "Validation samples: 5000\n",
            "\n",
            "Example:\n",
            "Abstract: The problem of statistical learning is to construct a predictor of a random variable $Y$ as a function of a related random variable $X$ on the basis of an i.i.d. training sample from the joint distribution of $(X,Y)$. Allowable predictors are drawn from some specified class, and the goal is to approach asymptotically the performance (expected loss) of the best predictor in the class. We consider the setting in which one has perfect observation of the $X$-part of the sample, while the $Y$-part has to be communicated at some finite bit rate. The encoding of the $Y$-values is allowed to depend on the $X$-values. Under suitable regularity conditions on the admissible predictors, the underlying family of probability distributions and the loss function, we give an information-theoretic characterization of achievable predictor performance in terms of conditional distortion-rate functions. The ideas are illustrated on the example of nonparametric regression in Gaussian noise.\n",
            "Summary: Learning from compressed observations\n"
          ]
        }
      ],
      "source": [
        "def load_arxiv_huggingface(num_samples=100000):\n",
        "    \"\"\"Load ML ArXiv papers from Hugging Face.\"\"\"\n",
        "    print(\"=\" * 60)\n",
        "    print(\"LOADING ARXIV DATASET FROM HUGGING FACE\")\n",
        "    print(\"=\" * 60)\n",
        "    print(f\"Dataset: CShorten/ML-ArXiv-Papers\")\n",
        "    print(f\"Target samples: {num_samples}\\n\")\n",
        "\n",
        "    print(\"Downloading dataset...\")\n",
        "    dataset = load_dataset(\"CShorten/ML-ArXiv-Papers\", split=\"train\")\n",
        "    print(f\"✓ Dataset loaded: {len(dataset)} papers available\")\n",
        "\n",
        "    abstracts, summaries = [], []\n",
        "    for i, paper in enumerate(dataset):\n",
        "        if len(abstracts) >= num_samples:\n",
        "            break\n",
        "\n",
        "        abstract = paper['abstract'].strip()\n",
        "        summary = paper['title'].strip()\n",
        "\n",
        "        # Filter: reasonable length abstracts\n",
        "        if 100 < len(abstract) < 5000:\n",
        "            abstract = ' '.join(abstract.split())\n",
        "            abstracts.append(abstract)\n",
        "\n",
        "        if 10 < len(summary) < 500:\n",
        "            abstract = ' '.join(abstract.split())\n",
        "            summaries.append(summary)\n",
        "\n",
        "    print(f\"✓ Extracted {len(abstracts)} quality abstracts\")\n",
        "    print(f\"✓ Average length: {sum(len(a.split()) for a in abstracts) / len(abstracts):.1f} words\")\n",
        "    print(\"=\" * 60)\n",
        "\n",
        "    return abstracts, summaries\n",
        "\n",
        "train_split = 0.8\n",
        "num_samples = 25000\n",
        "articles, summaries = load_arxiv_huggingface(num_samples)\n",
        "\n",
        "\n",
        "train_abstracts, train_summaries = articles[:int(train_split * num_samples)], summaries[:int(train_split * num_samples)]\n",
        "val_abstracts, val_summaries = articles[int(train_split * num_samples):], summaries[int(train_split * num_samples):]\n",
        "\n",
        "print(f\"Training samples: {len(train_abstracts)}\")\n",
        "print(f\"Validation samples: {len(val_abstracts)}\")\n",
        "print(\"\\nExample:\")\n",
        "print(f\"Abstract: {train_abstracts[0]}\")\n",
        "print(f\"Summary: {train_summaries[0]}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "21507e7c",
      "metadata": {
        "id": "21507e7c"
      },
      "source": [
        "## 10. Initialize Tokenizer and Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "id": "94a4bdfd",
      "metadata": {
        "lines_to_next_cell": 1,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "94a4bdfd",
        "outputId": "f4ec1757-2414-40b7-da7f-ec03614b0944"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vocabulary size: 25000\n",
            "Model parameters: 24,497,576\n"
          ]
        }
      ],
      "source": [
        "# Initialize tokenizer\n",
        "tokenizer = SimpleTokenizer(vocab_size=10000)\n",
        "\n",
        "# Fit tokenizer on all texts\n",
        "all_texts = train_abstracts + train_summaries + val_abstracts + val_summaries\n",
        "tokenizer.fit(all_texts)\n",
        "\n",
        "vocab_size = tokenizer.get_vocab_size()\n",
        "print(f\"Vocabulary size: {vocab_size}\")\n",
        "\n",
        "# Initialize model\n",
        "model = Transformer(\n",
        "    src_vocab_size=vocab_size,\n",
        "    tgt_vocab_size=vocab_size,\n",
        "    d_model=256,\n",
        "    n_heads=8,\n",
        "    num_encoder_layers=4,\n",
        "    num_decoder_layers=4,\n",
        "    d_ff=512,\n",
        "    max_seq_length=256,\n",
        "    dropout=0.1\n",
        ").to(device)\n",
        "\n",
        "print(f\"Model parameters: {sum(p.numel() for p in model.parameters()):,}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4388eb2b",
      "metadata": {
        "id": "4388eb2b"
      },
      "source": [
        "## 11. Dataset and DataLoader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "id": "c9943e80",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c9943e80",
        "outputId": "a55cc088-8450-449a-9e14-dabae767a4d4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train batches: 625\n",
            "Val batches: 157\n"
          ]
        }
      ],
      "source": [
        "class SummarizationDataset(torch.utils.data.Dataset):\n",
        "    \"\"\"Dataset for abstract-summary pairs\"\"\"\n",
        "\n",
        "    def __init__(self, abstracts, summaries, tokenizer, max_length=128):\n",
        "        self.abstracts = abstracts\n",
        "        self.summaries = summaries\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_length = max_length\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.abstracts)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # Encode abstract and summary\n",
        "        src = self.tokenizer.encode(\n",
        "            self.abstracts[idx],\n",
        "            max_length=self.max_length,\n",
        "            add_special_tokens=True\n",
        "        )\n",
        "\n",
        "        tgt = self.tokenizer.encode(\n",
        "            self.summaries[idx],\n",
        "            max_length=self.max_length,\n",
        "            add_special_tokens=True\n",
        "        )\n",
        "\n",
        "        # Input for decoder (shifted right)\n",
        "        tgt_input = tgt[:-1]\n",
        "        tgt_output = tgt[1:]  # Shifted by one for teacher forcing\n",
        "\n",
        "        return {\n",
        "            'src': torch.tensor(src, dtype=torch.long),\n",
        "            'tgt_input': torch.tensor(tgt_input, dtype=torch.long),\n",
        "            'tgt_output': torch.tensor(tgt_output, dtype=torch.long)\n",
        "        }\n",
        "\n",
        "# Create datasets and dataloaders\n",
        "train_dataset = SummarizationDataset(train_abstracts, train_summaries, tokenizer)\n",
        "val_dataset = SummarizationDataset(val_abstracts, val_summaries, tokenizer)\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(\n",
        "    train_dataset, batch_size=32, shuffle=True, num_workers=0\n",
        ")\n",
        "val_loader = torch.utils.data.DataLoader(\n",
        "    val_dataset, batch_size=32, shuffle=False, num_workers=0\n",
        ")\n",
        "\n",
        "print(f\"Train batches: {len(train_loader)}\")\n",
        "print(f\"Val batches: {len(val_loader)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "77c70b01",
      "metadata": {
        "id": "77c70b01"
      },
      "source": [
        "## 12. Training Configuration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "id": "a513ce99",
      "metadata": {
        "lines_to_next_cell": 1,
        "id": "a513ce99"
      },
      "outputs": [],
      "source": [
        "# Loss function (ignoring padding tokens)\n",
        "criterion = nn.CrossEntropyLoss(ignore_index=tokenizer.word2idx['<pad>'])\n",
        "\n",
        "# Optimizer\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.0001, betas=(0.9, 0.98), eps=1e-9)\n",
        "\n",
        "# Learning rate scheduler\n",
        "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.5)\n",
        "\n",
        "# Training parameters\n",
        "num_epochs = 30\n",
        "clip_grad = 1.0  # Gradient clipping"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bc3057cf",
      "metadata": {
        "id": "bc3057cf"
      },
      "source": [
        "## 13. Training Loop"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "id": "ebaed5e4",
      "metadata": {
        "lines_to_next_cell": 1,
        "id": "ebaed5e4"
      },
      "outputs": [],
      "source": [
        "def train_epoch(model, data_loader, optimizer, criterion, clip_grad):\n",
        "    \"\"\"Train for one epoch\"\"\"\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "\n",
        "    for batch in data_loader:\n",
        "        src = batch['src'].to(device)\n",
        "        tgt_input = batch['tgt_input'].to(device)\n",
        "        tgt_output = batch['tgt_output'].to(device)\n",
        "\n",
        "        # Create masks\n",
        "        src_mask = model.create_src_mask(src, tokenizer.word2idx['<pad>'])\n",
        "        tgt_mask = model.create_tgt_mask(tgt_input, tokenizer.word2idx['<pad>'])\n",
        "\n",
        "        # Forward pass\n",
        "        optimizer.zero_grad()\n",
        "        output = model(src, tgt_input, src_mask, tgt_mask)\n",
        "\n",
        "        # Reshape for loss calculation\n",
        "        output = output.view(-1, output.size(-1))\n",
        "        tgt_output = tgt_output.view(-1)\n",
        "\n",
        "        # Calculate loss\n",
        "        loss = criterion(output, tgt_output)\n",
        "\n",
        "        # Backward pass\n",
        "        loss.backward()\n",
        "\n",
        "        # Gradient clipping\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip_grad)\n",
        "\n",
        "        # Update parameters\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "\n",
        "    return total_loss / len(data_loader)\n",
        "\n",
        "def evaluate(model, data_loader, criterion):\n",
        "    \"\"\"Evaluate model on validation set\"\"\"\n",
        "    model.eval()\n",
        "    total_loss = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in data_loader:\n",
        "            src = batch['src'].to(device)\n",
        "            tgt_input = batch['tgt_input'].to(device)\n",
        "            tgt_output = batch['tgt_output'].to(device)\n",
        "\n",
        "            # Create masks\n",
        "            src_mask = model.create_src_mask(src, tokenizer.word2idx['<pad>'])\n",
        "            tgt_mask = model.create_tgt_mask(tgt_input, tokenizer.word2idx['<pad>'])\n",
        "\n",
        "            # Forward pass\n",
        "            output = model(src, tgt_input, src_mask, tgt_mask)\n",
        "\n",
        "            # Reshape for loss calculation\n",
        "            output = output.view(-1, output.size(-1))\n",
        "            tgt_output = tgt_output.view(-1)\n",
        "\n",
        "            # Calculate loss\n",
        "            loss = criterion(output, tgt_output)\n",
        "\n",
        "            total_loss += loss.item()\n",
        "\n",
        "    return total_loss / len(data_loader)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "43c389c5",
      "metadata": {
        "id": "43c389c5"
      },
      "source": [
        "## 14. Training Execution"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "id": "5763c088",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "5763c088",
        "outputId": "43e43f29-cca2-4e5b-9091-9747cc3ac78e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting training...\n",
            "Epoch   1/30 | Train Loss: 6.7706 | Val Loss: 6.2390 | Time: 96.64s\n",
            "Epoch   2/30 | Train Loss: 5.9130 | Val Loss: 6.0540 | Time: 98.07s\n",
            "Epoch   3/30 | Train Loss: 5.7108 | Val Loss: 5.9627 | Time: 98.29s\n",
            "Epoch   4/30 | Train Loss: 5.5785 | Val Loss: 5.8809 | Time: 98.23s\n",
            "Epoch   5/30 | Train Loss: 5.4646 | Val Loss: 5.8048 | Time: 98.31s\n",
            "\n",
            "--- Example Generation ---\n",
            "Abstract: while statistics and machine learning offers numerous methods for ensuring generalization these methods often fail in the presence of adaptivity the common practice in which the choice of analysis depends on previous interactions with the same dataset a recent line of work has introduced powerful general purpose algorithms that ensure post hoc generalization also called robust or post selection generalization which says that given the output of the algorithm it is hard to find any statistic for which the data differs significantly from the population it came from in this work we show several limitations on the power of algorithms satisfying post hoc generalization first we show a tight lower bound on the error of any algorithm that satisfies post hoc generalization and answers adaptively chosen\n",
            "Original Summary: minibatch gibbs sampling on large graphical models\n",
            "Generated Summary: easy as deep properties for transferred online prediction\n",
            "--------------------------\n",
            "\n",
            "Model saved to /content/drive/MyDrive/models/transformer_summarizer_epoch_4.pth\n",
            "Epoch   6/30 | Train Loss: 5.3619 | Val Loss: 5.7496 | Time: 98.79s\n",
            "Epoch   7/30 | Train Loss: 5.2689 | Val Loss: 5.7006 | Time: 98.56s\n",
            "Epoch   8/30 | Train Loss: 5.1818 | Val Loss: 5.6758 | Time: 98.30s\n",
            "Epoch   9/30 | Train Loss: 5.1014 | Val Loss: 5.6635 | Time: 98.37s\n",
            "Epoch  10/30 | Train Loss: 5.0260 | Val Loss: 5.6116 | Time: 98.29s\n",
            "\n",
            "--- Example Generation ---\n",
            "Abstract: we describe an adversarial learning approach to constrain convolutional neural network training for image registration replacing heuristic smoothness measures of displacement fields often used in these tasks using minimally invasive prostate cancer intervention as an example application we demonstrate the feasibility of utilizing biomechanical simulations to regularize a weakly supervised anatomical label driven registration network for aligning pre procedural magnetic resonance mr and 3d intra procedural transrectal ultrasound trus images a discriminator network is optimized to distinguish the registration predicted displacement fields from the motion data simulated by finite element analysis during training the registration network simultaneously aims to maximize similarity between anatomical labels that drives image alignment and to minimize an adversarial generator loss that measures divergence between the predicted and simulated deformation the end\n",
            "Original Summary: defending against adversarial attacks by leveraging an entire gan\n",
            "Generated Summary: optimal sgd for compressed bandits\n",
            "--------------------------\n",
            "\n",
            "Model saved to /content/drive/MyDrive/models/transformer_summarizer_epoch_9.pth\n",
            "Epoch  11/30 | Train Loss: 4.9377 | Val Loss: 5.6241 | Time: 98.40s\n",
            "Epoch  12/30 | Train Loss: 4.8989 | Val Loss: 5.6094 | Time: 98.41s\n",
            "Epoch  13/30 | Train Loss: 4.8644 | Val Loss: 5.6108 | Time: 98.27s\n",
            "Epoch  14/30 | Train Loss: 4.8304 | Val Loss: 5.6335 | Time: 98.39s\n",
            "Epoch  15/30 | Train Loss: 4.7990 | Val Loss: 5.6111 | Time: 98.32s\n",
            "\n",
            "--- Example Generation ---\n",
            "Abstract: deep neural networks have been shown to be beneficial for a variety of tasks in particular allowing for end to end learning and reducing the requirement for manual design decisions however still many parameters have to be chosen in advance also raising the need to optimize them one important but often ignored system parameter is the selection of a proper activation function thus in this paper we target to demonstrate the importance of activation functions in general and show that for different tasks different activation functions might be meaningful to avoid the manual design or selection of activation functions we build on the idea of genetic algorithms to learn the best activation function for a given task in addition we introduce two new activation functions <unk>\n",
            "Original Summary: online aggregation of unbounded losses using shifting experts with confidence\n",
            "Generated Summary: mean projections for neural functional data analysis\n",
            "--------------------------\n",
            "\n",
            "Model saved to /content/drive/MyDrive/models/transformer_summarizer_epoch_14.pth\n",
            "Epoch  16/30 | Train Loss: 4.7659 | Val Loss: 5.6374 | Time: 98.36s\n",
            "Epoch  17/30 | Train Loss: 4.7352 | Val Loss: 5.6704 | Time: 98.29s\n",
            "Epoch  18/30 | Train Loss: 4.7027 | Val Loss: 5.7084 | Time: 98.34s\n",
            "Epoch  19/30 | Train Loss: 4.6696 | Val Loss: 5.6596 | Time: 98.23s\n",
            "Epoch  20/30 | Train Loss: 4.6382 | Val Loss: 5.7444 | Time: 98.25s\n",
            "\n",
            "--- Example Generation ---\n",
            "Abstract: making sense of a dataset in an automatic and unsupervised fashion is a challenging problem in statistics and ai classical approaches for exploratory data analysis are usually not flexible enough to deal with the uncertainty inherent to real world data they are often restricted to fixed latent interaction models and homogeneous likelihoods they are sensitive to missing corrupt and anomalous data moreover their expressiveness generally comes at the price of intractable inference as a result supervision from statisticians is usually needed to find the right model for the data however since domain experts are not necessarily also experts in statistics we propose automatic bayesian density analysis abda to make exploratory data analysis accessible at large specifically abda allows for automatic and efficient missing value estimation statistical\n",
            "Original Summary: noise contrastive priors for functional uncertainty\n",
            "Generated Summary: deep whi player machine learning\n",
            "--------------------------\n",
            "\n",
            "Model saved to /content/drive/MyDrive/models/transformer_summarizer_epoch_19.pth\n",
            "Epoch  21/30 | Train Loss: 4.5884 | Val Loss: 5.7530 | Time: 98.38s\n",
            "Epoch  22/30 | Train Loss: 4.5671 | Val Loss: 5.7783 | Time: 98.38s\n",
            "Epoch  23/30 | Train Loss: 4.5503 | Val Loss: 5.7694 | Time: 98.26s\n",
            "Epoch  24/30 | Train Loss: 4.5332 | Val Loss: 5.8112 | Time: 98.36s\n",
            "Epoch  25/30 | Train Loss: 4.5126 | Val Loss: 5.8063 | Time: 98.19s\n",
            "\n",
            "--- Example Generation ---\n",
            "Abstract: in distributed training of deep neural networks parallel mini batch sgd is widely used to speed up the training process by using multiple workers it uses multiple workers to sample local stochastic gradient in parallel aggregates all gradients in a single server to obtain the average and update each worker's local model using a sgd update with the averaged gradient ideally parallel mini batch sgd can achieve a linear speed up of the training time with respect to the number of workers compared with sgd over a single worker however such linear scalability in practice is significantly limited by the growing demand for gradient communication as more workers are involved model averaging which periodically averages individual models trained over parallel workers is another common practice used\n",
            "Original Summary: learning noise invariant representations for robust speech recognition\n",
            "Generated Summary: topic models for visual multiscale dynamics\n",
            "--------------------------\n",
            "\n",
            "Model saved to /content/drive/MyDrive/models/transformer_summarizer_epoch_24.pth\n",
            "Epoch  26/30 | Train Loss: 4.4978 | Val Loss: 5.8220 | Time: 98.15s\n",
            "Epoch  27/30 | Train Loss: 4.4783 | Val Loss: 5.8912 | Time: 98.33s\n",
            "Epoch  28/30 | Train Loss: 4.4633 | Val Loss: 5.9275 | Time: 98.29s\n",
            "Epoch  29/30 | Train Loss: 4.4444 | Val Loss: 5.8836 | Time: 98.17s\n",
            "Epoch  30/30 | Train Loss: 4.4254 | Val Loss: 5.9515 | Time: 98.33s\n",
            "\n",
            "--- Example Generation ---\n",
            "Abstract: there is a growing body of literature showing that deep neural networks are vulnerable to adversarial input modification recently this work has been extended from image classification to malware classification over boolean features in this paper we present several new methods for training restricted networks in this specific domain that are highly effective at preventing adversarial perturbations we start with a fully adversarially resistant neural network that has hard non negative weight restrictions and is equivalent to learning a monotonic boolean function and then attempt to relax the constraints to improve classifier accuracy\n",
            "Original Summary: stroke based character reconstruction\n",
            "Generated Summary: an efficient distributed method for stochastic bandits and regression\n",
            "--------------------------\n",
            "\n",
            "Model saved to /content/drive/MyDrive/models/transformer_summarizer_epoch_29.pth\n",
            "Training complete!\n",
            "Model saved to /content/drive/MyDrive/models/transformer_summarizer.pth\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: 'content/drive/MyDrive/models/train_losses.pkl'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3747585024.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Training complete!\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[0msave_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34mf\"/content/drive/MyDrive/models/transformer_summarizer.pth\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 55\u001b[0;31m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"content/drive/MyDrive/models/train_losses.pkl\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"wb\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     56\u001b[0m     \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_losses\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'content/drive/MyDrive/models/train_losses.pkl'"
          ]
        }
      ],
      "source": [
        "# Training history\n",
        "train_losses = []\n",
        "val_losses = []\n",
        "\n",
        "print(\"Starting training...\")\n",
        "for epoch in range(num_epochs):\n",
        "    start_time = time.time()\n",
        "\n",
        "    # Train\n",
        "    train_loss = train_epoch(model, train_loader, optimizer, criterion, clip_grad)\n",
        "    train_losses.append(train_loss)\n",
        "\n",
        "    # Evaluate\n",
        "    val_loss = evaluate(model, val_loader, criterion)\n",
        "    val_losses.append(val_loss)\n",
        "\n",
        "    # Update learning rate\n",
        "    scheduler.step()\n",
        "\n",
        "    # Print progress\n",
        "    epoch_time = time.time() - start_time\n",
        "    print(f\"Epoch {epoch+1:3d}/{num_epochs} | \"\n",
        "          f\"Train Loss: {train_loss:.4f} | \"\n",
        "          f\"Val Loss: {val_loss:.4f} | \"\n",
        "          f\"Time: {epoch_time:.2f}s\")\n",
        "\n",
        "    # Generate example every 5 epochs\n",
        "    if (epoch + 1) % 5 == 0:\n",
        "        model.eval()\n",
        "        with torch.no_grad():\n",
        "            # Get a sample from validation set\n",
        "            sample_idx = random.randint(0, len(val_dataset) - 1)\n",
        "            sample = val_dataset[sample_idx]\n",
        "\n",
        "            src = sample['src'].unsqueeze(0).to(device)\n",
        "            src_mask = model.create_src_mask(src, tokenizer.word2idx['<pad>'])\n",
        "\n",
        "            # Generate summary\n",
        "            generated = model.generate(src, src_mask, max_length=30)\n",
        "\n",
        "            # Decode\n",
        "            original_abstract = tokenizer.decode(sample['src'].tolist())\n",
        "            original_summary = tokenizer.decode(sample['tgt_output'].tolist())\n",
        "            generated_summary = tokenizer.decode(generated[0].tolist())\n",
        "\n",
        "            print(\"\\n--- Example Generation ---\")\n",
        "            print(f\"Abstract: {original_abstract}\")\n",
        "            print(f\"Original Summary: {original_summary}\")\n",
        "            print(f\"Generated Summary: {generated_summary}\")\n",
        "            print(\"--------------------------\\n\")\n",
        "        save_model(model, tokenizer, f\"/content/drive/MyDrive/models/transformer_summarizer_epoch_{epoch}.pth\")\n",
        "\n",
        "print(\"Training complete!\")\n",
        "save_model(model, tokenizer, f\"/content/drive/MyDrive/models/transformer_summarizer.pth\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f478ae5f",
      "metadata": {
        "id": "f478ae5f"
      },
      "source": [
        "## 15. Loss Curve Visualization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "721d9dde",
      "metadata": {
        "lines_to_next_cell": 1,
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 487
        },
        "id": "721d9dde",
        "outputId": "03e10e41-914b-45cc-8c3f-72136c1e3d09"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1000x500 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "# Plot training and validation loss\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.plot(train_losses, label='Training Loss', linewidth=2)\n",
        "plt.plot(val_losses, label='Validation Loss', linewidth=2)\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.title('Training and Validation Loss Curves')\n",
        "plt.legend()\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1c05b271",
      "metadata": {
        "id": "1c05b271"
      },
      "source": [
        "## 16. ROUGE Score Implementation\n",
        "\n",
        "Implement a simplified version of ROUGE for evaluation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3e2389a6",
      "metadata": {
        "lines_to_next_cell": 1,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3e2389a6",
        "outputId": "af07658b-8c67-4cdb-8042-803df31ffb42"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ROUGE Evaluation Results:\n",
            "ROUGE-1 Precision: 0.0643\n",
            "ROUGE-1 Recall: 0.0615\n",
            "ROUGE-1 F1: 0.0611\n",
            "\n",
            "ROUGE-2 Precision: 0.0000\n",
            "ROUGE-2 Recall: 0.0000\n",
            "ROUGE-2 F1: 0.0000\n"
          ]
        }
      ],
      "source": [
        "def simple_rouge(generated, reference, n=1):\n",
        "    \"\"\"Simplified ROUGE-N score calculation\"\"\"\n",
        "\n",
        "    def get_ngrams(text, n):\n",
        "        words = text.lower().split()\n",
        "        ngrams = []\n",
        "        for i in range(len(words) - n + 1):\n",
        "            ngrams.append(tuple(words[i:i+n]))\n",
        "        return set(ngrams)\n",
        "\n",
        "    gen_ngrams = get_ngrams(generated, n)\n",
        "    ref_ngrams = get_ngrams(reference, n)\n",
        "\n",
        "    if len(gen_ngrams) == 0 or len(ref_ngrams) == 0:\n",
        "      return {'precision': 0.0, 'recall': 0.0, 'f1': 0.0}\n",
        "\n",
        "    # Calculate ROUGE-N precision, recall, and F1\n",
        "    overlap = len(gen_ngrams.intersection(ref_ngrams))\n",
        "    precision = overlap / len(gen_ngrams)\n",
        "    recall = overlap / len(ref_ngrams) if len(ref_ngrams) > 0 else 0\n",
        "\n",
        "    if precision + recall > 0:\n",
        "        f1 = 2 * precision * recall / (precision + recall)\n",
        "    else:\n",
        "        f1 = 0\n",
        "\n",
        "    return {'precision': precision, 'recall': recall, 'f1': f1}\n",
        "\n",
        "def evaluate_rouge(model, dataset, tokenizer, num_samples=20):\n",
        "    \"\"\"Evaluate model using ROUGE scores\"\"\"\n",
        "    model.eval()\n",
        "\n",
        "    rouge1_scores = {'precision': [], 'recall': [], 'f1': []}\n",
        "    rouge2_scores = {'precision': [], 'recall': [], 'f1': []}\n",
        "\n",
        "    with torch.no_grad():\n",
        "        # Randomly sample from dataset\n",
        "        indices = random.sample(range(len(dataset)), min(num_samples, len(dataset)))\n",
        "\n",
        "        for idx in indices:\n",
        "            sample = dataset[idx]\n",
        "\n",
        "            src = sample['src'].unsqueeze(0).to(device)\n",
        "            src_mask = model.create_src_mask(src, tokenizer.word2idx['<pad>'])\n",
        "\n",
        "            # Generate summary\n",
        "            generated = model.generate(src, src_mask, max_length=30)\n",
        "\n",
        "            # Decode\n",
        "            original_summary = tokenizer.decode(sample['tgt_output'].tolist())\n",
        "            generated_summary = tokenizer.decode(generated[0].tolist())\n",
        "\n",
        "            # Calculate ROUGE scores\n",
        "            rouge1 = simple_rouge(generated_summary, original_summary, n=1)\n",
        "            rouge2 = simple_rouge(generated_summary, original_summary, n=2)\n",
        "\n",
        "            for metric in ['precision', 'recall', 'f1']:\n",
        "                rouge1_scores[metric].append(rouge1[metric])\n",
        "                rouge2_scores[metric].append(rouge2[metric])\n",
        "\n",
        "    # Calculate average scores\n",
        "    avg_rouge1 = {metric: np.mean(scores) for metric, scores in rouge1_scores.items()}\n",
        "    avg_rouge2 = {metric: np.mean(scores) for metric, scores in rouge2_scores.items()}\n",
        "\n",
        "    return avg_rouge1, avg_rouge2\n",
        "\n",
        "# Evaluate model\n",
        "rouge1_scores, rouge2_scores = evaluate_rouge(model, val_dataset, tokenizer, num_samples=50)\n",
        "\n",
        "print(\"ROUGE Evaluation Results:\")\n",
        "print(f\"ROUGE-1 Precision: {rouge1_scores['precision']:.4f}\")\n",
        "print(f\"ROUGE-1 Recall: {rouge1_scores['recall']:.4f}\")\n",
        "print(f\"ROUGE-1 F1: {rouge1_scores['f1']:.4f}\")\n",
        "print()\n",
        "print(f\"ROUGE-2 Precision: {rouge2_scores['precision']:.4f}\")\n",
        "print(f\"ROUGE-2 Recall: {rouge2_scores['recall']:.4f}\")\n",
        "print(f\"ROUGE-2 F1: {rouge2_scores['f1']:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "20d2a08b",
      "metadata": {
        "id": "20d2a08b"
      },
      "source": [
        "## 17. Qualitative Evaluation\n",
        "\n",
        "Generate and evaluate some sample summaries."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "40257298",
      "metadata": {
        "lines_to_next_cell": 1,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "40257298",
        "outputId": "fc64f53f-2af2-4aa5-bd77-7e01ac8d3441"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Qualitative Evaluation:\n",
            "================================================================================\n",
            "\n",
            "Example 1:\n",
            "Abstract: the study analyzed the relationship between treatment and control groups in a clinical experiment with significant results showing positive effects\n",
            "Generated TL;DR: online learning via generalized future parameters selection\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "Example 2:\n",
            "Abstract: research findings demonstrate a correlation between data analysis methods and experimental outcomes with important implications for future work\n",
            "Generated TL;DR: classification of images using side information\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "Example 3:\n",
            "Abstract: the experiment evaluated participants in different conditions and measured the impact of various factors on the final results\n",
            "Generated TL;DR: <unk> transfer on adaboost\n",
            "--------------------------------------------------------------------------------\n"
          ]
        }
      ],
      "source": [
        "def generate_summary(model, abstract, tokenizer, max_length=30):\n",
        "    \"\"\"Generate summary for a given abstract\"\"\"\n",
        "    model.eval()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        # Encode abstract\n",
        "        src = torch.tensor(\n",
        "            tokenizer.encode(abstract, max_length=128, add_special_tokens=True),\n",
        "            dtype=torch.long\n",
        "        ).unsqueeze(0).to(device)\n",
        "\n",
        "        src_mask = model.create_src_mask(src, tokenizer.word2idx['<pad>'])\n",
        "\n",
        "        # Generate summary\n",
        "        generated = model.generate(src, src_mask, max_length=max_length)\n",
        "\n",
        "        # Decode\n",
        "        generated_summary = tokenizer.decode(generated[0].tolist())\n",
        "\n",
        "        return generated_summary\n",
        "\n",
        "# Test with some example abstracts\n",
        "test_abstracts = [\n",
        "    \"the study analyzed the relationship between treatment and control groups \"\n",
        "    \"in a clinical experiment with significant results showing positive effects\",\n",
        "\n",
        "    \"research findings demonstrate a correlation between data analysis methods \"\n",
        "    \"and experimental outcomes with important implications for future work\",\n",
        "\n",
        "    \"the experiment evaluated participants in different conditions and measured \"\n",
        "    \"the impact of various factors on the final results\"\n",
        "]\n",
        "\n",
        "print(\"Qualitative Evaluation:\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "for i, abstract in enumerate(test_abstracts):\n",
        "    generated = generate_summary(model, abstract, tokenizer)\n",
        "    print(f\"\\nExample {i+1}:\")\n",
        "    print(f\"Abstract: {abstract}\")\n",
        "    print(f\"Generated TL;DR: {generated}\")\n",
        "    print(\"-\" * 80)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a92ab861",
      "metadata": {
        "id": "a92ab861"
      },
      "source": [
        "## 19. Sanity Check Experiments\n",
        "\n",
        "Let's run some sanity checks to verify our implementation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0aa9cd75",
      "metadata": {
        "lines_to_next_cell": 1,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0aa9cd75",
        "outputId": "f775fcd5-0262-4e2c-ff64-60ed8d38af76"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running attention sanity check...\n",
            "Input shape: torch.Size([2, 5, 64])\n",
            "Output shape: torch.Size([2, 5, 64])\n",
            "Attention weights shape: torch.Size([2, 4, 5, 5])\n",
            "✓ Attention sanity check passed!\n",
            "\n",
            "Running positional encoding sanity check...\n",
            "Input shape: torch.Size([2, 10, 64])\n",
            "Output shape: torch.Size([2, 10, 64])\n",
            "Positional encoding range check: min=-0.9900, max=1.0000\n",
            "Max difference between pe_added_0 and pe_added_1: 0.0000002384\n",
            "Max difference between pe_added_0 and stored PE: 0.0000001192\n",
            "Position 0 (first 4 dims): [0. 1. 0. 1.]\n",
            "Position 1 (first 4 dims): [0.84147096 0.54030234 0.68156135 0.731761  ]\n",
            "Position 2 (first 2 dims): [ 0.9092974  -0.41614684]\n",
            "✓ Positional encoding sanity check passed!\n",
            "\n",
            "Running Transformer forward pass sanity check...\n",
            "Source shape: torch.Size([3, 10])\n",
            "Target shape: torch.Size([3, 10])\n",
            "Output shape: torch.Size([3, 10, 100])\n",
            "✓ Transformer forward pass sanity check passed!\n",
            "\n"
          ]
        }
      ],
      "source": [
        "def sanity_check_attention():\n",
        "    \"\"\"Sanity check for attention mechanism\"\"\"\n",
        "    print(\"Running attention sanity check...\")\n",
        "\n",
        "    # Create a small attention module\n",
        "    d_model = 64\n",
        "    n_heads = 4\n",
        "    batch_size = 2\n",
        "    seq_len = 5\n",
        "\n",
        "    attention = MultiHeadAttention(d_model, n_heads).to(device)\n",
        "\n",
        "    # Create random input\n",
        "    x = torch.randn(batch_size, seq_len, d_model).to(device)\n",
        "\n",
        "    # Self-attention\n",
        "    output, weights = attention(x, x, x)\n",
        "\n",
        "    print(f\"Input shape: {x.shape}\")\n",
        "    print(f\"Output shape: {output.shape}\")\n",
        "    print(f\"Attention weights shape: {weights.shape}\")\n",
        "\n",
        "    # Check shapes\n",
        "    assert output.shape == x.shape, f\"Output shape {output.shape} != input shape {x.shape}\"\n",
        "    assert weights.shape == (batch_size, n_heads, seq_len, seq_len), \\\n",
        "        f\"Attention weights shape {weights.shape} incorrect\"\n",
        "\n",
        "    print(\"✓ Attention sanity check passed!\\n\")\n",
        "\n",
        "def sanity_check_positional_encoding():\n",
        "    \"\"\"Sanity check for positional encoding\"\"\"\n",
        "    print(\"Running positional encoding sanity check...\")\n",
        "\n",
        "    d_model = 64\n",
        "    seq_len = 10\n",
        "    batch_size = 2\n",
        "\n",
        "    pe = PositionalEncoding(d_model, max_len=100, dropout=0.0).to(device)  # No dropout\n",
        "\n",
        "    # Create random input\n",
        "    x = torch.randn(batch_size, seq_len, d_model).to(device)\n",
        "\n",
        "    # Apply positional encoding\n",
        "    output = pe(x)\n",
        "\n",
        "    print(f\"Input shape: {x.shape}\")\n",
        "    print(f\"Output shape: {output.shape}\")\n",
        "\n",
        "    # Check that output is different from input (positional encoding was added)\n",
        "    # They should be different because we added positional encoding\n",
        "    assert not torch.allclose(x, output, rtol=1e-5), \"Positional encoding didn't change input\"\n",
        "\n",
        "    # The issue: We should check the positional encoding VALUES, not the output\n",
        "    # Get the positional encoding matrix that was added\n",
        "    pe_matrix = pe.pe[:, :seq_len, :]\n",
        "\n",
        "    # Check that positional encoding values are bounded (sin/cos are between -1 and 1)\n",
        "    assert pe_matrix.min() >= -1.0 and pe_matrix.max() <= 1.0, \\\n",
        "        f\"Positional encoding values out of range: min={pe_matrix.min():.4f}, max={pe_matrix.max():.4f}\"\n",
        "\n",
        "    print(f\"Positional encoding range check: min={pe_matrix.min():.4f}, max={pe_matrix.max():.4f}\")\n",
        "\n",
        "    # Check that encoding for same position is the same across batches\n",
        "    # For two different inputs at the same batch dimension, the added PE should be the same\n",
        "    # Since PE is added to input, output = input + PE, so PE = output - input\n",
        "    pe_added_0 = output[0] - x[0]\n",
        "    pe_added_1 = output[1] - x[1]\n",
        "\n",
        "    pe_computed_from_batch0 = output[0] - x[0]  # Should be stored_pe[0]\n",
        "    pe_computed_from_batch1 = output[1] - x[1]  # Should also be stored_pe[0]\n",
        "\n",
        "\n",
        "    # Print some debug info\n",
        "    print(f\"Max difference between pe_added_0 and pe_added_1: {(pe_added_0 - pe_added_1).abs().max().item():.10f}\")\n",
        "    print(f\"Max difference between pe_added_0 and stored PE: {(pe_added_0 - pe_matrix[0]).abs().max().item():.10f}\")\n",
        "\n",
        "    # They should be exactly the same (within floating point precision)\n",
        "    if not torch.allclose(pe_computed_from_batch0, pe_matrix[0], rtol=1e-5, atol=1e-6):  # Increased atol to 1e-6\n",
        "        max_diff = (pe_computed_from_batch0 - pe_matrix[0]).abs().max().item()\n",
        "        print(f\"Batch 0: Max diff from stored PE: {max_diff:.10f}\")\n",
        "        print(f\"pe_computed_from_batch0[0,:3]: {pe_computed_from_batch0[0,:3]}\")\n",
        "        print(f\"stored_pe[0,0,:3]: {pe_matrix[0,0,:3]}\")\n",
        "        print(f\"Difference: {pe_computed_from_batch0[0,:3] - pe_matrix[0,0,:3]}\")\n",
        "\n",
        "        # Check if it's just floating point error\n",
        "        if max_diff < 2e-7:  # If difference is really small, it's probably floating point\n",
        "            print(\"Note: Very small difference detected, likely floating-point precision\")\n",
        "            print(\"Adjusting tolerance for this check...\")\n",
        "        else:\n",
        "            raise AssertionError(\"PE computed from batch 0 doesn't match stored PE\")\n",
        "\n",
        "\n",
        "\n",
        "    # Check sinusoidal properties for a few positions\n",
        "    # For position 0 and position 1, the encoding should follow sin/cos pattern\n",
        "    pos0 = pe_matrix[0, 0, :4].cpu().numpy()  # First position, first 4 dimensions\n",
        "    pos1 = pe_matrix[0, 1, :4].cpu().numpy()  # Second position, first 4 dimensions\n",
        "\n",
        "    print(f\"Position 0 (first 4 dims): {pos0}\")\n",
        "    print(f\"Position 1 (first 4 dims): {pos1}\")\n",
        "\n",
        "    # Check alternating sin/cos pattern for position 0\n",
        "    # Even indices should be sin, odd should be cos\n",
        "    # sin(0) = 0, cos(0) = 1\n",
        "    assert abs(pos0[0]) < 1e-5, f\"sin(0) should be 0, got {pos0[0]}\"\n",
        "    assert abs(pos0[1] - 1) < 1e-5, f\"cos(0) should be 1, got {pos0[1]}\"\n",
        "\n",
        "    # For position 1, check the values are reasonable\n",
        "    for i in range(0, 4, 2):\n",
        "        assert abs(pos1[i]) <= 1.0, f\"sin value out of range: {pos1[i]}\"\n",
        "        assert abs(pos1[i+1]) <= 1.0, f\"cos value out of range: {pos1[i+1]}\"\n",
        "\n",
        "    # Test specific properties of sinusoidal encoding\n",
        "    # For dimension 0: frequency = 1/(10000^(0/d_model)) = 1\n",
        "    # So PE(pos, 0) = sin(pos), PE(pos, 1) = cos(pos)\n",
        "\n",
        "    # Check a few more positions to verify the pattern\n",
        "    pos2 = pe_matrix[0, 2, :2].cpu().numpy()  # Third position, first 2 dimensions\n",
        "    print(f\"Position 2 (first 2 dims): {pos2}\")\n",
        "\n",
        "    # Check that values change with position (not all the same)\n",
        "    assert not np.allclose(pos0[:2], pos1[:2], rtol=1e-5), \\\n",
        "        \"Position 0 and 1 encodings should be different\"\n",
        "\n",
        "    print(\"✓ Positional encoding sanity check passed!\\n\")\n",
        "\n",
        "def sanity_check_transformer_forward():\n",
        "    \"\"\"Sanity check for Transformer forward pass\"\"\"\n",
        "    print(\"Running Transformer forward pass sanity check...\")\n",
        "\n",
        "    # Create a small model\n",
        "    test_model = Transformer(\n",
        "        src_vocab_size=100,\n",
        "        tgt_vocab_size=100,\n",
        "        d_model=64,\n",
        "        n_heads=2,\n",
        "        num_encoder_layers=2,\n",
        "        num_decoder_layers=2,\n",
        "        d_ff=128,\n",
        "        max_seq_length=20,\n",
        "        dropout=0.0\n",
        "    ).to(device)\n",
        "\n",
        "    batch_size = 3\n",
        "    seq_len = 10\n",
        "\n",
        "    # Create random input\n",
        "    src = torch.randint(0, 100, (batch_size, seq_len)).to(device)\n",
        "    tgt = torch.randint(0, 100, (batch_size, seq_len)).to(device)\n",
        "\n",
        "    # Create masks\n",
        "    src_mask = test_model.create_src_mask(src)\n",
        "    tgt_mask = test_model.create_tgt_mask(tgt)\n",
        "\n",
        "    # Forward pass\n",
        "    output = test_model(src, tgt, src_mask, tgt_mask)\n",
        "\n",
        "    print(f\"Source shape: {src.shape}\")\n",
        "    print(f\"Target shape: {tgt.shape}\")\n",
        "    print(f\"Output shape: {output.shape}\")\n",
        "\n",
        "    # Check shapes\n",
        "    assert output.shape == (batch_size, seq_len, 100), \\\n",
        "        f\"Output shape {output.shape} incorrect\"\n",
        "\n",
        "    print(\"✓ Transformer forward pass sanity check passed!\\n\")\n",
        "\n",
        "\n",
        "\n",
        "# Run all sanity checks\n",
        "sanity_check_attention()\n",
        "sanity_check_positional_encoding()\n",
        "sanity_check_transformer_forward()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7c25fd34",
      "metadata": {
        "id": "7c25fd34"
      },
      "source": [
        "## 20. Save and Load Model"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0rHR5jixPMVo",
        "outputId": "4dd8b73d-f006-41b0-b8ea-5ee860381471"
      },
      "id": "0rHR5jixPMVo",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "36ae0966",
      "metadata": {
        "lines_to_next_cell": 1,
        "id": "36ae0966",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "09bda08c-8702-4a30-d875-3961436038d6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model saved to /content/drive/MyDrive/models/transformer_summarizer.pth\n"
          ]
        }
      ],
      "source": [
        "def save_model(model, tokenizer, path='/content/drive/MyDrive/models/transformer_summarizer.pth'):\n",
        "    \"\"\"Save model and tokenizer\"\"\"\n",
        "    torch.save({\n",
        "        'model_state_dict': model.state_dict(),\n",
        "        'tokenizer': tokenizer,\n",
        "        'config': {\n",
        "            'src_vocab_size': model.src_vocab_size,\n",
        "            'tgt_vocab_size': model.tgt_vocab_size,\n",
        "            'd_model': model.d_model,\n",
        "            'n_heads': 8,\n",
        "            'num_encoder_layers': 4,\n",
        "            'num_decoder_layers': 4,\n",
        "            'd_ff': 512,\n",
        "            'max_seq_length': 256,\n",
        "            'dropout': 0.1\n",
        "        }\n",
        "    }, path)\n",
        "    print(f\"Model saved to {path}\")\n",
        "\n",
        "def load_model(path='transformer_summarizer.pth'):\n",
        "    \"\"\"Load model and tokenizer\"\"\"\n",
        "    checkpoint = torch.load(path, map_location=device)\n",
        "\n",
        "    config = checkpoint['config']\n",
        "    model = Transformer(**config).to(device)\n",
        "    model.load_state_dict(checkpoint['model_state_dict'])\n",
        "\n",
        "    tokenizer = checkpoint['tokenizer']\n",
        "\n",
        "    print(f\"Model loaded from {path}\")\n",
        "    return model, tokenizer\n",
        "\n",
        "# Save the trained model\n",
        "save_model(model, tokenizer)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0358bbdd",
      "metadata": {
        "id": "0358bbdd"
      },
      "source": [
        "## 21. Interactive Demo\n",
        "\n",
        "Create an interactive demo for testing the model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "adea684d",
      "metadata": {
        "lines_to_next_cell": 1,
        "id": "adea684d"
      },
      "outputs": [],
      "source": [
        "def interactive_demo():\n",
        "    \"\"\"Interactive demo for testing the model\"\"\"\n",
        "    print(\"Interactive TL;DR Generator\")\n",
        "    print(\"=\" * 50)\n",
        "    print(\"Enter an abstract (or 'quit' to exit):\")\n",
        "\n",
        "    while True:\n",
        "        abstract = input(\"\\nAbstract: \")\n",
        "\n",
        "        if abstract.lower() == 'quit':\n",
        "            break\n",
        "\n",
        "        if len(abstract.strip()) < 10:\n",
        "            print(\"Please enter a longer abstract.\")\n",
        "            continue\n",
        "\n",
        "        # Generate summary\n",
        "        summary = generate_summary(model, abstract, tokenizer)\n",
        "\n",
        "        print(f\"\\nGenerated TL;DR: {summary}\")\n",
        "\n",
        "        # Also show ROUGE scores if we have a reference\n",
        "        print(\"\\nWould you like to provide a reference summary for comparison? (y/n)\")\n",
        "        if input().lower() == 'y':\n",
        "            reference = input(\"Reference summary: \")\n",
        "\n",
        "            rouge1 = simple_rouge(summary, reference, n=1)\n",
        "            rouge2 = simple_rouge(summary, reference, n=2)\n",
        "\n",
        "            print(f\"\\nROUGE-1 F1: {rouge1['f1']:.4f}\")\n",
        "            print(f\"ROUGE-2 F1: {rouge2['f1']:.4f}\")\n",
        "\n",
        "# Uncomment to run interactive demo\n",
        "interactive_demo()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b921019d",
      "metadata": {
        "id": "b921019d"
      },
      "source": [
        "## 22. Conclusion\n",
        "\n",
        "We have successfully implemented:\n",
        "\n",
        "1. **A complete Transformer encoder-decoder model** from scratch with:\n",
        "   - Multi-head attention\n",
        "   - Positional encoding\n",
        "   - Feed-forward networks\n",
        "   - Residual connections and layer normalization\n",
        "   - Proper masking for decoder\n",
        "\n",
        "2. **A simple tokenizer** for text processing\n",
        "\n",
        "3. **Training pipeline** with:\n",
        "   - Custom dataset class\n",
        "   - Teacher forcing\n",
        "   - Gradient clipping\n",
        "   - Learning rate scheduling\n",
        "\n",
        "4. **Evaluation metrics** including:\n",
        "   - ROUGE scores (simplified)\n",
        "   - Qualitative analysis\n",
        "\n",
        "5. **Sanity checks** to verify:\n",
        "   - Attention mechanism works correctly\n",
        "   - Positional encoding adds position information\n",
        "   - Model can overfit small dataset\n",
        "   - Forward pass produces correct shapes\n",
        "\n",
        "The model is small enough to train on CPU/GPU with limited resources and demonstrates the core concepts of Transformers for sequence-to-sequence tasks."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "102a5103",
      "metadata": {
        "id": "102a5103"
      },
      "outputs": [],
      "source": [
        "print(\"Notebook completed successfully!\")\n",
        "print(\"\\nModel Summary:\")\n",
        "print(f\"- Parameters: {sum(p.numel() for p in model.parameters()):,}\")\n",
        "print(f\"- d_model: {model.d_model}\")\n",
        "print(f\"- Layers: 3 encoder, 3 decoder\")\n",
        "print(f\"- Heads: 4\")\n",
        "print(f\"- Max sequence length: 128\")\n",
        "print(f\"- Vocabulary size: {vocab_size}\")"
      ]
    }
  ],
  "metadata": {
    "jupytext": {
      "cell_metadata_filter": "-all",
      "main_language": "python",
      "notebook_metadata_filter": "-all"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "toc_visible": true
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}