{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "aa47be9f",
      "metadata": {
        "id": "aa47be9f"
      },
      "source": [
        "# Encoder-Decoder Transformer for TL;DR Summarization\n",
        "\n",
        "This notebook implements a small Transformer model for generating TL;DR summaries from abstracts.\n",
        "\n",
        "## Model Specifications\n",
        "- d_model: 128-256 (we'll use 192)\n",
        "- Layers: 2-4 (we'll use 3 for both encoder and decoder)\n",
        "- Heads: 2-4 (we'll use 4)\n",
        "- Max sequence length: 128-256 (we'll use 128)\n",
        "\n",
        "## Implementation Plan\n",
        "1. Implement basic components (positional encoding, attention, FFN, etc.)\n",
        "2. Build encoder and decoder layers\n",
        "3. Assemble full Transformer model\n",
        "4. Create data processing pipeline\n",
        "5. Train on a small dataset\n",
        "6. Evaluate with ROUGE and qualitative analysis"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5b96e562",
      "metadata": {
        "id": "5b96e562"
      },
      "source": [
        "## 1. Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e3b24efc",
      "metadata": {
        "id": "e3b24efc"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "a99c8049",
      "metadata": {
        "lines_to_next_cell": 1,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a99c8049",
        "outputId": "ec637a29-d635-44a1-d37c-b1894fd369dc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "import math\n",
        "import random\n",
        "import string\n",
        "import re\n",
        "from collections import Counter\n",
        "import matplotlib.pyplot as plt\n",
        "from IPython.display import clear_output\n",
        "import time\n",
        "import json\n",
        "import warnings\n",
        "import os\n",
        "import pickle\n",
        "# from torchtext.datasets import CNNDM\n",
        "from tqdm import tqdm\n",
        "warnings.filterwarnings('ignore')\n",
        "from datasets import load_dataset, Split\n",
        "\n",
        "\n",
        "# Set random seeds for reproducibility\n",
        "torch.manual_seed(42)\n",
        "np.random.seed(42)\n",
        "random.seed(42)\n",
        "\n",
        "# Device configuration\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Using device: {device}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f29a97b8",
      "metadata": {
        "id": "f29a97b8",
        "outputId": "4c117708-4bbd-4848-94c9-d13f0be7af5c"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Generating train split: 100%|██████████| 116722/116722 [00:00<00:00, 201126.05 examples/s]\n",
            "Generating validation split: 100%|██████████| 6447/6447 [00:00<00:00, 200015.37 examples/s]\n",
            "Generating test split: 100%|██████████| 6553/6553 [00:00<00:00, 191354.99 examples/s]\n"
          ]
        }
      ],
      "source": [
        "ds = load_dataset(\"trl-lib/tldr\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "68dfe083",
      "metadata": {
        "id": "68dfe083",
        "outputId": "115f2921-8617-4d3b-c702-28aaa9181af2"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'train': ['prompt', 'completion'],\n",
              " 'validation': ['prompt', 'completion'],\n",
              " 'test': ['prompt', 'completion']}"
            ]
          },
          "execution_count": 26,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "ds.column_names"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "04aee45a",
      "metadata": {
        "id": "04aee45a"
      },
      "source": [
        "## 2. Tokenizer Implementation\n",
        "\n",
        "We'll implement a simple word-level tokenizer with special tokens."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "6a18d8e7",
      "metadata": {
        "lines_to_next_cell": 1,
        "id": "6a18d8e7"
      },
      "outputs": [],
      "source": [
        "class SimpleTokenizer:\n",
        "    \"\"\"Simple word-level tokenizer for demonstration\"\"\"\n",
        "\n",
        "    def __init__(self, vocab_size=5000):\n",
        "        self.vocab_size = vocab_size\n",
        "        self.word2idx = {}\n",
        "        self.idx2word = {}\n",
        "        self.special_tokens = {\n",
        "            '<pad>': 0,\n",
        "            '<sos>': 1,  # Start of sequence\n",
        "            '<eos>': 2,  # End of sequence\n",
        "            '<unk>': 3   # Unknown word\n",
        "        }\n",
        "\n",
        "    def fit(self, texts):\n",
        "        \"\"\"Build vocabulary from texts\"\"\"\n",
        "        word_counts = Counter()\n",
        "\n",
        "        for text in texts:\n",
        "            # Simple text cleaning and tokenization\n",
        "            tokens = self._tokenize(text)\n",
        "            word_counts.update(tokens)\n",
        "\n",
        "        # Keep most common words\n",
        "        most_common = word_counts.most_common(self.vocab_size - len(self.special_tokens))\n",
        "\n",
        "        # Build vocabulary\n",
        "        self.word2idx = self.special_tokens.copy()\n",
        "        self.idx2word = {idx: token for token, idx in self.special_tokens.items()}\n",
        "\n",
        "        # Add most common words\n",
        "        for word, _ in most_common:\n",
        "            idx = len(self.word2idx)\n",
        "            self.word2idx[word] = idx\n",
        "            self.idx2word[idx] = word\n",
        "\n",
        "        return self\n",
        "\n",
        "    def _tokenize(self, text):\n",
        "        \"\"\"Tokenize text into words\"\"\"\n",
        "        # Convert to lowercase and split on whitespace/punctuation\n",
        "        text = text.lower()\n",
        "        # Split on whitespace and punctuation (keeping apostrophes for contractions)\n",
        "        tokens = re.findall(r\"\\b[\\w']+\\b\", text)\n",
        "        return tokens\n",
        "\n",
        "    def encode(self, text, max_length=128, add_special_tokens=True):\n",
        "        \"\"\"Convert text to token indices\"\"\"\n",
        "        tokens = self._tokenize(text)\n",
        "\n",
        "        # Convert to indices\n",
        "        indices = []\n",
        "        if add_special_tokens:\n",
        "            indices.append(self.word2idx['<sos>'])\n",
        "\n",
        "        for token in tokens:\n",
        "            indices.append(self.word2idx.get(token, self.word2idx['<unk>']))\n",
        "\n",
        "        if add_special_tokens:\n",
        "            indices.append(self.word2idx['<eos>'])\n",
        "\n",
        "        # Truncate or pad\n",
        "        if len(indices) > max_length:\n",
        "            indices = indices[:max_length-1] + [self.word2idx['<eos>']]\n",
        "        else:\n",
        "            indices = indices + [self.word2idx['<pad>']] * (max_length - len(indices))\n",
        "\n",
        "        return indices[:max_length]\n",
        "\n",
        "    def decode(self, indices):\n",
        "        \"\"\"Convert token indices back to text\"\"\"\n",
        "        tokens = []\n",
        "        for idx in indices:\n",
        "            if idx == self.word2idx['<pad>']:\n",
        "                continue\n",
        "            if idx == self.word2idx['<sos>']:\n",
        "                continue\n",
        "            if idx == self.word2idx['<eos>']:\n",
        "                break\n",
        "            tokens.append(self.idx2word.get(idx, '<unk>'))\n",
        "\n",
        "        return ' '.join(tokens)\n",
        "\n",
        "    def get_vocab_size(self):\n",
        "        return len(self.word2idx)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9c3d5dcc",
      "metadata": {
        "id": "9c3d5dcc"
      },
      "source": [
        "## 3. Positional Encoding\n",
        "\n",
        "Implement sinusoidal positional encoding as in the original Transformer paper."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "ff6a83c9",
      "metadata": {
        "lines_to_next_cell": 1,
        "id": "ff6a83c9"
      },
      "outputs": [],
      "source": [
        "class PositionalEncoding(nn.Module):\n",
        "    \"\"\"Positional encoding for Transformer models\"\"\"\n",
        "\n",
        "    def __init__(self, d_model, max_len=5000, dropout=0.1):\n",
        "        super(PositionalEncoding, self).__init__()\n",
        "        self.dropout = nn.Dropout(p=dropout)\n",
        "\n",
        "        # Create positional encoding matrix\n",
        "        pe = torch.zeros(max_len, d_model)\n",
        "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
        "        div_term = torch.exp(torch.arange(0, d_model, 2).float() *\n",
        "                            (-math.log(10000.0) / d_model))\n",
        "\n",
        "        pe[:, 0::2] = torch.sin(position * div_term)  # Even indices\n",
        "        pe[:, 1::2] = torch.cos(position * div_term)  # Odd indices\n",
        "\n",
        "        pe = pe.unsqueeze(0)  # Shape: (1, max_len, d_model)\n",
        "        self.register_buffer('pe', pe)\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            x: Tensor of shape (batch_size, seq_len, d_model)\n",
        "        Returns:\n",
        "            Tensor with positional encoding added\n",
        "        \"\"\"\n",
        "        x = x + self.pe[:, :x.size(1)]\n",
        "        return self.dropout(x)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7d6da6a1",
      "metadata": {
        "id": "7d6da6a1"
      },
      "source": [
        "## 4. Multi-Head Attention\n",
        "\n",
        "We'll implement multi-head attention from scratch to understand the core logic."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "ff4d8b11",
      "metadata": {
        "lines_to_next_cell": 1,
        "id": "ff4d8b11"
      },
      "outputs": [],
      "source": [
        "class MultiHeadAttention(nn.Module):\n",
        "    \"\"\"Multi-head attention mechanism\"\"\"\n",
        "\n",
        "    def __init__(self, d_model, n_heads, dropout=0.1):\n",
        "        super(MultiHeadAttention, self).__init__()\n",
        "        assert d_model % n_heads == 0, \"d_model must be divisible by n_heads\"\n",
        "\n",
        "        self.d_model = d_model\n",
        "        self.n_heads = n_heads\n",
        "        self.d_k = d_model // n_heads\n",
        "\n",
        "        # Linear projections for query, key, value\n",
        "        self.w_q = nn.Linear(d_model, d_model)\n",
        "        self.w_k = nn.Linear(d_model, d_model)\n",
        "        self.w_v = nn.Linear(d_model, d_model)\n",
        "\n",
        "        # Output projection\n",
        "        self.w_o = nn.Linear(d_model, d_model)\n",
        "\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, query, key, value, mask=None):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            query, key, value: Tensors of shape (batch_size, seq_len, d_model)\n",
        "            mask: Optional mask tensor\n",
        "        Returns:\n",
        "            Attention output and attention weights\n",
        "        \"\"\"\n",
        "        batch_size = query.size(0)\n",
        "\n",
        "        # Project and reshape for multi-head attention\n",
        "        # (batch_size, seq_len, d_model) -> (batch_size, seq_len, n_heads, d_k)\n",
        "        # -> (batch_size, n_heads, seq_len, d_k)\n",
        "        Q = self.w_q(query).view(batch_size, -1, self.n_heads, self.d_k).transpose(1, 2)\n",
        "        K = self.w_k(key).view(batch_size, -1, self.n_heads, self.d_k).transpose(1, 2)\n",
        "        V = self.w_v(value).view(batch_size, -1, self.n_heads, self.d_k).transpose(1, 2)\n",
        "\n",
        "        # Compute attention scores\n",
        "        # (batch_size, n_heads, seq_len_q, seq_len_k)\n",
        "        scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(self.d_k)\n",
        "\n",
        "        # Apply mask if provided\n",
        "        if mask is not None:\n",
        "            scores = scores.masked_fill(mask == 0, -1e9)\n",
        "\n",
        "        # Apply softmax to get attention weights\n",
        "        attn_weights = F.softmax(scores, dim=-1)\n",
        "        attn_weights = self.dropout(attn_weights)\n",
        "\n",
        "        # Apply attention to values\n",
        "        # (batch_size, n_heads, seq_len_q, d_k)\n",
        "        context = torch.matmul(attn_weights, V)\n",
        "\n",
        "        # Reshape back to original dimensions\n",
        "        # (batch_size, seq_len_q, d_model)\n",
        "        context = context.transpose(1, 2).contiguous().view(\n",
        "            batch_size, -1, self.d_model)\n",
        "\n",
        "        # Final linear projection\n",
        "        output = self.w_o(context)\n",
        "\n",
        "        return output, attn_weights"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0991dffc",
      "metadata": {
        "id": "0991dffc"
      },
      "source": [
        "## 5. Feed-Forward Network\n",
        "\n",
        "Position-wise feed-forward network as in the original Transformer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "2a58b1a2",
      "metadata": {
        "lines_to_next_cell": 1,
        "id": "2a58b1a2"
      },
      "outputs": [],
      "source": [
        "class FeedForward(nn.Module):\n",
        "    \"\"\"Position-wise feed-forward network\"\"\"\n",
        "\n",
        "    def __init__(self, d_model, d_ff=512, dropout=0.1):\n",
        "        super(FeedForward, self).__init__()\n",
        "        self.linear1 = nn.Linear(d_model, d_ff)\n",
        "        self.linear2 = nn.Linear(d_ff, d_model)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.linear2(self.dropout(F.relu(self.linear1(x))))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e7f50434",
      "metadata": {
        "id": "e7f50434"
      },
      "source": [
        "## 6. Encoder Layer\n",
        "\n",
        "A single encoder layer with multi-head self-attention and feed-forward network."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "eef4b95d",
      "metadata": {
        "lines_to_next_cell": 1,
        "id": "eef4b95d"
      },
      "outputs": [],
      "source": [
        "class EncoderLayer(nn.Module):\n",
        "    \"\"\"Single Transformer encoder layer\"\"\"\n",
        "\n",
        "    def __init__(self, d_model, n_heads, d_ff=512, dropout=0.1):\n",
        "        super(EncoderLayer, self).__init__()\n",
        "        self.self_attn = MultiHeadAttention(d_model, n_heads, dropout)\n",
        "        self.feed_forward = FeedForward(d_model, d_ff, dropout)\n",
        "\n",
        "        self.norm1 = nn.LayerNorm(d_model)\n",
        "        self.norm2 = nn.LayerNorm(d_model)\n",
        "\n",
        "        self.dropout1 = nn.Dropout(dropout)\n",
        "        self.dropout2 = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x, mask=None):\n",
        "        # Self-attention with residual connection\n",
        "        attn_output, _ = self.self_attn(x, x, x, mask)\n",
        "        x = x + self.dropout1(attn_output)\n",
        "        x = self.norm1(x)\n",
        "\n",
        "        # Feed-forward with residual connection\n",
        "        ff_output = self.feed_forward(x)\n",
        "        x = x + self.dropout2(ff_output)\n",
        "        x = self.norm2(x)\n",
        "\n",
        "        return x"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fd69a484",
      "metadata": {
        "id": "fd69a484"
      },
      "source": [
        "## 7. Decoder Layer\n",
        "\n",
        "A single decoder layer with masked self-attention, encoder-decoder attention, and feed-forward network."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "87c547ed",
      "metadata": {
        "lines_to_next_cell": 1,
        "id": "87c547ed"
      },
      "outputs": [],
      "source": [
        "class DecoderLayer(nn.Module):\n",
        "    \"\"\"Single Transformer decoder layer\"\"\"\n",
        "\n",
        "    def __init__(self, d_model, n_heads, d_ff=512, dropout=0.1):\n",
        "        super(DecoderLayer, self).__init__()\n",
        "        self.self_attn = MultiHeadAttention(d_model, n_heads, dropout)\n",
        "        self.cross_attn = MultiHeadAttention(d_model, n_heads, dropout)\n",
        "        self.feed_forward = FeedForward(d_model, d_ff, dropout)\n",
        "\n",
        "        self.norm1 = nn.LayerNorm(d_model)\n",
        "        self.norm2 = nn.LayerNorm(d_model)\n",
        "        self.norm3 = nn.LayerNorm(d_model)\n",
        "\n",
        "        self.dropout1 = nn.Dropout(dropout)\n",
        "        self.dropout2 = nn.Dropout(dropout)\n",
        "        self.dropout3 = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x, encoder_output, src_mask=None, tgt_mask=None):\n",
        "        # Masked self-attention with residual connection\n",
        "        attn_output, _ = self.self_attn(x, x, x, tgt_mask)\n",
        "        x = x + self.dropout1(attn_output)\n",
        "        x = self.norm1(x)\n",
        "\n",
        "        # Encoder-decoder attention with residual connection\n",
        "        attn_output, _ = self.cross_attn(x, encoder_output, encoder_output, src_mask)\n",
        "        x = x + self.dropout2(attn_output)\n",
        "        x = self.norm2(x)\n",
        "\n",
        "        # Feed-forward with residual connection\n",
        "        ff_output = self.feed_forward(x)\n",
        "        x = x + self.dropout3(ff_output)\n",
        "        x = self.norm3(x)\n",
        "\n",
        "        return x"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "41d9ca67",
      "metadata": {
        "id": "41d9ca67"
      },
      "source": [
        "## 8. Full Transformer Model\n",
        "\n",
        "Assembling the encoder, decoder, and output layers into a complete Transformer model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "06ac48e1",
      "metadata": {
        "lines_to_next_cell": 1,
        "id": "06ac48e1"
      },
      "outputs": [],
      "source": [
        "class Transformer(nn.Module):\n",
        "    \"\"\"Full Transformer model for sequence-to-sequence tasks\"\"\"\n",
        "\n",
        "    def __init__(self, src_vocab_size, tgt_vocab_size, d_model=192, n_heads=4,\n",
        "                 num_encoder_layers=3, num_decoder_layers=3, d_ff=512,\n",
        "                 max_seq_length=128, dropout=0.1):\n",
        "        super(Transformer, self).__init__()\n",
        "\n",
        "        self.d_model = d_model\n",
        "        self.src_vocab_size = src_vocab_size\n",
        "        self.tgt_vocab_size = tgt_vocab_size\n",
        "\n",
        "        # Embedding layers\n",
        "        self.src_embedding = nn.Embedding(src_vocab_size, d_model)\n",
        "        self.tgt_embedding = nn.Embedding(tgt_vocab_size, d_model)\n",
        "\n",
        "        # Positional encoding\n",
        "        self.positional_encoding = PositionalEncoding(d_model, max_seq_length, dropout)\n",
        "\n",
        "        # Encoder\n",
        "        self.encoder_layers = nn.ModuleList([\n",
        "            EncoderLayer(d_model, n_heads, d_ff, dropout)\n",
        "            for _ in range(num_encoder_layers)\n",
        "        ])\n",
        "\n",
        "        # Decoder\n",
        "        self.decoder_layers = nn.ModuleList([\n",
        "            DecoderLayer(d_model, n_heads, d_ff, dropout)\n",
        "            for _ in range(num_decoder_layers)\n",
        "        ])\n",
        "\n",
        "        # Final output layer\n",
        "        self.output_layer = nn.Linear(d_model, tgt_vocab_size)\n",
        "\n",
        "        # Layer norm for encoder and decoder outputs\n",
        "        self.encoder_norm = nn.LayerNorm(d_model)\n",
        "        self.decoder_norm = nn.LayerNorm(d_model)\n",
        "\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "        # Initialize parameters\n",
        "        self._init_parameters()\n",
        "\n",
        "    def _init_parameters(self):\n",
        "        \"\"\"Initialize parameters with Xavier uniform\"\"\"\n",
        "        for p in self.parameters():\n",
        "            if p.dim() > 1:\n",
        "                nn.init.xavier_uniform_(p)\n",
        "\n",
        "    def encode(self, src, src_mask=None):\n",
        "        \"\"\"Encode source sequence\"\"\"\n",
        "        # Embed source tokens\n",
        "        src_embedded = self.src_embedding(src) * math.sqrt(self.d_model)\n",
        "        src_embedded = self.positional_encoding(src_embedded)\n",
        "        src_embedded = self.dropout(src_embedded)\n",
        "\n",
        "        # Pass through encoder layers\n",
        "        encoder_output = src_embedded\n",
        "        for encoder_layer in self.encoder_layers:\n",
        "            encoder_output = encoder_layer(encoder_output, src_mask)\n",
        "\n",
        "        encoder_output = self.encoder_norm(encoder_output)\n",
        "        return encoder_output\n",
        "\n",
        "    def decode(self, tgt, encoder_output, src_mask=None, tgt_mask=None):\n",
        "        \"\"\"Decode target sequence\"\"\"\n",
        "        # Embed target tokens\n",
        "        tgt_embedded = self.tgt_embedding(tgt) * math.sqrt(self.d_model)\n",
        "        tgt_embedded = self.positional_encoding(tgt_embedded)\n",
        "        tgt_embedded = self.dropout(tgt_embedded)\n",
        "\n",
        "        # Pass through decoder layers\n",
        "        decoder_output = tgt_embedded\n",
        "        for decoder_layer in self.decoder_layers:\n",
        "            decoder_output = decoder_layer(\n",
        "                decoder_output, encoder_output, src_mask, tgt_mask)\n",
        "\n",
        "        decoder_output = self.decoder_norm(decoder_output)\n",
        "        return decoder_output\n",
        "\n",
        "    def forward(self, src, tgt, src_mask=None, tgt_mask=None):\n",
        "        \"\"\"Forward pass through the Transformer\"\"\"\n",
        "        # Encode source\n",
        "        encoder_output = self.encode(src, src_mask)\n",
        "\n",
        "        # Decode target\n",
        "        decoder_output = self.decode(tgt, encoder_output, src_mask, tgt_mask)\n",
        "\n",
        "        # Project to vocabulary\n",
        "        output = self.output_layer(decoder_output)\n",
        "\n",
        "        return output\n",
        "\n",
        "    def generate(self, src, src_mask=None, max_length=128, temperature=1.0):\n",
        "        \"\"\"Generate summary for given source sequence\"\"\"\n",
        "        self.eval()\n",
        "        with torch.no_grad():\n",
        "            # Encode source\n",
        "            encoder_output = self.encode(src, src_mask)\n",
        "\n",
        "            # Start with SOS token\n",
        "            batch_size = src.size(0)\n",
        "            tgt = torch.full((batch_size, 1), 1, dtype=torch.long, device=device)  # SOS token\n",
        "\n",
        "            for _ in range(max_length - 1):\n",
        "                # Create target mask for autoregressive generation\n",
        "                tgt_mask = self.create_decoder_mask(tgt.size(1)).to(device)\n",
        "\n",
        "                # Decode\n",
        "                decoder_output = self.decode(tgt, encoder_output, src_mask, tgt_mask)\n",
        "\n",
        "                # Get next token prediction\n",
        "                next_token_logits = self.output_layer(decoder_output[:, -1, :]) / temperature\n",
        "                next_token_probs = F.softmax(next_token_logits, dim=-1)\n",
        "                next_token = torch.multinomial(next_token_probs, num_samples=1)\n",
        "\n",
        "                # Append to target sequence\n",
        "                tgt = torch.cat([tgt, next_token], dim=1)\n",
        "\n",
        "                # Stop if all sequences generated EOS\n",
        "                if (next_token == 2).all():  # EOS token\n",
        "                    break\n",
        "\n",
        "            return tgt\n",
        "\n",
        "    @staticmethod\n",
        "    def create_src_mask(src, pad_token=0):\n",
        "        \"\"\"Create source mask to ignore padding tokens\"\"\"\n",
        "        return (src != pad_token).unsqueeze(1).unsqueeze(2)\n",
        "\n",
        "    @staticmethod\n",
        "    def create_tgt_mask(tgt, pad_token=0):\n",
        "        \"\"\"Create target mask for autoregressive generation\"\"\"\n",
        "        batch_size, seq_len = tgt.size()\n",
        "\n",
        "        # Padding mask\n",
        "        pad_mask = (tgt != pad_token).unsqueeze(1).unsqueeze(2)\n",
        "\n",
        "        # Subsequent mask to prevent looking ahead\n",
        "        subsequent_mask = torch.tril(torch.ones(seq_len, seq_len, device=tgt.device)).bool()\n",
        "        subsequent_mask = subsequent_mask.unsqueeze(0).unsqueeze(0)\n",
        "\n",
        "        # Combine masks\n",
        "        tgt_mask = pad_mask & subsequent_mask\n",
        "\n",
        "        return tgt_mask\n",
        "\n",
        "    @staticmethod\n",
        "    def create_decoder_mask(seq_len):\n",
        "        \"\"\"Create decoder mask for autoregressive generation\"\"\"\n",
        "        mask = torch.tril(torch.ones(seq_len, seq_len)).bool()\n",
        "        return mask.unsqueeze(0).unsqueeze(0)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c04d87f1",
      "metadata": {
        "id": "c04d87f1"
      },
      "source": [
        "## 9. Data Preparation\n",
        "\n",
        "Create a synthetic dataset for training and evaluation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "cc3804a4",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cc3804a4",
        "outputId": "e7e887a4-c837-4668-9ec6-2b937aa8e18e"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Dataset({\n",
              "    features: ['Unnamed: 0.1', 'Unnamed: 0', 'title', 'abstract'],\n",
              "    num_rows: 117592\n",
              "})"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ],
      "source": [
        "dataset = load_dataset(\"CShorten/ML-ArXiv-Papers\", split=\"train\")\n",
        "dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "id": "009abe9f",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "009abe9f",
        "outputId": "22a2263d-edf8-495e-cc9d-111148cc4d6c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "============================================================\n",
            "LOADING ARXIV DATASET FROM HUGGING FACE\n",
            "============================================================\n",
            "Dataset: CShorten/ML-ArXiv-Papers\n",
            "Target samples: 10000\n",
            "\n",
            "Downloading dataset...\n",
            "✓ Dataset loaded: 117592 papers available\n",
            "✓ Extracted 10000 quality abstracts\n",
            "✓ Average length: 152.3 words\n",
            "============================================================\n",
            "Training samples: 8000\n",
            "Validation samples: 2000\n",
            "\n",
            "Example:\n",
            "Abstract: The problem of statistical learning is to construct a predictor of a random variable $Y$ as a function of a related random variable $X$ on the basis of an i.i.d. training sample from the joint distribution of $(X,Y)$. Allowable predictors are drawn from some specified class, and the goal is to approach asymptotically the performance (expected loss) of the best predictor in the class. We consider the setting in which one has perfect observation of the $X$-part of the sample, while the $Y$-part has to be communicated at some finite bit rate. The encoding of the $Y$-values is allowed to depend on the $X$-values. Under suitable regularity conditions on the admissible predictors, the underlying family of probability distributions and the loss function, we give an information-theoretic characterization of achievable predictor performance in terms of conditional distortion-rate functions. The ideas are illustrated on the example of nonparametric regression in Gaussian noise.\n",
            "Summary: Learning from compressed observations\n"
          ]
        }
      ],
      "source": [
        "def load_arxiv_huggingface(num_samples=10000):\n",
        "    \"\"\"Load ML ArXiv papers from Hugging Face.\"\"\"\n",
        "    print(\"=\" * 60)\n",
        "    print(\"LOADING ARXIV DATASET FROM HUGGING FACE\")\n",
        "    print(\"=\" * 60)\n",
        "    print(f\"Dataset: CShorten/ML-ArXiv-Papers\")\n",
        "    print(f\"Target samples: {num_samples}\\n\")\n",
        "\n",
        "    print(\"Downloading dataset...\")\n",
        "    dataset = load_dataset(\"CShorten/ML-ArXiv-Papers\", split=\"train\")\n",
        "    print(f\"✓ Dataset loaded: {len(dataset)} papers available\")\n",
        "\n",
        "    abstracts, summaries = [], []\n",
        "    for i, paper in enumerate(dataset):\n",
        "        if len(abstracts) >= num_samples:\n",
        "            break\n",
        "\n",
        "        abstract = paper['abstract'].strip()\n",
        "        summary = paper['title'].strip()\n",
        "\n",
        "        # Filter: reasonable length abstracts\n",
        "        if 100 < len(abstract) < 5000:\n",
        "            abstract = ' '.join(abstract.split())\n",
        "            abstracts.append(abstract)\n",
        "\n",
        "        if 10 < len(summary) < 500:\n",
        "            abstract = ' '.join(abstract.split())\n",
        "            summaries.append(summary)\n",
        "\n",
        "    print(f\"✓ Extracted {len(abstracts)} quality abstracts\")\n",
        "    print(f\"✓ Average length: {sum(len(a.split()) for a in abstracts) / len(abstracts):.1f} words\")\n",
        "    print(\"=\" * 60)\n",
        "\n",
        "    return abstracts, summaries\n",
        "\n",
        "train_split = 0.8\n",
        "num_samples = 10000\n",
        "articles, summaries = load_arxiv_huggingface(num_samples)\n",
        "\n",
        "\n",
        "train_abstracts, train_summaries = articles[:int(train_split * num_samples)], summaries[:int(train_split * num_samples)]\n",
        "val_abstracts, val_summaries = articles[int(train_split * num_samples):], summaries[int(train_split * num_samples):]\n",
        "\n",
        "print(f\"Training samples: {len(train_abstracts)}\")\n",
        "print(f\"Validation samples: {len(val_abstracts)}\")\n",
        "print(\"\\nExample:\")\n",
        "print(f\"Abstract: {train_abstracts[0]}\")\n",
        "print(f\"Summary: {train_summaries[0]}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "21507e7c",
      "metadata": {
        "id": "21507e7c"
      },
      "source": [
        "## 10. Initialize Tokenizer and Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "id": "94a4bdfd",
      "metadata": {
        "lines_to_next_cell": 1,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "94a4bdfd",
        "outputId": "e26e4251-ee59-4a32-a964-32fb5ab82837"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vocabulary size: 10000\n",
            "Model parameters: 12,962,576\n"
          ]
        }
      ],
      "source": [
        "# Initialize tokenizer\n",
        "tokenizer = SimpleTokenizer(vocab_size=10000)\n",
        "\n",
        "# Fit tokenizer on all texts\n",
        "all_texts = train_abstracts + train_summaries + val_abstracts + val_summaries\n",
        "tokenizer.fit(all_texts)\n",
        "\n",
        "vocab_size = tokenizer.get_vocab_size()\n",
        "print(f\"Vocabulary size: {vocab_size}\")\n",
        "\n",
        "# Initialize model\n",
        "model = Transformer(\n",
        "    src_vocab_size=vocab_size,\n",
        "    tgt_vocab_size=vocab_size,\n",
        "    d_model=256,\n",
        "    n_heads=8,\n",
        "    num_encoder_layers=4,\n",
        "    num_decoder_layers=4,\n",
        "    d_ff=512,\n",
        "    max_seq_length=256,\n",
        "    dropout=0.1\n",
        ").to(device)\n",
        "\n",
        "print(f\"Model parameters: {sum(p.numel() for p in model.parameters()):,}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4388eb2b",
      "metadata": {
        "id": "4388eb2b"
      },
      "source": [
        "## 11. Dataset and DataLoader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "id": "c9943e80",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c9943e80",
        "outputId": "443e5398-46a2-42f1-d453-543fceb0a95d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train batches: 250\n",
            "Val batches: 63\n"
          ]
        }
      ],
      "source": [
        "class SummarizationDataset(torch.utils.data.Dataset):\n",
        "    \"\"\"Dataset for abstract-summary pairs\"\"\"\n",
        "\n",
        "    def __init__(self, abstracts, summaries, tokenizer, max_length=128):\n",
        "        self.abstracts = abstracts\n",
        "        self.summaries = summaries\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_length = max_length\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.abstracts)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # Encode abstract and summary\n",
        "        src = self.tokenizer.encode(\n",
        "            self.abstracts[idx],\n",
        "            max_length=self.max_length,\n",
        "            add_special_tokens=True\n",
        "        )\n",
        "\n",
        "        tgt = self.tokenizer.encode(\n",
        "            self.summaries[idx],\n",
        "            max_length=self.max_length,\n",
        "            add_special_tokens=True\n",
        "        )\n",
        "\n",
        "        # Input for decoder (shifted right)\n",
        "        tgt_input = tgt[:-1]\n",
        "        tgt_output = tgt[1:]  # Shifted by one for teacher forcing\n",
        "\n",
        "        return {\n",
        "            'src': torch.tensor(src, dtype=torch.long),\n",
        "            'tgt_input': torch.tensor(tgt_input, dtype=torch.long),\n",
        "            'tgt_output': torch.tensor(tgt_output, dtype=torch.long)\n",
        "        }\n",
        "\n",
        "# Create datasets and dataloaders\n",
        "train_dataset = SummarizationDataset(train_abstracts, train_summaries, tokenizer)\n",
        "val_dataset = SummarizationDataset(val_abstracts, val_summaries, tokenizer)\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(\n",
        "    train_dataset, batch_size=32, shuffle=True, num_workers=0\n",
        ")\n",
        "val_loader = torch.utils.data.DataLoader(\n",
        "    val_dataset, batch_size=32, shuffle=False, num_workers=0\n",
        ")\n",
        "\n",
        "print(f\"Train batches: {len(train_loader)}\")\n",
        "print(f\"Val batches: {len(val_loader)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "77c70b01",
      "metadata": {
        "id": "77c70b01"
      },
      "source": [
        "## 12. Training Configuration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "id": "a513ce99",
      "metadata": {
        "lines_to_next_cell": 1,
        "id": "a513ce99"
      },
      "outputs": [],
      "source": [
        "# Loss function (ignoring padding tokens)\n",
        "criterion = nn.CrossEntropyLoss(ignore_index=tokenizer.word2idx['<pad>'])\n",
        "\n",
        "# Optimizer\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.0001, betas=(0.9, 0.98), eps=1e-9)\n",
        "\n",
        "# Learning rate scheduler\n",
        "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.5)\n",
        "\n",
        "# Training parameters\n",
        "num_epochs = 30\n",
        "clip_grad = 1.0  # Gradient clipping"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bc3057cf",
      "metadata": {
        "id": "bc3057cf"
      },
      "source": [
        "## 13. Training Loop"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "id": "ebaed5e4",
      "metadata": {
        "lines_to_next_cell": 1,
        "id": "ebaed5e4"
      },
      "outputs": [],
      "source": [
        "def train_epoch(model, data_loader, optimizer, criterion, clip_grad):\n",
        "    \"\"\"Train for one epoch\"\"\"\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "\n",
        "    for batch in data_loader:\n",
        "        src = batch['src'].to(device)\n",
        "        tgt_input = batch['tgt_input'].to(device)\n",
        "        tgt_output = batch['tgt_output'].to(device)\n",
        "\n",
        "        # Create masks\n",
        "        src_mask = model.create_src_mask(src, tokenizer.word2idx['<pad>'])\n",
        "        tgt_mask = model.create_tgt_mask(tgt_input, tokenizer.word2idx['<pad>'])\n",
        "\n",
        "        # Forward pass\n",
        "        optimizer.zero_grad()\n",
        "        output = model(src, tgt_input, src_mask, tgt_mask)\n",
        "\n",
        "        # Reshape for loss calculation\n",
        "        output = output.view(-1, output.size(-1))\n",
        "        tgt_output = tgt_output.view(-1)\n",
        "\n",
        "        # Calculate loss\n",
        "        loss = criterion(output, tgt_output)\n",
        "\n",
        "        # Backward pass\n",
        "        loss.backward()\n",
        "\n",
        "        # Gradient clipping\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip_grad)\n",
        "\n",
        "        # Update parameters\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "\n",
        "    return total_loss / len(data_loader)\n",
        "\n",
        "def evaluate(model, data_loader, criterion):\n",
        "    \"\"\"Evaluate model on validation set\"\"\"\n",
        "    model.eval()\n",
        "    total_loss = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in data_loader:\n",
        "            src = batch['src'].to(device)\n",
        "            tgt_input = batch['tgt_input'].to(device)\n",
        "            tgt_output = batch['tgt_output'].to(device)\n",
        "\n",
        "            # Create masks\n",
        "            src_mask = model.create_src_mask(src, tokenizer.word2idx['<pad>'])\n",
        "            tgt_mask = model.create_tgt_mask(tgt_input, tokenizer.word2idx['<pad>'])\n",
        "\n",
        "            # Forward pass\n",
        "            output = model(src, tgt_input, src_mask, tgt_mask)\n",
        "\n",
        "            # Reshape for loss calculation\n",
        "            output = output.view(-1, output.size(-1))\n",
        "            tgt_output = tgt_output.view(-1)\n",
        "\n",
        "            # Calculate loss\n",
        "            loss = criterion(output, tgt_output)\n",
        "\n",
        "            total_loss += loss.item()\n",
        "\n",
        "    return total_loss / len(data_loader)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "43c389c5",
      "metadata": {
        "id": "43c389c5"
      },
      "source": [
        "## 14. Training Execution"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5763c088",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5763c088",
        "outputId": "e6598b9d-b36a-49cc-95b1-6fc750872307"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting training...\n",
            "Epoch   1/30 | Train Loss: 6.9369 | Val Loss: 6.2788 | Time: 29.92s\n",
            "Epoch   2/30 | Train Loss: 6.0636 | Val Loss: 6.0793 | Time: 30.41s\n",
            "Epoch   3/30 | Train Loss: 5.8293 | Val Loss: 5.9616 | Time: 30.42s\n",
            "Epoch   4/30 | Train Loss: 5.6840 | Val Loss: 5.9156 | Time: 30.27s\n",
            "Epoch   5/30 | Train Loss: 5.5780 | Val Loss: 5.8844 | Time: 30.39s\n",
            "\n",
            "--- Example Generation ---\n",
            "Abstract: clustering is a powerful tool in data analysis but it is often difficult to find a grouping that aligns with a user's needs to address this several methods incorporate constraints obtained from users into clustering algorithms but unfortunately do not apply to hierarchical clustering we design an interactive bayesian algorithm that incorporates user interaction into hierarchical clustering while still utilizing the geometry of the data by sampling a constrained posterior distribution over hierarchies we also suggest several ways to intelligently query a user the algorithm along with the querying schemes shows promising results on real data\n",
            "Original Summary: minimum conditional description length estimation for markov random fields\n",
            "Generated Summary: rank on mapreduce optimization for regret problem\n",
            "--------------------------\n",
            "\n",
            "Epoch   6/30 | Train Loss: 5.4795 | Val Loss: 5.8352 | Time: 30.43s\n",
            "Epoch   7/30 | Train Loss: 5.3949 | Val Loss: 5.7939 | Time: 30.38s\n",
            "Epoch   8/30 | Train Loss: 5.3114 | Val Loss: 5.7741 | Time: 30.38s\n",
            "Epoch   9/30 | Train Loss: 5.2213 | Val Loss: 5.7535 | Time: 30.35s\n",
            "Epoch  10/30 | Train Loss: 5.1397 | Val Loss: 5.7373 | Time: 30.36s\n",
            "\n",
            "--- Example Generation ---\n",
            "Abstract: deep convolutional networks provide state of the art classifications and regressions results over many high dimensional problems we review their architecture which <unk> data with a cascade of linear filter weights and non linearities a mathematical framework is introduced to analyze their properties computations of invariants involve multiscale contractions the linearization of hierarchical symmetries and sparse <unk> applications are discussed\n",
            "Original Summary: spectral theory of <unk> and signed graphs applications to graph clustering a survey\n",
            "Generated Summary: rapid verifying planning on genomic detection with unknown uniform matching\n",
            "--------------------------\n",
            "\n",
            "Epoch  11/30 | Train Loss: 5.0384 | Val Loss: 5.7242 | Time: 30.34s\n",
            "Epoch  12/30 | Train Loss: 4.9912 | Val Loss: 5.7203 | Time: 30.34s\n",
            "Epoch  13/30 | Train Loss: 4.9459 | Val Loss: 5.7442 | Time: 30.34s\n",
            "Epoch  14/30 | Train Loss: 4.9060 | Val Loss: 5.7240 | Time: 30.38s\n",
            "Epoch  15/30 | Train Loss: 4.8610 | Val Loss: 5.7664 | Time: 30.43s\n",
            "\n",
            "--- Example Generation ---\n",
            "Abstract: adaptive learning rate algorithms such as rmsprop are widely used for training deep neural networks rmsprop offers efficient training since it uses first order gradients to approximate hessian based preconditioning however since the first order gradients include noise caused by stochastic optimization the approximation may be inaccurate in this paper we propose a novel adaptive learning rate algorithm called <unk> its key idea is effective handling of the noise by preconditioning based on covariance matrix for various neural networks our approach is more efficient and effective than rmsprop and its variant\n",
            "Original Summary: a neural autoregressive approach to collaborative filtering\n",
            "Generated Summary: graph catalogs for self secant in stochastic <unk>\n",
            "--------------------------\n",
            "\n",
            "Epoch  16/30 | Train Loss: 4.8162 | Val Loss: 5.7586 | Time: 30.42s\n",
            "Epoch  17/30 | Train Loss: 4.7784 | Val Loss: 5.7275 | Time: 30.45s\n",
            "Epoch  18/30 | Train Loss: 4.7356 | Val Loss: 5.7280 | Time: 30.43s\n"
          ]
        }
      ],
      "source": [
        "# Training history\n",
        "train_losses = []\n",
        "val_losses = []\n",
        "\n",
        "print(\"Starting training...\")\n",
        "for epoch in range(num_epochs):\n",
        "    start_time = time.time()\n",
        "\n",
        "    # Train\n",
        "    train_loss = train_epoch(model, train_loader, optimizer, criterion, clip_grad)\n",
        "    train_losses.append(train_loss)\n",
        "\n",
        "    # Evaluate\n",
        "    val_loss = evaluate(model, val_loader, criterion)\n",
        "    val_losses.append(val_loss)\n",
        "\n",
        "    # Update learning rate\n",
        "    scheduler.step()\n",
        "\n",
        "    # Print progress\n",
        "    epoch_time = time.time() - start_time\n",
        "    print(f\"Epoch {epoch+1:3d}/{num_epochs} | \"\n",
        "          f\"Train Loss: {train_loss:.4f} | \"\n",
        "          f\"Val Loss: {val_loss:.4f} | \"\n",
        "          f\"Time: {epoch_time:.2f}s\")\n",
        "\n",
        "    # Generate example every 5 epochs\n",
        "    if (epoch + 1) % 5 == 0:\n",
        "        model.eval()\n",
        "        with torch.no_grad():\n",
        "            # Get a sample from validation set\n",
        "            sample_idx = random.randint(0, len(val_dataset) - 1)\n",
        "            sample = val_dataset[sample_idx]\n",
        "\n",
        "            src = sample['src'].unsqueeze(0).to(device)\n",
        "            src_mask = model.create_src_mask(src, tokenizer.word2idx['<pad>'])\n",
        "\n",
        "            # Generate summary\n",
        "            generated = model.generate(src, src_mask, max_length=30)\n",
        "\n",
        "            # Decode\n",
        "            original_abstract = tokenizer.decode(sample['src'].tolist())\n",
        "            original_summary = tokenizer.decode(sample['tgt_output'].tolist())\n",
        "            generated_summary = tokenizer.decode(generated[0].tolist())\n",
        "\n",
        "            print(\"\\n--- Example Generation ---\")\n",
        "            print(f\"Abstract: {original_abstract}\")\n",
        "            print(f\"Original Summary: {original_summary}\")\n",
        "            print(f\"Generated Summary: {generated_summary}\")\n",
        "            print(\"--------------------------\\n\")\n",
        "\n",
        "print(\"Training complete!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f478ae5f",
      "metadata": {
        "id": "f478ae5f"
      },
      "source": [
        "## 15. Loss Curve Visualization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "721d9dde",
      "metadata": {
        "lines_to_next_cell": 1,
        "id": "721d9dde",
        "outputId": "f173342c-0eb2-4f32-a873-a1cf5a1a39f7"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 1000x500 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Plot training and validation loss\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.plot(train_losses, label='Training Loss', linewidth=2)\n",
        "plt.plot(val_losses, label='Validation Loss', linewidth=2)\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.title('Training and Validation Loss Curves')\n",
        "plt.legend()\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1c05b271",
      "metadata": {
        "id": "1c05b271"
      },
      "source": [
        "## 16. ROUGE Score Implementation\n",
        "\n",
        "Implement a simplified version of ROUGE for evaluation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3e2389a6",
      "metadata": {
        "lines_to_next_cell": 1,
        "id": "3e2389a6"
      },
      "outputs": [],
      "source": [
        "def simple_rouge(generated, reference, n=1):\n",
        "    \"\"\"Simplified ROUGE-N score calculation\"\"\"\n",
        "\n",
        "    def get_ngrams(text, n):\n",
        "        words = text.lower().split()\n",
        "        ngrams = []\n",
        "        for i in range(len(words) - n + 1):\n",
        "            ngrams.append(tuple(words[i:i+n]))\n",
        "        return set(ngrams)\n",
        "\n",
        "    gen_ngrams = get_ngrams(generated, n)\n",
        "    ref_ngrams = get_ngrams(reference, n)\n",
        "\n",
        "    if len(gen_ngrams) == 0:\n",
        "        return 0.0\n",
        "\n",
        "    # Calculate ROUGE-N precision, recall, and F1\n",
        "    overlap = len(gen_ngrams.intersection(ref_ngrams))\n",
        "    precision = overlap / len(gen_ngrams)\n",
        "    recall = overlap / len(ref_ngrams) if len(ref_ngrams) > 0 else 0\n",
        "\n",
        "    if precision + recall > 0:\n",
        "        f1 = 2 * precision * recall / (precision + recall)\n",
        "    else:\n",
        "        f1 = 0\n",
        "\n",
        "    return {'precision': precision, 'recall': recall, 'f1': f1}\n",
        "\n",
        "def evaluate_rouge(model, dataset, tokenizer, num_samples=20):\n",
        "    \"\"\"Evaluate model using ROUGE scores\"\"\"\n",
        "    model.eval()\n",
        "\n",
        "    rouge1_scores = {'precision': [], 'recall': [], 'f1': []}\n",
        "    rouge2_scores = {'precision': [], 'recall': [], 'f1': []}\n",
        "\n",
        "    with torch.no_grad():\n",
        "        # Randomly sample from dataset\n",
        "        indices = random.sample(range(len(dataset)), min(num_samples, len(dataset)))\n",
        "\n",
        "        for idx in indices:\n",
        "            sample = dataset[idx]\n",
        "\n",
        "            src = sample['src'].unsqueeze(0).to(device)\n",
        "            src_mask = model.create_src_mask(src, tokenizer.word2idx['<pad>'])\n",
        "\n",
        "            # Generate summary\n",
        "            generated = model.generate(src, src_mask, max_length=30)\n",
        "\n",
        "            # Decode\n",
        "            original_summary = tokenizer.decode(sample['tgt_output'].tolist())\n",
        "            generated_summary = tokenizer.decode(generated[0].tolist())\n",
        "\n",
        "            # Calculate ROUGE scores\n",
        "            rouge1 = simple_rouge(generated_summary, original_summary, n=1)\n",
        "            rouge2 = simple_rouge(generated_summary, original_summary, n=2)\n",
        "\n",
        "            for metric in ['precision', 'recall', 'f1']:\n",
        "                rouge1_scores[metric].append(rouge1[metric])\n",
        "                rouge2_scores[metric].append(rouge2[metric])\n",
        "\n",
        "    # Calculate average scores\n",
        "    avg_rouge1 = {metric: np.mean(scores) for metric, scores in rouge1_scores.items()}\n",
        "    avg_rouge2 = {metric: np.mean(scores) for metric, scores in rouge2_scores.items()}\n",
        "\n",
        "    return avg_rouge1, avg_rouge2\n",
        "\n",
        "# Evaluate model\n",
        "rouge1_scores, rouge2_scores = evaluate_rouge(model, val_dataset, tokenizer, num_samples=50)\n",
        "\n",
        "print(\"ROUGE Evaluation Results:\")\n",
        "print(f\"ROUGE-1 Precision: {rouge1_scores['precision']:.4f}\")\n",
        "print(f\"ROUGE-1 Recall: {rouge1_scores['recall']:.4f}\")\n",
        "print(f\"ROUGE-1 F1: {rouge1_scores['f1']:.4f}\")\n",
        "print()\n",
        "print(f\"ROUGE-2 Precision: {rouge2_scores['precision']:.4f}\")\n",
        "print(f\"ROUGE-2 Recall: {rouge2_scores['recall']:.4f}\")\n",
        "print(f\"ROUGE-2 F1: {rouge2_scores['f1']:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "20d2a08b",
      "metadata": {
        "id": "20d2a08b"
      },
      "source": [
        "## 17. Qualitative Evaluation\n",
        "\n",
        "Generate and evaluate some sample summaries."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "40257298",
      "metadata": {
        "lines_to_next_cell": 1,
        "id": "40257298",
        "outputId": "dbc49563-a43b-4256-a644-1695c2ea74c5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Qualitative Evaluation:\n",
            "================================================================================\n",
            "\n",
            "Example 1:\n",
            "Abstract: the study analyzed the relationship between treatment and control groups in a clinical experiment with significant results showing positive effects\n",
            "Generated TL;DR: causing so three 2 <unk> haven't the my entire but times my was <unk> <unk> to <unk> he's 21 close <unk> the during the same way wouldn't time buy\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "Example 2:\n",
            "Abstract: research findings demonstrate a correlation between data analysis methods and experimental outcomes with important implications for future work\n",
            "Generated TL;DR: doctor last <unk> the <unk> each class with don't <unk> <unk> slowly <unk> the <unk> on <unk> opinions wants killing health <unk> in the bed mostly need an <unk>\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "Example 3:\n",
            "Abstract: the experiment evaluated participants in different conditions and measured the impact of various factors on the final results\n",
            "Generated TL;DR: in <unk> basically what <unk>\n",
            "--------------------------------------------------------------------------------\n"
          ]
        }
      ],
      "source": [
        "def generate_summary(model, abstract, tokenizer, max_length=30):\n",
        "    \"\"\"Generate summary for a given abstract\"\"\"\n",
        "    model.eval()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        # Encode abstract\n",
        "        src = torch.tensor(\n",
        "            tokenizer.encode(abstract, max_length=128, add_special_tokens=True),\n",
        "            dtype=torch.long\n",
        "        ).unsqueeze(0).to(device)\n",
        "\n",
        "        src_mask = model.create_src_mask(src, tokenizer.word2idx['<pad>'])\n",
        "\n",
        "        # Generate summary\n",
        "        generated = model.generate(src, src_mask, max_length=max_length)\n",
        "\n",
        "        # Decode\n",
        "        generated_summary = tokenizer.decode(generated[0].tolist())\n",
        "\n",
        "        return generated_summary\n",
        "\n",
        "# Test with some example abstracts\n",
        "test_abstracts = [\n",
        "    \"the study analyzed the relationship between treatment and control groups \"\n",
        "    \"in a clinical experiment with significant results showing positive effects\",\n",
        "\n",
        "    \"research findings demonstrate a correlation between data analysis methods \"\n",
        "    \"and experimental outcomes with important implications for future work\",\n",
        "\n",
        "    \"the experiment evaluated participants in different conditions and measured \"\n",
        "    \"the impact of various factors on the final results\"\n",
        "]\n",
        "\n",
        "print(\"Qualitative Evaluation:\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "for i, abstract in enumerate(test_abstracts):\n",
        "    generated = generate_summary(model, abstract, tokenizer)\n",
        "    print(f\"\\nExample {i+1}:\")\n",
        "    print(f\"Abstract: {abstract}\")\n",
        "    print(f\"Generated TL;DR: {generated}\")\n",
        "    print(\"-\" * 80)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a92ab861",
      "metadata": {
        "id": "a92ab861"
      },
      "source": [
        "## 19. Sanity Check Experiments\n",
        "\n",
        "Let's run some sanity checks to verify our implementation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0aa9cd75",
      "metadata": {
        "lines_to_next_cell": 1,
        "id": "0aa9cd75"
      },
      "outputs": [],
      "source": [
        "def sanity_check_attention():\n",
        "    \"\"\"Sanity check for attention mechanism\"\"\"\n",
        "    print(\"Running attention sanity check...\")\n",
        "\n",
        "    # Create a small attention module\n",
        "    d_model = 64\n",
        "    n_heads = 4\n",
        "    batch_size = 2\n",
        "    seq_len = 5\n",
        "\n",
        "    attention = MultiHeadAttention(d_model, n_heads).to(device)\n",
        "\n",
        "    # Create random input\n",
        "    x = torch.randn(batch_size, seq_len, d_model).to(device)\n",
        "\n",
        "    # Self-attention\n",
        "    output, weights = attention(x, x, x)\n",
        "\n",
        "    print(f\"Input shape: {x.shape}\")\n",
        "    print(f\"Output shape: {output.shape}\")\n",
        "    print(f\"Attention weights shape: {weights.shape}\")\n",
        "\n",
        "    # Check shapes\n",
        "    assert output.shape == x.shape, f\"Output shape {output.shape} != input shape {x.shape}\"\n",
        "    assert weights.shape == (batch_size, n_heads, seq_len, seq_len), \\\n",
        "        f\"Attention weights shape {weights.shape} incorrect\"\n",
        "\n",
        "    print(\"✓ Attention sanity check passed!\\n\")\n",
        "\n",
        "def sanity_check_positional_encoding():\n",
        "    \"\"\"Sanity check for positional encoding\"\"\"\n",
        "    print(\"Running positional encoding sanity check...\")\n",
        "\n",
        "    d_model = 64\n",
        "    seq_len = 10\n",
        "    batch_size = 2\n",
        "\n",
        "    pe = PositionalEncoding(d_model, max_len=100).to(device)\n",
        "\n",
        "    # Create random input\n",
        "    x = torch.randn(batch_size, seq_len, d_model).to(device)\n",
        "\n",
        "    # Apply positional encoding\n",
        "    output = pe(x)\n",
        "\n",
        "    print(f\"Input shape: {x.shape}\")\n",
        "    print(f\"Output shape: {output.shape}\")\n",
        "\n",
        "    # Check that output is different from input\n",
        "    assert not torch.allclose(x, output), \"Positional encoding didn't change input\"\n",
        "\n",
        "    # Check that encoding is the same for all batches\n",
        "    diff = output[0] - output[1]\n",
        "    pos_encoding_diff = torch.abs(diff).max().item()\n",
        "\n",
        "    print(f\"Max difference between batches: {pos_encoding_diff}\")\n",
        "    assert pos_encoding_diff < 1e-6, \"Positional encoding differs between batches\"\n",
        "\n",
        "    print(\"✓ Positional encoding sanity check passed!\\n\")\n",
        "\n",
        "def sanity_check_transformer_forward():\n",
        "    \"\"\"Sanity check for Transformer forward pass\"\"\"\n",
        "    print(\"Running Transformer forward pass sanity check...\")\n",
        "\n",
        "    # Create a small model\n",
        "    test_model = Transformer(\n",
        "        src_vocab_size=100,\n",
        "        tgt_vocab_size=100,\n",
        "        d_model=64,\n",
        "        n_heads=2,\n",
        "        num_encoder_layers=2,\n",
        "        num_decoder_layers=2,\n",
        "        d_ff=128,\n",
        "        max_seq_length=20,\n",
        "        dropout=0.0\n",
        "    ).to(device)\n",
        "\n",
        "    batch_size = 3\n",
        "    seq_len = 10\n",
        "\n",
        "    # Create random input\n",
        "    src = torch.randint(0, 100, (batch_size, seq_len)).to(device)\n",
        "    tgt = torch.randint(0, 100, (batch_size, seq_len)).to(device)\n",
        "\n",
        "    # Create masks\n",
        "    src_mask = test_model.create_src_mask(src)\n",
        "    tgt_mask = test_model.create_tgt_mask(tgt)\n",
        "\n",
        "    # Forward pass\n",
        "    output = test_model(src, tgt, src_mask, tgt_mask)\n",
        "\n",
        "    print(f\"Source shape: {src.shape}\")\n",
        "    print(f\"Target shape: {tgt.shape}\")\n",
        "    print(f\"Output shape: {output.shape}\")\n",
        "\n",
        "    # Check shapes\n",
        "    assert output.shape == (batch_size, seq_len, 100), \\\n",
        "        f\"Output shape {output.shape} incorrect\"\n",
        "\n",
        "    print(\"✓ Transformer forward pass sanity check passed!\\n\")\n",
        "\n",
        "# Run all sanity checks\n",
        "sanity_check_attention()\n",
        "sanity_check_positional_encoding()\n",
        "sanity_check_transformer_forward()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7c25fd34",
      "metadata": {
        "id": "7c25fd34"
      },
      "source": [
        "## 20. Save and Load Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "36ae0966",
      "metadata": {
        "lines_to_next_cell": 1,
        "id": "36ae0966"
      },
      "outputs": [],
      "source": [
        "def save_model(model, tokenizer, path='transformer_summarizer.pth'):\n",
        "    \"\"\"Save model and tokenizer\"\"\"\n",
        "    torch.save({\n",
        "        'model_state_dict': model.state_dict(),\n",
        "        'tokenizer': tokenizer,\n",
        "        'config': {\n",
        "            'src_vocab_size': model.src_vocab_size,\n",
        "            'tgt_vocab_size': model.tgt_vocab_size,\n",
        "            'd_model': model.d_model,\n",
        "            'n_heads': 4,\n",
        "            'num_encoder_layers': 3,\n",
        "            'num_decoder_layers': 3,\n",
        "            'd_ff': 512,\n",
        "            'max_seq_length': 128,\n",
        "            'dropout': 0.1\n",
        "        }\n",
        "    }, path)\n",
        "    print(f\"Model saved to {path}\")\n",
        "\n",
        "def load_model(path='transformer_summarizer.pth'):\n",
        "    \"\"\"Load model and tokenizer\"\"\"\n",
        "    checkpoint = torch.load(path, map_location=device)\n",
        "\n",
        "    config = checkpoint['config']\n",
        "    model = Transformer(**config).to(device)\n",
        "    model.load_state_dict(checkpoint['model_state_dict'])\n",
        "\n",
        "    tokenizer = checkpoint['tokenizer']\n",
        "\n",
        "    print(f\"Model loaded from {path}\")\n",
        "    return model, tokenizer\n",
        "\n",
        "# Save the trained model\n",
        "save_model(model, tokenizer)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0358bbdd",
      "metadata": {
        "id": "0358bbdd"
      },
      "source": [
        "## 21. Interactive Demo\n",
        "\n",
        "Create an interactive demo for testing the model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "adea684d",
      "metadata": {
        "lines_to_next_cell": 1,
        "id": "adea684d"
      },
      "outputs": [],
      "source": [
        "def interactive_demo():\n",
        "    \"\"\"Interactive demo for testing the model\"\"\"\n",
        "    print(\"Interactive TL;DR Generator\")\n",
        "    print(\"=\" * 50)\n",
        "    print(\"Enter an abstract (or 'quit' to exit):\")\n",
        "\n",
        "    while True:\n",
        "        abstract = input(\"\\nAbstract: \")\n",
        "\n",
        "        if abstract.lower() == 'quit':\n",
        "            break\n",
        "\n",
        "        if len(abstract.strip()) < 10:\n",
        "            print(\"Please enter a longer abstract.\")\n",
        "            continue\n",
        "\n",
        "        # Generate summary\n",
        "        summary = generate_summary(model, abstract, tokenizer)\n",
        "\n",
        "        print(f\"\\nGenerated TL;DR: {summary}\")\n",
        "\n",
        "        # Also show ROUGE scores if we have a reference\n",
        "        print(\"\\nWould you like to provide a reference summary for comparison? (y/n)\")\n",
        "        if input().lower() == 'y':\n",
        "            reference = input(\"Reference summary: \")\n",
        "\n",
        "            rouge1 = simple_rouge(summary, reference, n=1)\n",
        "            rouge2 = simple_rouge(summary, reference, n=2)\n",
        "\n",
        "            print(f\"\\nROUGE-1 F1: {rouge1['f1']:.4f}\")\n",
        "            print(f\"ROUGE-2 F1: {rouge2['f1']:.4f}\")\n",
        "\n",
        "# Uncomment to run interactive demo\n",
        "# interactive_demo()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b921019d",
      "metadata": {
        "id": "b921019d"
      },
      "source": [
        "## 22. Conclusion\n",
        "\n",
        "We have successfully implemented:\n",
        "\n",
        "1. **A complete Transformer encoder-decoder model** from scratch with:\n",
        "   - Multi-head attention\n",
        "   - Positional encoding\n",
        "   - Feed-forward networks\n",
        "   - Residual connections and layer normalization\n",
        "   - Proper masking for decoder\n",
        "\n",
        "2. **A simple tokenizer** for text processing\n",
        "\n",
        "3. **Training pipeline** with:\n",
        "   - Custom dataset class\n",
        "   - Teacher forcing\n",
        "   - Gradient clipping\n",
        "   - Learning rate scheduling\n",
        "\n",
        "4. **Evaluation metrics** including:\n",
        "   - ROUGE scores (simplified)\n",
        "   - Qualitative analysis\n",
        "\n",
        "5. **Sanity checks** to verify:\n",
        "   - Attention mechanism works correctly\n",
        "   - Positional encoding adds position information\n",
        "   - Model can overfit small dataset\n",
        "   - Forward pass produces correct shapes\n",
        "\n",
        "The model is small enough to train on CPU/GPU with limited resources and demonstrates the core concepts of Transformers for sequence-to-sequence tasks."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "102a5103",
      "metadata": {
        "id": "102a5103"
      },
      "outputs": [],
      "source": [
        "print(\"Notebook completed successfully!\")\n",
        "print(\"\\nModel Summary:\")\n",
        "print(f\"- Parameters: {sum(p.numel() for p in model.parameters()):,}\")\n",
        "print(f\"- d_model: {model.d_model}\")\n",
        "print(f\"- Layers: 3 encoder, 3 decoder\")\n",
        "print(f\"- Heads: 4\")\n",
        "print(f\"- Max sequence length: 128\")\n",
        "print(f\"- Vocabulary size: {vocab_size}\")"
      ]
    }
  ],
  "metadata": {
    "jupytext": {
      "cell_metadata_filter": "-all",
      "main_language": "python",
      "notebook_metadata_filter": "-all"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}